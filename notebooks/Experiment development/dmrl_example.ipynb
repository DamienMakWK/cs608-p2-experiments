{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from https://static.preferred.ai/cornac/datasets/citeulike/text.zip\n",
      "will be cached into C:\\Users\\User\\.cornac\\citeulike/raw-data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8.06MB [00:01, 5.01MB/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping ...\n",
      "File cached!\n",
      "Data from https://static.preferred.ai/cornac/datasets/citeulike/users.zip\n",
      "will be cached into C:\\Users\\User\\.cornac\\citeulike/users.dat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "467kB [00:00, 511kB/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping ...\n",
      "File cached!\n",
      "rating_threshold = 0.5\n",
      "exclude_unknowns = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\cornac\\data\\dataset.py:335: UserWarning: 18 duplicated observations are removed!\n",
      "  warnings.warn(\"%d duplicated observations are removed!\" % dup_count)\n",
      "c:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\cornac\\data\\dataset.py:335: UserWarning: 5 duplicated observations are removed!\n",
      "  warnings.warn(\"%d duplicated observations are removed!\" % dup_count)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Training data:\n",
      "Number of users = 5551\n",
      "Number of items = 16949\n",
      "Number of ratings = 168396\n",
      "Max rating = 1.0\n",
      "Min rating = 1.0\n",
      "Global mean = 1.0\n",
      "---\n",
      "Test data:\n",
      "Number of users = 5551\n",
      "Number of items = 16949\n",
      "Number of ratings = 42053\n",
      "Number of unknown users = 0\n",
      "Number of unknown items = 0\n",
      "---\n",
      "Total users = 5551\n",
      "Total items = 16949\n",
      "\n",
      "[DMRL] Training started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding the entire corpus. This might take a while.\n",
      "Using device cpu for training\n",
      "  batch 5 loss: 2840.7529296875\n",
      "  batch 10 loss: 2840.68525390625\n",
      "  batch 15 loss: 2827.552294921875\n",
      "  batch 20 loss: 2791.51708984375\n",
      "  batch 25 loss: 2779.209716796875\n",
      "  batch 30 loss: 2778.14609375\n",
      "  batch 35 loss: 2774.850927734375\n",
      "  batch 40 loss: 2762.80126953125\n",
      "Epoch: 0 is done\n",
      "  batch 5 loss: 2714.612890625\n",
      "  batch 10 loss: 2691.10478515625\n",
      "  batch 15 loss: 2656.2841796875\n",
      "  batch 20 loss: 2618.62783203125\n",
      "  batch 25 loss: 2548.287841796875\n",
      "  batch 30 loss: 2466.146435546875\n",
      "  batch 35 loss: 2372.91669921875\n",
      "  batch 40 loss: 2266.306396484375\n",
      "Epoch: 1 is done\n",
      "  batch 5 loss: 1900.1217041015625\n",
      "  batch 10 loss: 1783.2693359375\n",
      "  batch 15 loss: 1664.2704345703125\n",
      "  batch 20 loss: 1598.8288818359374\n",
      "  batch 25 loss: 1522.9539794921875\n",
      "  batch 30 loss: 1465.861572265625\n",
      "  batch 35 loss: 1423.40009765625\n",
      "  batch 40 loss: 1373.8553955078125\n",
      "Epoch: 2 is done\n",
      "  batch 5 loss: 989.97392578125\n",
      "  batch 10 loss: 943.6797729492188\n",
      "  batch 15 loss: 940.7709594726563\n",
      "  batch 20 loss: 925.9378540039063\n",
      "  batch 25 loss: 915.3305053710938\n",
      "  batch 30 loss: 900.6221801757813\n",
      "  batch 35 loss: 902.533056640625\n",
      "  batch 40 loss: 909.6389404296875\n",
      "Epoch: 3 is done\n",
      "  batch 5 loss: 657.2115356445313\n",
      "  batch 10 loss: 641.725048828125\n",
      "  batch 15 loss: 642.927978515625\n",
      "  batch 20 loss: 664.0928833007813\n",
      "  batch 25 loss: 665.4040283203125\n",
      "  batch 30 loss: 676.293212890625\n",
      "  batch 35 loss: 674.4314697265625\n",
      "  batch 40 loss: 689.1371215820312\n",
      "Epoch: 4 is done\n",
      "  batch 5 loss: 516.9606079101562\n",
      "  batch 10 loss: 510.8924255371094\n",
      "  batch 15 loss: 534.9673828125\n",
      "  batch 20 loss: 545.8792602539063\n",
      "  batch 25 loss: 556.6737670898438\n",
      "  batch 30 loss: 587.1862915039062\n",
      "  batch 35 loss: 576.6352661132812\n",
      "  batch 40 loss: 599.5447998046875\n",
      "Epoch: 5 is done\n",
      "  batch 5 loss: 457.11341552734376\n",
      "  batch 10 loss: 466.89808349609376\n",
      "  batch 15 loss: 481.72600708007815\n",
      "  batch 20 loss: 494.5773681640625\n",
      "  batch 25 loss: 508.235791015625\n",
      "  batch 30 loss: 529.7137634277344\n",
      "  batch 35 loss: 532.2187622070312\n",
      "  batch 40 loss: 559.7495727539062\n",
      "Epoch: 6 is done\n",
      "  batch 5 loss: 427.5651062011719\n",
      "  batch 10 loss: 436.08103637695314\n",
      "  batch 15 loss: 439.7329467773437\n",
      "  batch 20 loss: 457.3554626464844\n",
      "  batch 25 loss: 478.0798767089844\n",
      "  batch 30 loss: 495.31626586914064\n",
      "  batch 35 loss: 509.5243713378906\n",
      "  batch 40 loss: 516.6975708007812\n",
      "Epoch: 7 is done\n",
      "  batch 5 loss: 411.4134948730469\n",
      "  batch 10 loss: 412.996435546875\n",
      "  batch 15 loss: 440.9315612792969\n",
      "  batch 20 loss: 442.7626098632812\n",
      "  batch 25 loss: 462.9421630859375\n",
      "  batch 30 loss: 484.2298889160156\n",
      "  batch 35 loss: 492.39581298828125\n",
      "  batch 40 loss: 499.24620361328124\n",
      "Epoch: 8 is done\n",
      "  batch 5 loss: 394.785107421875\n",
      "  batch 10 loss: 401.0290100097656\n",
      "  batch 15 loss: 426.76002807617186\n",
      "  batch 20 loss: 439.16016845703126\n",
      "  batch 25 loss: 441.389111328125\n",
      "  batch 30 loss: 467.4001525878906\n",
      "  batch 35 loss: 473.5777954101562\n",
      "  batch 40 loss: 489.1389221191406\n",
      "Epoch: 9 is done\n",
      "  batch 5 loss: 391.8931945800781\n",
      "  batch 10 loss: 395.544140625\n",
      "  batch 15 loss: 416.45319213867185\n",
      "  batch 20 loss: 425.3159118652344\n",
      "  batch 25 loss: 447.6186157226563\n",
      "  batch 30 loss: 454.6284912109375\n",
      "  batch 35 loss: 462.23701171875\n",
      "  batch 40 loss: 475.42750854492186\n",
      "Epoch: 10 is done\n",
      "  batch 5 loss: 382.7917541503906\n",
      "  batch 10 loss: 378.41925659179685\n",
      "  batch 15 loss: 395.4667114257812\n",
      "  batch 20 loss: 409.8716552734375\n",
      "  batch 25 loss: 432.80388793945315\n",
      "  batch 30 loss: 447.30125732421874\n",
      "  batch 35 loss: 449.09990234375\n",
      "  batch 40 loss: 469.39235229492186\n",
      "Epoch: 11 is done\n",
      "  batch 5 loss: 372.33298950195314\n",
      "  batch 10 loss: 383.49506225585935\n",
      "  batch 15 loss: 393.9685485839844\n",
      "  batch 20 loss: 411.0425537109375\n",
      "  batch 25 loss: 421.85065307617185\n",
      "  batch 30 loss: 436.4193176269531\n",
      "  batch 35 loss: 449.58237915039064\n",
      "  batch 40 loss: 458.5092407226563\n",
      "Epoch: 12 is done\n",
      "  batch 5 loss: 371.9871398925781\n",
      "  batch 10 loss: 377.47516479492185\n",
      "  batch 15 loss: 383.1972290039063\n",
      "  batch 20 loss: 401.92473754882815\n",
      "  batch 25 loss: 414.3533142089844\n",
      "  batch 30 loss: 432.4623596191406\n",
      "  batch 35 loss: 448.55336303710936\n",
      "  batch 40 loss: 451.83240966796876\n",
      "Epoch: 13 is done\n",
      "  batch 5 loss: 363.8068115234375\n",
      "  batch 10 loss: 372.83501586914065\n",
      "  batch 15 loss: 377.6275634765625\n",
      "  batch 20 loss: 398.482568359375\n",
      "  batch 25 loss: 405.011376953125\n",
      "  batch 30 loss: 425.6121337890625\n",
      "  batch 35 loss: 432.0047119140625\n",
      "  batch 40 loss: 451.4864135742188\n",
      "Epoch: 14 is done\n",
      "  batch 5 loss: 352.1548156738281\n",
      "  batch 10 loss: 365.2312072753906\n",
      "  batch 15 loss: 372.8147705078125\n",
      "  batch 20 loss: 391.8482604980469\n",
      "  batch 25 loss: 404.10724487304685\n",
      "  batch 30 loss: 419.96209106445315\n",
      "  batch 35 loss: 433.61383666992185\n",
      "  batch 40 loss: 452.3128234863281\n",
      "Epoch: 15 is done\n",
      "  batch 5 loss: 352.61460571289064\n",
      "  batch 10 loss: 356.47565307617185\n",
      "  batch 15 loss: 373.3610412597656\n",
      "  batch 20 loss: 383.2672058105469\n",
      "  batch 25 loss: 401.94086303710935\n",
      "  batch 30 loss: 416.069384765625\n",
      "  batch 35 loss: 431.8043640136719\n",
      "  batch 40 loss: 441.70363159179686\n",
      "Epoch: 16 is done\n",
      "  batch 5 loss: 348.3980712890625\n",
      "  batch 10 loss: 356.9334411621094\n",
      "  batch 15 loss: 367.91298828125\n",
      "  batch 20 loss: 381.0550903320312\n",
      "  batch 25 loss: 385.2768493652344\n",
      "  batch 30 loss: 419.6335083007813\n",
      "  batch 35 loss: 427.58277587890626\n",
      "  batch 40 loss: 433.55364990234375\n",
      "Epoch: 17 is done\n",
      "  batch 5 loss: 342.3075317382812\n",
      "  batch 10 loss: 347.33599853515625\n",
      "  batch 15 loss: 363.1128662109375\n",
      "  batch 20 loss: 385.501806640625\n",
      "  batch 25 loss: 393.2908020019531\n",
      "  batch 30 loss: 406.07301635742186\n",
      "  batch 35 loss: 418.6151550292969\n",
      "  batch 40 loss: 434.7763427734375\n",
      "Epoch: 18 is done\n",
      "  batch 5 loss: 335.640283203125\n",
      "  batch 10 loss: 351.593408203125\n",
      "  batch 15 loss: 363.9140991210937\n",
      "  batch 20 loss: 372.8306396484375\n",
      "  batch 25 loss: 395.4289794921875\n",
      "  batch 30 loss: 395.85963745117186\n",
      "  batch 35 loss: 414.61396484375\n",
      "  batch 40 loss: 425.0697021484375\n",
      "Epoch: 19 is done\n",
      "Finished training!\n",
      "\n",
      "[DMRL] Evaluation started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking: 100%|██████████| 5444/5444 [10:51<00:00,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "     | Precision@30 | Recall@300 | Train (s) | Test (s)\n",
      "---- + ------------ + ---------- + --------- + --------\n",
      "DMRL |       0.0642 |     0.6266 | 1870.7587 | 651.5259\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Example for Disentangled Multimodal Recommendation, with only feedback and textual modality.\n",
    "For an example including image modality please see dmrl_clothes_example.py\"\"\"\n",
    "\n",
    "import cornac\n",
    "from cornac.data import Reader\n",
    "from cornac.datasets import citeulike\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.data import TextModality\n",
    "\n",
    "# The necessary data can be loaded as follows\n",
    "docs, item_ids = citeulike.load_text()\n",
    "feedback = citeulike.load_feedback(reader=Reader(item_set=item_ids))\n",
    "\n",
    "item_text_modality = TextModality(\n",
    "    corpus=docs,\n",
    "    ids=item_ids,\n",
    ")\n",
    "\n",
    "# Define an evaluation method to split feedback into train and test sets\n",
    "ratio_split = RatioSplit(\n",
    "    data=feedback,\n",
    "    test_size=0.2,\n",
    "    exclude_unknowns=True,\n",
    "    verbose=True,\n",
    "    seed=123,\n",
    "    rating_threshold=0.5,\n",
    "    item_text=item_text_modality,\n",
    ")\n",
    "\n",
    "# Instantiate DMRL recommender\n",
    "dmrl_recommender = cornac.models.dmrl.DMRL(\n",
    "    batch_size=4096,\n",
    "    epochs=20,\n",
    "    log_metrics=False,\n",
    "    learning_rate=0.01,\n",
    "    num_factors=2,\n",
    "    decay_r=0.5,\n",
    "    decay_c=0.01,\n",
    "    num_neg=3,\n",
    "    embedding_dim=100,\n",
    ")\n",
    "\n",
    "# Use Recall@300 for evaluations\n",
    "rec_300 = cornac.metrics.Recall(k=300)\n",
    "prec_30 = cornac.metrics.Precision(k=30)\n",
    "\n",
    "# Put everything together into an experiment and run it\n",
    "cornac.Experiment(\n",
    "    eval_method=ratio_split, models=[dmrl_recommender], metrics=[prec_30, rec_300]\n",
    ").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring what each modality in the example look like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cornac\n",
    "from cornac.data import Reader\n",
    "from cornac.datasets import citeulike\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.data import TextModality\n",
    "\n",
    "# The necessary data can be loaded as follows\n",
    "docs, item_ids = citeulike.load_text()\n",
    "feedback = citeulike.load_feedback(reader=Reader(item_set=item_ids))\n",
    "\n",
    "item_text_modality = TextModality(\n",
    "    corpus=docs,\n",
    "    ids=item_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '70', 1.0),\n",
       " ('1', '495', 1.0),\n",
       " ('1', '1631', 1.0),\n",
       " ('1', '2317', 1.0),\n",
       " ('1', '2526', 1.0),\n",
       " ('1', '2846', 1.0),\n",
       " ('1', '2931', 1.0),\n",
       " ('1', '3171', 1.0),\n",
       " ('1', '3297', 1.0),\n",
       " ('1', '3332', 1.0),\n",
       " ('1', '3404', 1.0),\n",
       " ('1', '3591', 1.0),\n",
       " ('1', '3595', 1.0),\n",
       " ('1', '3770', 1.0),\n",
       " ('1', '3950', 1.0),\n",
       " ('1', '4626', 1.0),\n",
       " ('1', '4662', 1.0),\n",
       " ('1', '4871', 1.0),\n",
       " ('1', '4889', 1.0),\n",
       " ('1', '5114', 1.0),\n",
       " ('1', '5324', 1.0),\n",
       " ('1', '5325', 1.0),\n",
       " ('1', '5614', 1.0),\n",
       " ('1', '5991', 1.0),\n",
       " ('1', '6103', 1.0),\n",
       " ('1', '6874', 1.0),\n",
       " ('1', '6968', 1.0),\n",
       " ('1', '7106', 1.0),\n",
       " ('1', '7801', 1.0),\n",
       " ('1', '7867', 1.0),\n",
       " ('1', '8903', 1.0),\n",
       " ('1', '9907', 1.0),\n",
       " ('1', '10008', 1.0),\n",
       " ('1', '10204', 1.0),\n",
       " ('1', '10272', 1.0),\n",
       " ('1', '10288', 1.0),\n",
       " ('1', '10508', 1.0),\n",
       " ('1', '10588', 1.0),\n",
       " ('1', '11009', 1.0),\n",
       " ('1', '11105', 1.0),\n",
       " ('1', '11226', 1.0),\n",
       " ('1', '11320', 1.0),\n",
       " ('1', '11650', 1.0),\n",
       " ('1', '11853', 1.0),\n",
       " ('1', '11919', 1.0),\n",
       " ('1', '12684', 1.0),\n",
       " ('1', '12716', 1.0),\n",
       " ('1', '12802', 1.0),\n",
       " ('1', '12803', 1.0),\n",
       " ('1', '12804', 1.0),\n",
       " ('1', '12805', 1.0),\n",
       " ('1', '12831', 1.0),\n",
       " ('1', '12916', 1.0),\n",
       " ('1', '13172', 1.0),\n",
       " ('1', '13187', 1.0),\n",
       " ('1', '13363', 1.0),\n",
       " ('1', '13923', 1.0),\n",
       " ('1', '13924', 1.0),\n",
       " ('1', '14662', 1.0),\n",
       " ('1', '14676', 1.0),\n",
       " ('1', '14851', 1.0),\n",
       " ('1', '15026', 1.0),\n",
       " ('1', '15165', 1.0),\n",
       " ('1', '15191', 1.0),\n",
       " ('1', '15282', 1.0),\n",
       " ('1', '15300', 1.0),\n",
       " ('1', '15336', 1.0),\n",
       " ('1', '15833', 1.0),\n",
       " ('1', '15894', 1.0),\n",
       " ('1', '16163', 1.0),\n",
       " ('1', '16424', 1.0),\n",
       " ('2', '38', 1.0),\n",
       " ('2', '493', 1.0),\n",
       " ('2', '942', 1.0),\n",
       " ('2', '1519', 1.0),\n",
       " ('2', '1843', 1.0),\n",
       " ('2', '1844', 1.0),\n",
       " ('2', '1896', 1.0),\n",
       " ('2', '2819', 1.0),\n",
       " ('2', '3391', 1.0),\n",
       " ('2', '5031', 1.0),\n",
       " ('2', '5572', 1.0),\n",
       " ('2', '6457', 1.0),\n",
       " ('2', '7399', 1.0),\n",
       " ('2', '8669', 1.0),\n",
       " ('2', '8990', 1.0),\n",
       " ('2', '10413', 1.0),\n",
       " ('2', '11913', 1.0),\n",
       " ('2', '12588', 1.0),\n",
       " ('2', '12896', 1.0),\n",
       " ('2', '13468', 1.0),\n",
       " ('2', '13616', 1.0),\n",
       " ('2', '13846', 1.0),\n",
       " ('2', '13856', 1.0),\n",
       " ('2', '13999', 1.0),\n",
       " ('2', '14037', 1.0),\n",
       " ('2', '14476', 1.0),\n",
       " ('2', '15047', 1.0),\n",
       " ('2', '15382', 1.0),\n",
       " ('2', '15422', 1.0),\n",
       " ('2', '15458', 1.0),\n",
       " ('2', '15515', 1.0),\n",
       " ('2', '15526', 1.0),\n",
       " ('2', '15634', 1.0),\n",
       " ('2', '15800', 1.0),\n",
       " ('2', '16022', 1.0),\n",
       " ('2', '16098', 1.0),\n",
       " ('2', '16438', 1.0),\n",
       " ('2', '16495', 1.0),\n",
       " ('2', '16745', 1.0),\n",
       " ('3', '20', 1.0),\n",
       " ('3', '517', 1.0),\n",
       " ('3', '791', 1.0),\n",
       " ('3', '800', 1.0),\n",
       " ('3', '1329', 1.0),\n",
       " ('3', '1767', 1.0),\n",
       " ('3', '1984', 1.0),\n",
       " ('3', '2126', 1.0),\n",
       " ('3', '3009', 1.0),\n",
       " ('3', '4458', 1.0),\n",
       " ('3', '4671', 1.0),\n",
       " ('3', '4792', 1.0),\n",
       " ('3', '5129', 1.0),\n",
       " ('3', '5266', 1.0),\n",
       " ('3', '6331', 1.0),\n",
       " ('3', '6385', 1.0),\n",
       " ('3', '6449', 1.0),\n",
       " ('3', '7748', 1.0),\n",
       " ('3', '7872', 1.0),\n",
       " ('3', '8800', 1.0),\n",
       " ('3', '9090', 1.0),\n",
       " ('4', '12', 1.0),\n",
       " ('4', '706', 1.0),\n",
       " ('4', '709', 1.0),\n",
       " ('4', '721', 1.0),\n",
       " ('4', '755', 1.0),\n",
       " ('4', '756', 1.0),\n",
       " ('4', '776', 1.0),\n",
       " ('4', '892', 1.0),\n",
       " ('4', '895', 1.0),\n",
       " ('4', '2588', 1.0),\n",
       " ('4', '2991', 1.0),\n",
       " ('4', '4973', 1.0),\n",
       " ('4', '5056', 1.0),\n",
       " ('5', '12', 1.0),\n",
       " ('5', '761', 1.0),\n",
       " ('5', '2678', 1.0),\n",
       " ('5', '4034', 1.0),\n",
       " ('5', '4035', 1.0),\n",
       " ('5', '5439', 1.0),\n",
       " ('5', '5494', 1.0),\n",
       " ('5', '5758', 1.0),\n",
       " ('5', '7798', 1.0),\n",
       " ('5', '9046', 1.0),\n",
       " ('5', '11877', 1.0),\n",
       " ('5', '11904', 1.0),\n",
       " ('5', '13250', 1.0),\n",
       " ('6', '31', 1.0),\n",
       " ('6', '648', 1.0),\n",
       " ('6', '649', 1.0),\n",
       " ('6', '661', 1.0),\n",
       " ('6', '670', 1.0),\n",
       " ('6', '689', 1.0),\n",
       " ('6', '690', 1.0),\n",
       " ('6', '691', 1.0),\n",
       " ('6', '809', 1.0),\n",
       " ('6', '899', 1.0),\n",
       " ('6', '953', 1.0),\n",
       " ('6', '954', 1.0),\n",
       " ('6', '1709', 1.0),\n",
       " ('6', '1964', 1.0),\n",
       " ('6', '1965', 1.0),\n",
       " ('6', '2025', 1.0),\n",
       " ('6', '3033', 1.0),\n",
       " ('6', '3657', 1.0),\n",
       " ('6', '4075', 1.0),\n",
       " ('6', '5451', 1.0),\n",
       " ('6', '5531', 1.0),\n",
       " ('6', '5548', 1.0),\n",
       " ('6', '5772', 1.0),\n",
       " ('6', '6037', 1.0),\n",
       " ('6', '6344', 1.0),\n",
       " ('6', '7167', 1.0),\n",
       " ('6', '7168', 1.0),\n",
       " ('6', '7302', 1.0),\n",
       " ('6', '7342', 1.0),\n",
       " ('6', '8024', 1.0),\n",
       " ('6', '8034', 1.0),\n",
       " ('6', '9481', 1.0),\n",
       " ('7', '12', 1.0),\n",
       " ('7', '3292', 1.0),\n",
       " ('7', '3579', 1.0),\n",
       " ('7', '3917', 1.0),\n",
       " ('7', '6247', 1.0),\n",
       " ('7', '6723', 1.0),\n",
       " ('7', '7492', 1.0),\n",
       " ('7', '7495', 1.0),\n",
       " ('7', '7499', 1.0),\n",
       " ('7', '8644', 1.0),\n",
       " ('7', '10223', 1.0),\n",
       " ('7', '11253', 1.0),\n",
       " ('7', '13406', 1.0),\n",
       " ('8', '10', 1.0),\n",
       " ('8', '1545', 1.0),\n",
       " ('8', '2526', 1.0),\n",
       " ('8', '5384', 1.0),\n",
       " ('8', '8898', 1.0),\n",
       " ('8', '9422', 1.0),\n",
       " ('8', '11105', 1.0),\n",
       " ('8', '11321', 1.0),\n",
       " ('8', '11546', 1.0),\n",
       " ('8', '12899', 1.0),\n",
       " ('8', '13587', 1.0),\n",
       " ('9', '34', 1.0),\n",
       " ('9', '861', 1.0),\n",
       " ('9', '2215', 1.0),\n",
       " ('9', '2461', 1.0),\n",
       " ('9', '3082', 1.0),\n",
       " ('9', '3445', 1.0),\n",
       " ('9', '3446', 1.0),\n",
       " ('9', '3515', 1.0),\n",
       " ('9', '4363', 1.0),\n",
       " ('9', '4397', 1.0),\n",
       " ('9', '4506', 1.0),\n",
       " ('9', '4832', 1.0),\n",
       " ('9', '4862', 1.0),\n",
       " ('9', '4887', 1.0),\n",
       " ('9', '4903', 1.0),\n",
       " ('9', '5118', 1.0),\n",
       " ('9', '5138', 1.0),\n",
       " ('9', '5267', 1.0),\n",
       " ('9', '5433', 1.0),\n",
       " ('9', '5768', 1.0),\n",
       " ('9', '6364', 1.0),\n",
       " ('9', '6783', 1.0),\n",
       " ('9', '6956', 1.0),\n",
       " ('9', '6978', 1.0),\n",
       " ('9', '6979', 1.0),\n",
       " ('9', '7280', 1.0),\n",
       " ('9', '7568', 1.0),\n",
       " ('9', '8587', 1.0),\n",
       " ('9', '8811', 1.0),\n",
       " ('9', '9055', 1.0),\n",
       " ('9', '9604', 1.0),\n",
       " ('9', '9849', 1.0),\n",
       " ('9', '10964', 1.0),\n",
       " ('9', '11828', 1.0),\n",
       " ('9', '11906', 1.0),\n",
       " ('10', '26', 1.0),\n",
       " ('10', '746', 1.0),\n",
       " ('10', '830', 1.0),\n",
       " ('10', '2000', 1.0),\n",
       " ('10', '2095', 1.0),\n",
       " ('10', '2439', 1.0),\n",
       " ('10', '2476', 1.0),\n",
       " ('10', '3090', 1.0),\n",
       " ('10', '3593', 1.0),\n",
       " ('10', '3887', 1.0),\n",
       " ('10', '4899', 1.0),\n",
       " ('10', '6620', 1.0),\n",
       " ('10', '7518', 1.0),\n",
       " ('10', '7590', 1.0),\n",
       " ('10', '7786', 1.0),\n",
       " ('10', '8183', 1.0),\n",
       " ('10', '8223', 1.0),\n",
       " ('10', '8286', 1.0),\n",
       " ('10', '8469', 1.0),\n",
       " ('10', '8703', 1.0),\n",
       " ('10', '8711', 1.0),\n",
       " ('10', '9590', 1.0),\n",
       " ('10', '10384', 1.0),\n",
       " ('10', '11900', 1.0),\n",
       " ('10', '13415', 1.0),\n",
       " ('10', '14002', 1.0),\n",
       " ('10', '14329', 1.0),\n",
       " ('11', '33', 1.0),\n",
       " ('11', '135', 1.0),\n",
       " ('11', '240', 1.0),\n",
       " ('11', '773', 1.0),\n",
       " ('11', '1124', 1.0),\n",
       " ('11', '1126', 1.0),\n",
       " ('11', '1128', 1.0),\n",
       " ('11', '1238', 1.0),\n",
       " ('11', '1389', 1.0),\n",
       " ('11', '1423', 1.0),\n",
       " ('11', '2449', 1.0),\n",
       " ('11', '2451', 1.0),\n",
       " ('11', '2452', 1.0),\n",
       " ('11', '2770', 1.0),\n",
       " ('11', '2914', 1.0),\n",
       " ('11', '2992', 1.0),\n",
       " ('11', '3468', 1.0),\n",
       " ('11', '3569', 1.0),\n",
       " ('11', '3660', 1.0),\n",
       " ('11', '3672', 1.0),\n",
       " ('11', '4702', 1.0),\n",
       " ('11', '4775', 1.0),\n",
       " ('11', '4780', 1.0),\n",
       " ('11', '5040', 1.0),\n",
       " ('11', '5216', 1.0),\n",
       " ('11', '6355', 1.0),\n",
       " ('11', '7650', 1.0),\n",
       " ('11', '7706', 1.0),\n",
       " ('11', '8968', 1.0),\n",
       " ('11', '9134', 1.0),\n",
       " ('11', '10410', 1.0),\n",
       " ('11', '10712', 1.0),\n",
       " ('11', '11027', 1.0),\n",
       " ('11', '11028', 1.0),\n",
       " ('12', '62', 1.0),\n",
       " ('12', '211', 1.0),\n",
       " ('12', '240', 1.0),\n",
       " ('12', '277', 1.0),\n",
       " ('12', '450', 1.0),\n",
       " ('12', '970', 1.0),\n",
       " ('12', '1041', 1.0),\n",
       " ('12', '1116', 1.0),\n",
       " ('12', '1755', 1.0),\n",
       " ('12', '1806', 1.0),\n",
       " ('12', '1919', 1.0),\n",
       " ('12', '2094', 1.0),\n",
       " ('12', '2434', 1.0),\n",
       " ('12', '2445', 1.0),\n",
       " ('12', '2451', 1.0),\n",
       " ('12', '2452', 1.0),\n",
       " ('12', '2742', 1.0),\n",
       " ('12', '2768', 1.0),\n",
       " ('12', '2829', 1.0),\n",
       " ('12', '2914', 1.0),\n",
       " ('12', '3007', 1.0),\n",
       " ('12', '3052', 1.0),\n",
       " ('12', '3076', 1.0),\n",
       " ('12', '3120', 1.0),\n",
       " ('12', '3866', 1.0),\n",
       " ('12', '4010', 1.0),\n",
       " ('12', '4012', 1.0),\n",
       " ('12', '4254', 1.0),\n",
       " ('12', '4381', 1.0),\n",
       " ('12', '4550', 1.0),\n",
       " ('12', '4551', 1.0),\n",
       " ('12', '4605', 1.0),\n",
       " ('12', '4609', 1.0),\n",
       " ('12', '4610', 1.0),\n",
       " ('12', '4638', 1.0),\n",
       " ('12', '4728', 1.0),\n",
       " ('12', '4777', 1.0),\n",
       " ('12', '4953', 1.0),\n",
       " ('12', '5001', 1.0),\n",
       " ('12', '5005', 1.0),\n",
       " ('12', '6401', 1.0),\n",
       " ('12', '6431', 1.0),\n",
       " ('12', '6657', 1.0),\n",
       " ('12', '6797', 1.0),\n",
       " ('12', '7318', 1.0),\n",
       " ('12', '8071', 1.0),\n",
       " ('12', '9024', 1.0),\n",
       " ('12', '9630', 1.0),\n",
       " ('12', '9648', 1.0),\n",
       " ('12', '9780', 1.0),\n",
       " ('12', '10276', 1.0),\n",
       " ('12', '11129', 1.0),\n",
       " ('12', '11135', 1.0),\n",
       " ('12', '11175', 1.0),\n",
       " ('12', '11209', 1.0),\n",
       " ('12', '11664', 1.0),\n",
       " ('12', '12000', 1.0),\n",
       " ('12', '12209', 1.0),\n",
       " ('12', '12313', 1.0),\n",
       " ('12', '12352', 1.0),\n",
       " ('12', '12463', 1.0),\n",
       " ('12', '12969', 1.0),\n",
       " ('12', '14287', 1.0),\n",
       " ('13', '82', 1.0),\n",
       " ('13', '2', 1.0),\n",
       " ('13', '14', 1.0),\n",
       " ('13', '122', 1.0),\n",
       " ('13', '352', 1.0),\n",
       " ('13', '407', 1.0),\n",
       " ('13', '617', 1.0),\n",
       " ('13', '927', 1.0),\n",
       " ('13', '1480', 1.0),\n",
       " ('13', '1538', 1.0),\n",
       " ('13', '2198', 1.0),\n",
       " ('13', '2289', 1.0),\n",
       " ('13', '2768', 1.0),\n",
       " ('13', '2830', 1.0),\n",
       " ('13', '2975', 1.0),\n",
       " ('13', '3160', 1.0),\n",
       " ('13', '3307', 1.0),\n",
       " ('13', '3358', 1.0),\n",
       " ('13', '3627', 1.0),\n",
       " ('13', '3705', 1.0),\n",
       " ('13', '3814', 1.0),\n",
       " ('13', '4202', 1.0),\n",
       " ('13', '4211', 1.0),\n",
       " ('13', '4223', 1.0),\n",
       " ('13', '4408', 1.0),\n",
       " ('13', '4411', 1.0),\n",
       " ('13', '4943', 1.0),\n",
       " ('13', '4979', 1.0),\n",
       " ('13', '5130', 1.0),\n",
       " ('13', '6164', 1.0),\n",
       " ('13', '6191', 1.0),\n",
       " ('13', '6442', 1.0),\n",
       " ('13', '6782', 1.0),\n",
       " ('13', '6820', 1.0),\n",
       " ('13', '6890', 1.0),\n",
       " ('13', '7369', 1.0),\n",
       " ('13', '7870', 1.0),\n",
       " ('13', '8062', 1.0),\n",
       " ('13', '8119', 1.0),\n",
       " ('13', '8138', 1.0),\n",
       " ('13', '8153', 1.0),\n",
       " ('13', '8594', 1.0),\n",
       " ('13', '8711', 1.0),\n",
       " ('13', '8934', 1.0),\n",
       " ('13', '9015', 1.0),\n",
       " ('13', '9835', 1.0),\n",
       " ('13', '9887', 1.0),\n",
       " ('13', '10230', 1.0),\n",
       " ('13', '10265', 1.0),\n",
       " ('13', '10628', 1.0),\n",
       " ('13', '10683', 1.0),\n",
       " ('13', '10987', 1.0),\n",
       " ('13', '11246', 1.0),\n",
       " ('13', '11277', 1.0),\n",
       " ('13', '11491', 1.0),\n",
       " ('13', '11504', 1.0),\n",
       " ('13', '11567', 1.0),\n",
       " ('13', '11602', 1.0),\n",
       " ('13', '11731', 1.0),\n",
       " ('13', '12259', 1.0),\n",
       " ('13', '12305', 1.0),\n",
       " ('13', '12391', 1.0),\n",
       " ('13', '12441', 1.0),\n",
       " ('13', '12990', 1.0),\n",
       " ('13', '13002', 1.0),\n",
       " ('13', '13727', 1.0),\n",
       " ('13', '13916', 1.0),\n",
       " ('13', '14187', 1.0),\n",
       " ('13', '14254', 1.0),\n",
       " ('13', '14332', 1.0),\n",
       " ('13', '14789', 1.0),\n",
       " ('13', '15244', 1.0),\n",
       " ('13', '15878', 1.0),\n",
       " ('13', '15955', 1.0),\n",
       " ('13', '15957', 1.0),\n",
       " ('13', '15978', 1.0),\n",
       " ('13', '16039', 1.0),\n",
       " ('13', '16045', 1.0),\n",
       " ('13', '16223', 1.0),\n",
       " ('13', '16300', 1.0),\n",
       " ('13', '16721', 1.0),\n",
       " ('13', '16814', 1.0),\n",
       " ('13', '16878', 1.0),\n",
       " ('14', '20', 1.0),\n",
       " ('14', '277', 1.0),\n",
       " ('14', '966', 1.0),\n",
       " ('14', '1403', 1.0),\n",
       " ('14', '1837', 1.0),\n",
       " ('14', '1929', 1.0),\n",
       " ('14', '2589', 1.0),\n",
       " ('14', '2829', 1.0),\n",
       " ('14', '2878', 1.0),\n",
       " ('14', '2902', 1.0),\n",
       " ('14', '3075', 1.0),\n",
       " ('14', '3877', 1.0),\n",
       " ('14', '4579', 1.0),\n",
       " ('14', '4777', 1.0),\n",
       " ('14', '5040', 1.0),\n",
       " ('14', '6037', 1.0),\n",
       " ('14', '6512', 1.0),\n",
       " ('14', '7000', 1.0),\n",
       " ('14', '7054', 1.0),\n",
       " ('14', '7229', 1.0),\n",
       " ('14', '8071', 1.0),\n",
       " ('15', '12', 1.0),\n",
       " ('15', '374', 1.0),\n",
       " ('15', '2533', 1.0),\n",
       " ('15', '4079', 1.0),\n",
       " ('15', '4420', 1.0),\n",
       " ('15', '7029', 1.0),\n",
       " ('15', '7453', 1.0),\n",
       " ('15', '9533', 1.0),\n",
       " ('15', '9628', 1.0),\n",
       " ('15', '10737', 1.0),\n",
       " ('15', '12571', 1.0),\n",
       " ('15', '12681', 1.0),\n",
       " ('15', '13447', 1.0),\n",
       " ('16', '10', 1.0),\n",
       " ('16', '823', 1.0),\n",
       " ('16', '4179', 1.0),\n",
       " ('16', '6359', 1.0),\n",
       " ('16', '11640', 1.0),\n",
       " ('16', '12826', 1.0),\n",
       " ('16', '15113', 1.0),\n",
       " ('16', '15708', 1.0),\n",
       " ('16', '16045', 1.0),\n",
       " ('16', '16098', 1.0),\n",
       " ('16', '16319', 1.0),\n",
       " ('17', '19', 1.0),\n",
       " ('17', '220', 1.0),\n",
       " ('17', '885', 1.0),\n",
       " ('17', '1180', 1.0),\n",
       " ('17', '1181', 1.0),\n",
       " ('17', '2462', 1.0),\n",
       " ('17', '2635', 1.0),\n",
       " ('17', '2637', 1.0),\n",
       " ('17', '2794', 1.0),\n",
       " ('17', '3012', 1.0),\n",
       " ('17', '4283', 1.0),\n",
       " ('17', '5603', 1.0),\n",
       " ('17', '5849', 1.0),\n",
       " ('17', '8020', 1.0),\n",
       " ('17', '9222', 1.0),\n",
       " ('17', '10918', 1.0),\n",
       " ('17', '12201', 1.0),\n",
       " ('17', '13086', 1.0),\n",
       " ('17', '14022', 1.0),\n",
       " ('17', '15928', 1.0),\n",
       " ('18', '15', 1.0),\n",
       " ('18', '441', 1.0),\n",
       " ('18', '2719', 1.0),\n",
       " ('18', '2720', 1.0),\n",
       " ('18', '2721', 1.0),\n",
       " ('18', '2821', 1.0),\n",
       " ('18', '3481', 1.0),\n",
       " ('18', '3591', 1.0),\n",
       " ('18', '3821', 1.0),\n",
       " ('18', '3970', 1.0),\n",
       " ('18', '4139', 1.0),\n",
       " ('18', '4667', 1.0),\n",
       " ('18', '5701', 1.0),\n",
       " ('18', '6416', 1.0),\n",
       " ('18', '6572', 1.0),\n",
       " ('18', '7313', 1.0),\n",
       " ('19', '10', 1.0),\n",
       " ('19', '2464', 1.0),\n",
       " ('19', '3064', 1.0),\n",
       " ('19', '3614', 1.0),\n",
       " ('19', '4235', 1.0),\n",
       " ('19', '4629', 1.0),\n",
       " ('19', '4797', 1.0),\n",
       " ('19', '5836', 1.0),\n",
       " ('19', '5994', 1.0),\n",
       " ('19', '6863', 1.0),\n",
       " ('19', '9700', 1.0),\n",
       " ('20', '23', 1.0),\n",
       " ('20', '6484', 1.0),\n",
       " ('20', '10505', 1.0),\n",
       " ('20', '10605', 1.0),\n",
       " ('20', '11158', 1.0),\n",
       " ('20', '11211', 1.0),\n",
       " ('20', '11328', 1.0),\n",
       " ('20', '11584', 1.0),\n",
       " ('20', '12462', 1.0),\n",
       " ('20', '12506', 1.0),\n",
       " ('20', '12842', 1.0),\n",
       " ('20', '13095', 1.0),\n",
       " ('20', '13443', 1.0),\n",
       " ('20', '13444', 1.0),\n",
       " ('20', '13491', 1.0),\n",
       " ('20', '13874', 1.0),\n",
       " ('20', '13942', 1.0),\n",
       " ('20', '14018', 1.0),\n",
       " ('20', '14164', 1.0),\n",
       " ('20', '14168', 1.0),\n",
       " ('20', '14176', 1.0),\n",
       " ('20', '14177', 1.0),\n",
       " ('20', '14348', 1.0),\n",
       " ('20', '14523', 1.0),\n",
       " ('21', '114', 1.0),\n",
       " ('21', '20', 1.0),\n",
       " ('21', '108', 1.0),\n",
       " ('21', '264', 1.0),\n",
       " ('21', '328', 1.0),\n",
       " ('21', '752', 1.0),\n",
       " ('21', '903', 1.0),\n",
       " ('21', '921', 1.0),\n",
       " ('21', '944', 1.0),\n",
       " ('21', '994', 1.0),\n",
       " ('21', '1005', 1.0),\n",
       " ('21', '1007', 1.0),\n",
       " ('21', '1113', 1.0),\n",
       " ('21', '1114', 1.0),\n",
       " ('21', '1122', 1.0),\n",
       " ('21', '1168', 1.0),\n",
       " ('21', '1181', 1.0),\n",
       " ('21', '1194', 1.0),\n",
       " ('21', '1197', 1.0),\n",
       " ('21', '1210', 1.0),\n",
       " ('21', '1250', 1.0),\n",
       " ('21', '1534', 1.0),\n",
       " ('21', '1544', 1.0),\n",
       " ('21', '1565', 1.0),\n",
       " ('21', '1602', 1.0),\n",
       " ('21', '1625', 1.0),\n",
       " ('21', '1626', 1.0),\n",
       " ('21', '1664', 1.0),\n",
       " ('21', '1665', 1.0),\n",
       " ('21', '1671', 1.0),\n",
       " ('21', '1677', 1.0),\n",
       " ('21', '1689', 1.0),\n",
       " ('21', '1691', 1.0),\n",
       " ('21', '2166', 1.0),\n",
       " ('21', '2219', 1.0),\n",
       " ('21', '2365', 1.0),\n",
       " ('21', '2368', 1.0),\n",
       " ('21', '2416', 1.0),\n",
       " ('21', '2596', 1.0),\n",
       " ('21', '2710', 1.0),\n",
       " ('21', '3213', 1.0),\n",
       " ('21', '3246', 1.0),\n",
       " ('21', '3316', 1.0),\n",
       " ('21', '3321', 1.0),\n",
       " ('21', '3364', 1.0),\n",
       " ('21', '3365', 1.0),\n",
       " ('21', '3367', 1.0),\n",
       " ('21', '4388', 1.0),\n",
       " ('21', '4957', 1.0),\n",
       " ('21', '5044', 1.0),\n",
       " ('21', '5070', 1.0),\n",
       " ('21', '5107', 1.0),\n",
       " ('21', '5108', 1.0),\n",
       " ('21', '5615', 1.0),\n",
       " ('21', '5616', 1.0),\n",
       " ('21', '5617', 1.0),\n",
       " ('21', '5618', 1.0),\n",
       " ('21', '5619', 1.0),\n",
       " ('21', '5620', 1.0),\n",
       " ('21', '5621', 1.0),\n",
       " ('21', '5622', 1.0),\n",
       " ('21', '5623', 1.0),\n",
       " ('21', '5624', 1.0),\n",
       " ('21', '5625', 1.0),\n",
       " ('21', '5626', 1.0),\n",
       " ('21', '5627', 1.0),\n",
       " ('21', '5628', 1.0),\n",
       " ('21', '5629', 1.0),\n",
       " ('21', '5630', 1.0),\n",
       " ('21', '5631', 1.0),\n",
       " ('21', '5632', 1.0),\n",
       " ('21', '5633', 1.0),\n",
       " ('21', '5634', 1.0),\n",
       " ('21', '5635', 1.0),\n",
       " ('21', '5659', 1.0),\n",
       " ('21', '5721', 1.0),\n",
       " ('21', '5875', 1.0),\n",
       " ('21', '5876', 1.0),\n",
       " ('21', '5877', 1.0),\n",
       " ('21', '5878', 1.0),\n",
       " ('21', '5879', 1.0),\n",
       " ('21', '5880', 1.0),\n",
       " ('21', '5881', 1.0),\n",
       " ('21', '5882', 1.0),\n",
       " ('21', '5883', 1.0),\n",
       " ('21', '5884', 1.0),\n",
       " ('21', '5885', 1.0),\n",
       " ('21', '5886', 1.0),\n",
       " ('21', '5887', 1.0),\n",
       " ('21', '5888', 1.0),\n",
       " ('21', '5889', 1.0),\n",
       " ('21', '5890', 1.0),\n",
       " ('21', '5891', 1.0),\n",
       " ('21', '5892', 1.0),\n",
       " ('21', '5893', 1.0),\n",
       " ('21', '5894', 1.0),\n",
       " ('21', '5895', 1.0),\n",
       " ('21', '5896', 1.0),\n",
       " ('21', '5897', 1.0),\n",
       " ('21', '5898', 1.0),\n",
       " ('21', '5899', 1.0),\n",
       " ('21', '5900', 1.0),\n",
       " ('21', '5901', 1.0),\n",
       " ('21', '5902', 1.0),\n",
       " ('21', '5903', 1.0),\n",
       " ('21', '5904', 1.0),\n",
       " ('21', '7072', 1.0),\n",
       " ('21', '9761', 1.0),\n",
       " ('21', '9864', 1.0),\n",
       " ('21', '9865', 1.0),\n",
       " ('21', '9866', 1.0),\n",
       " ('21', '9867', 1.0),\n",
       " ('21', '9868', 1.0),\n",
       " ('21', '10363', 1.0),\n",
       " ('21', '15131', 1.0),\n",
       " ('22', '75', 1.0),\n",
       " ('22', '20', 1.0),\n",
       " ('22', '108', 1.0),\n",
       " ('22', '328', 1.0),\n",
       " ('22', '752', 1.0),\n",
       " ('22', '944', 1.0),\n",
       " ('22', '994', 1.0),\n",
       " ('22', '1005', 1.0),\n",
       " ('22', '1007', 1.0),\n",
       " ('22', '1113', 1.0),\n",
       " ('22', '1114', 1.0),\n",
       " ('22', '1168', 1.0),\n",
       " ('22', '1181', 1.0),\n",
       " ('22', '1194', 1.0),\n",
       " ('22', '1197', 1.0),\n",
       " ('22', '1210', 1.0),\n",
       " ('22', '1250', 1.0),\n",
       " ('22', '1534', 1.0),\n",
       " ('22', '1544', 1.0),\n",
       " ('22', '1565', 1.0),\n",
       " ('22', '1602', 1.0),\n",
       " ('22', '1625', 1.0),\n",
       " ('22', '1626', 1.0),\n",
       " ('22', '1664', 1.0),\n",
       " ('22', '1665', 1.0),\n",
       " ('22', '1671', 1.0),\n",
       " ('22', '1677', 1.0),\n",
       " ('22', '1689', 1.0),\n",
       " ('22', '1691', 1.0),\n",
       " ('22', '2219', 1.0),\n",
       " ('22', '2368', 1.0),\n",
       " ('22', '2416', 1.0),\n",
       " ('22', '2596', 1.0),\n",
       " ('22', '2710', 1.0),\n",
       " ('22', '3213', 1.0),\n",
       " ('22', '3316', 1.0),\n",
       " ('22', '3321', 1.0),\n",
       " ('22', '3364', 1.0),\n",
       " ('22', '3365', 1.0),\n",
       " ('22', '3367', 1.0),\n",
       " ('22', '4388', 1.0),\n",
       " ('22', '4957', 1.0),\n",
       " ('22', '5044', 1.0),\n",
       " ('22', '5107', 1.0),\n",
       " ('22', '5108', 1.0),\n",
       " ('22', '5659', 1.0),\n",
       " ('22', '5875', 1.0),\n",
       " ('22', '5876', 1.0),\n",
       " ('22', '5877', 1.0),\n",
       " ('22', '5878', 1.0),\n",
       " ('22', '5879', 1.0),\n",
       " ('22', '5880', 1.0),\n",
       " ('22', '5881', 1.0),\n",
       " ('22', '5882', 1.0),\n",
       " ('22', '5883', 1.0),\n",
       " ('22', '5884', 1.0),\n",
       " ('22', '5885', 1.0),\n",
       " ('22', '5886', 1.0),\n",
       " ('22', '5887', 1.0),\n",
       " ('22', '5888', 1.0),\n",
       " ('22', '5889', 1.0),\n",
       " ('22', '5890', 1.0),\n",
       " ('22', '5891', 1.0),\n",
       " ('22', '5892', 1.0),\n",
       " ('22', '5893', 1.0),\n",
       " ('22', '5894', 1.0),\n",
       " ('22', '5895', 1.0),\n",
       " ('22', '5896', 1.0),\n",
       " ('22', '5897', 1.0),\n",
       " ('22', '5898', 1.0),\n",
       " ('22', '5899', 1.0),\n",
       " ('22', '5900', 1.0),\n",
       " ('22', '5901', 1.0),\n",
       " ('22', '5902', 1.0),\n",
       " ('22', '5903', 1.0),\n",
       " ('22', '5904', 1.0),\n",
       " ('23', '70', 1.0),\n",
       " ('23', '267', 1.0),\n",
       " ('23', '280', 1.0),\n",
       " ('23', '285', 1.0),\n",
       " ('23', '286', 1.0),\n",
       " ('23', '398', 1.0),\n",
       " ('23', '399', 1.0),\n",
       " ('23', '430', 1.0),\n",
       " ('23', '710', 1.0),\n",
       " ('23', '711', 1.0),\n",
       " ('23', '814', 1.0),\n",
       " ('23', '909', 1.0),\n",
       " ('23', '910', 1.0),\n",
       " ('23', '1115', 1.0),\n",
       " ('23', '1156', 1.0),\n",
       " ('23', '1355', 1.0),\n",
       " ('23', '1358', 1.0),\n",
       " ('23', '1364', 1.0),\n",
       " ('23', '1368', 1.0),\n",
       " ('23', '1369', 1.0),\n",
       " ('23', '1371', 1.0),\n",
       " ('23', '1372', 1.0),\n",
       " ('23', '1707', 1.0),\n",
       " ('23', '2077', 1.0),\n",
       " ('23', '2334', 1.0),\n",
       " ('23', '2338', 1.0),\n",
       " ('23', '2342', 1.0),\n",
       " ('23', '2345', 1.0),\n",
       " ('23', '2346', 1.0),\n",
       " ('23', '2785', 1.0),\n",
       " ('23', '2787', 1.0),\n",
       " ('23', '2790', 1.0),\n",
       " ('23', '2883', 1.0),\n",
       " ('23', '2921', 1.0),\n",
       " ('23', '2983', 1.0),\n",
       " ('23', '3067', 1.0),\n",
       " ('23', '3226', 1.0),\n",
       " ('23', '3502', 1.0),\n",
       " ('23', '3638', 1.0),\n",
       " ('23', '3658', 1.0),\n",
       " ('23', '3659', 1.0),\n",
       " ('23', '3687', 1.0),\n",
       " ('23', '3688', 1.0),\n",
       " ('23', '4461', 1.0),\n",
       " ('23', '4462', 1.0),\n",
       " ('23', '4585', 1.0),\n",
       " ('23', '5395', 1.0),\n",
       " ('23', '5602', 1.0),\n",
       " ('23', '5603', 1.0),\n",
       " ('23', '5733', 1.0),\n",
       " ('23', '5781', 1.0),\n",
       " ('23', '5782', 1.0),\n",
       " ('23', '6919', 1.0),\n",
       " ('23', '6938', 1.0),\n",
       " ('23', '6971', 1.0),\n",
       " ('23', '6972', 1.0),\n",
       " ('23', '6973', 1.0),\n",
       " ('23', '7096', 1.0),\n",
       " ('23', '7187', 1.0),\n",
       " ('23', '7349', 1.0),\n",
       " ('23', '7401', 1.0),\n",
       " ('23', '7437', 1.0),\n",
       " ('23', '7506', 1.0),\n",
       " ('23', '8016', 1.0),\n",
       " ('23', '8112', 1.0),\n",
       " ('23', '8178', 1.0),\n",
       " ('23', '8698', 1.0),\n",
       " ('23', '9374', 1.0),\n",
       " ('23', '9759', 1.0),\n",
       " ('23', '10465', 1.0),\n",
       " ('23', '11697', 1.0),\n",
       " ('24', '180', 1.0),\n",
       " ('24', '461', 1.0),\n",
       " ('24', '466', 1.0),\n",
       " ('24', '501', 1.0),\n",
       " ('24', '685', 1.0),\n",
       " ('24', '687', 1.0),\n",
       " ('24', '767', 1.0),\n",
       " ('24', '1303', 1.0),\n",
       " ('24', '1304', 1.0),\n",
       " ('24', '1580', 1.0),\n",
       " ('24', '1599', 1.0),\n",
       " ('24', '1673', 1.0),\n",
       " ('24', '1737', 1.0),\n",
       " ('24', '1937', 1.0),\n",
       " ('24', '2193', 1.0),\n",
       " ('24', '2364', 1.0),\n",
       " ('24', '3088', 1.0),\n",
       " ('24', '3441', 1.0),\n",
       " ('24', '3691', 1.0),\n",
       " ('24', '3726', 1.0),\n",
       " ('24', '3802', 1.0),\n",
       " ('24', '3903', 1.0),\n",
       " ('24', '3962', 1.0),\n",
       " ('24', '3963', 1.0),\n",
       " ('24', '4048', 1.0),\n",
       " ('24', '4155', 1.0),\n",
       " ('24', '4177', 1.0),\n",
       " ('24', '4310', 1.0),\n",
       " ('24', '4329', 1.0),\n",
       " ('24', '4359', 1.0),\n",
       " ('24', '4648', 1.0),\n",
       " ('24', '4689', 1.0),\n",
       " ('24', '4850', 1.0),\n",
       " ('24', '4872', 1.0),\n",
       " ('24', '4910', 1.0),\n",
       " ('24', '4915', 1.0),\n",
       " ('24', '5164', 1.0),\n",
       " ('24', '5165', 1.0),\n",
       " ('24', '5166', 1.0),\n",
       " ('24', '5344', 1.0),\n",
       " ('24', '5376', 1.0),\n",
       " ('24', '5380', 1.0),\n",
       " ('24', '5638', 1.0),\n",
       " ('24', '5716', 1.0),\n",
       " ('24', '5717', 1.0),\n",
       " ('24', '5818', 1.0),\n",
       " ('24', '6273', 1.0),\n",
       " ('24', '7076', 1.0),\n",
       " ('24', '7260', 1.0),\n",
       " ('24', '7312', 1.0),\n",
       " ('24', '7371', 1.0),\n",
       " ('24', '7561', 1.0),\n",
       " ('24', '7617', 1.0),\n",
       " ('24', '7647', 1.0),\n",
       " ('24', '7758', 1.0),\n",
       " ('24', '7824', 1.0),\n",
       " ('24', '8272', 1.0),\n",
       " ('24', '8362', 1.0),\n",
       " ('24', '8683', 1.0),\n",
       " ('24', '8755', 1.0),\n",
       " ('24', '9130', 1.0),\n",
       " ('24', '9147', 1.0),\n",
       " ('24', '9321', 1.0),\n",
       " ('24', '9420', 1.0),\n",
       " ('24', '9497', 1.0),\n",
       " ('24', '9625', 1.0),\n",
       " ('24', '9692', 1.0),\n",
       " ('24', '9693', 1.0),\n",
       " ('24', '9695', 1.0),\n",
       " ('24', '9903', 1.0),\n",
       " ('24', '9905', 1.0),\n",
       " ('24', '9914', 1.0),\n",
       " ('24', '10041', 1.0),\n",
       " ('24', '10215', 1.0),\n",
       " ('24', '10252', 1.0),\n",
       " ('24', '10315', 1.0),\n",
       " ('24', '10530', 1.0),\n",
       " ('24', '10560', 1.0),\n",
       " ('24', '10568', 1.0),\n",
       " ('24', '10632', 1.0),\n",
       " ('24', '10648', 1.0),\n",
       " ('24', '10650', 1.0),\n",
       " ('24', '10662', 1.0),\n",
       " ('24', '10705', 1.0),\n",
       " ('24', '10706', 1.0),\n",
       " ('24', '10714', 1.0),\n",
       " ('24', '10716', 1.0),\n",
       " ('24', '10722', 1.0),\n",
       " ('24', '10730', 1.0),\n",
       " ('24', '10757', 1.0),\n",
       " ('24', '10768', 1.0),\n",
       " ('24', '10794', 1.0),\n",
       " ('24', '10860', 1.0),\n",
       " ('24', '10943', 1.0),\n",
       " ('24', '10950', 1.0),\n",
       " ('24', '11191', 1.0),\n",
       " ('24', '11246', 1.0),\n",
       " ('24', '11273', 1.0),\n",
       " ('24', '11323', 1.0),\n",
       " ('24', '11582', 1.0),\n",
       " ('24', '11819', 1.0),\n",
       " ('24', '11838', 1.0),\n",
       " ('24', '11956', 1.0),\n",
       " ('24', '12084', 1.0),\n",
       " ('24', '12153', 1.0),\n",
       " ('24', '12227', 1.0),\n",
       " ('24', '12288', 1.0),\n",
       " ('24', '12429', 1.0),\n",
       " ('24', '12509', 1.0),\n",
       " ('24', '12615', 1.0),\n",
       " ('24', '12643', 1.0),\n",
       " ('24', '12790', 1.0),\n",
       " ('24', '12919', 1.0),\n",
       " ('24', '12945', 1.0),\n",
       " ('24', '13034', 1.0),\n",
       " ('24', '13058', 1.0),\n",
       " ('24', '13078', 1.0),\n",
       " ('24', '13092', 1.0),\n",
       " ('24', '13110', 1.0),\n",
       " ('24', '13130', 1.0),\n",
       " ('24', '13286', 1.0),\n",
       " ('24', '13301', 1.0),\n",
       " ('24', '13316', 1.0),\n",
       " ('24', '13320', 1.0),\n",
       " ('24', '13321', 1.0),\n",
       " ('24', '13348', 1.0),\n",
       " ('24', '13357', 1.0),\n",
       " ('24', '13410', 1.0),\n",
       " ('24', '13542', 1.0),\n",
       " ('24', '13546', 1.0),\n",
       " ('24', '13547', 1.0),\n",
       " ('24', '13610', 1.0),\n",
       " ('24', '13647', 1.0),\n",
       " ('24', '13745', 1.0),\n",
       " ('24', '13886', 1.0),\n",
       " ('24', '13900', 1.0),\n",
       " ('24', '14008', 1.0),\n",
       " ('24', '14173', 1.0),\n",
       " ('24', '14201', 1.0),\n",
       " ('24', '14449', 1.0),\n",
       " ('24', '14498', 1.0),\n",
       " ('24', '14502', 1.0),\n",
       " ('24', '14510', 1.0),\n",
       " ('24', '14526', 1.0),\n",
       " ('24', '14556', 1.0),\n",
       " ('24', '14573', 1.0),\n",
       " ('24', '14580', 1.0),\n",
       " ('24', '14635', 1.0),\n",
       " ('24', '14640', 1.0),\n",
       " ('24', '14656', 1.0),\n",
       " ('24', '14703', 1.0),\n",
       " ('24', '14705', 1.0),\n",
       " ('24', '14716', 1.0),\n",
       " ('24', '14729', 1.0),\n",
       " ('24', '14745', 1.0),\n",
       " ('24', '14770', 1.0),\n",
       " ('24', '14782', 1.0),\n",
       " ('24', '14946', 1.0),\n",
       " ('24', '15030', 1.0),\n",
       " ('24', '15066', 1.0),\n",
       " ('24', '15141', 1.0),\n",
       " ('24', '15218', 1.0),\n",
       " ('24', '15225', 1.0),\n",
       " ('24', '15267', 1.0),\n",
       " ('24', '15354', 1.0),\n",
       " ('24', '15357', 1.0),\n",
       " ('24', '15412', 1.0),\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cornac.data.text.TextModality at 0x2331df78c10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_text_modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The metabolic world of Escherichia coli is not small. To elucidate the organizational and evolutionary principles of the metabolism of living organisms, recent studies have addressed the graph-theoretic analysis of large biochemical networks responsible for the synthesis and degradation of cellular building blocks [Jeong, H., Tombor, B., Albert, R., Oltvai, Z. N. \\\\& Barab\\\\{\\\\'a\\\\}si, A. L. (2000) Nature 407, 651-654; Wagner, A. \\\\& Fell, D. A. (2001) Proc. R. Soc. London Ser. B 268, 1803-1810; and Ma, H.-W. \\\\& Zeng, A.-P. (2003) Bioinformatics 19, 270-277]. In such studies, the global properties of the network are computed by considering enzymatic reactions as links between metabolites. However, the pathways computed in this manner do not conserve their structural moieties and therefore do not correspond to biochemical pathways on the traditional metabolic map. In this work, we reassessed earlier results by digitizing carbon atomic traces in metabolic reactions annotated for Escherichia coli. Our analysis revealed that the average path length of its metabolism is much longer than previously thought and that the metabolic world of this organism is not small in terms of biosynthesis and degradation.\",\n",
       " 'Reverse Engineering of Biological Complexity. Advanced technologies and biology have extremely different physical implementations, but they are far more alike in systems-level organization than is widely appreciated. {C}onvergent evolution in both domains produces modular architectures that are composed of elaborate hierarchies of protocols and layers of feedback regulation, are driven by demand for robustness to uncertain environments, and use often imprecise components. {T}his complexity may be largely hidden in idealized laboratory settings and in normal operation, becoming conspicuous only when contributing to rare cascading failures. {T}hese puzzling and paradoxical features are neither accidental nor artificial, but derive from a deep and necessary interplay between complexity and robustness, modularity, feedback, and fragility. {T}his review describes insights from engineering theory and practice that can shed some light on biological complexity.',\n",
       " 'Exploring complex networks. The study of networks pervades all of science, from neurobiology to statistical physics. {T}he most basic issues are structural: how does one characterize the wiring diagram of a food web or the {I}nternet or the metabolic network of the bacterium {E}scherichia coli? {A}re there any unifying principles underlying their topology? {F}rom the perspective of nonlinear dynamics, we would also like to understand how an enormous network of interacting dynamical systems-be they neurons, power stations or lasers-will behave collectively, given their individual dynamics and coupling architecture. {R}esearchers are only now beginning to unravel the structure and dynamics of complex networks.',\n",
       " 'Comparative assessment of large-scale data sets of protein-protein interactions.. Comprehensive protein protein interaction maps promise to reveal many aspects of the complex regulatory network underlying cellular function. Recently, large-scale approaches have predicted many new protein interactions in yeast. To measure their accuracy and potential as well as to identify biases, strengths and weaknesses, we compare the methods with each other and with a reference set of previously reported protein interactions.',\n",
       " 'Navigation in a small world. The small-world phenomenon — the principle that most of us are linked by short chains of acquaintances — was first investigated as a question in sociology1, 2 and is a feature of a range of networks arising in nature and technology3, 4, 5. Experimental study of the phenomenon1 revealed that it has two fundamental components: first, such short chains are ubiquitous, and second, individuals operating with purely local information are very adept at finding these chains. The first issue has been analysed2, 3, 4, and here I investigate the second by modelling how individuals can find short chains in a large social network.',\n",
       " 'Random graphs with arbitrary degree distributions and their applications.. Recent work on the structure of social networks and the internet has focussed attention on graphs with distributions of vertex degree that are significantly different from the Poisson degree distributions that have been widely studied in the past. In this paper we develop in detail the theory of random graphs with arbitrary degree distributions. In addition to simple undirected, unipartite graphs, we examine the properties of directed and bipartite graphs. Among other results, we derive exact expressions for the position of the phase transition at which a giant component first forms, the mean component size, the size of the giant component if there is one, the mean number of vertices a certain distance away from a randomly chosen vertex, and the average vertex-vertex distance within a graph. We apply our theory to some real-world graphs, including the world-wide web and collaboration graphs of scientists and Fortune 1000 company directors. We demonstrate that in some cases random graphs with appropriate distributions of vertex degree predict with surprising accuracy the behavior of the real world, while in others there is a measurable discrepancy between theory and reality, perhaps indicating the presence of additional social structure in the network that is not captured by the random graph.',\n",
       " 'Artificial gene networks for objective comparison of analysis  algorithms. Motivation: Large-scale gene expression profiling generates data sets that are rich in observed features but poor in numbers of observations. The analysis of such data sets is a challenge that has been object of vigorous research. The algorithms in use for this purpose have been poorly documented and rarely compared objectively, posing a problem of uncertainty about the outcomes of the analyses. One way to objectively test such analysis algorithms is to apply them on computational gene network models for which the mechanisms are completely know.  Results: We present a system that generates random artificial   gene networks according to well-defined topological and kinetic   properties. These are used to run in silico experiments simulating   real laboratory microarray experiments. Noise with controlled   properties is added to the simulation results several times   emulating measurement replicates, before expression ratios are calculated.  Availability: The data sets and kinetic models described here are available from http://www.vbi.vt.edu/~mendes/AGN/as  biochemical dynamic models in SBML and Gepasi formats.  Contact: mendes@vt.edu 10.1093/bioinformatics/btg1069',\n",
       " \"The segment polarity network is a robust developmental module. All insects possess homologous segments, but segment specification differs radically among insect orders. {I}n {D}rosophila, maternal morphogens control the patterned activation of gap genes, which encode transcriptional regulators that shape the patterned expression of pair-rule genes. {T}his patterning cascade takes place before cellularization. {P}air-rule gene products subsequently 'imprint' segment polarity genes with reiterated patterns, thus defining the primordial segments. {T}his mechanism must be greatly modified in insect groups in which many segments emerge only after cellularization. {I}n beetles and parasitic wasps, for instance, pair-rule homologues are expressed in patterns consistent with roles during segmentation, but these patterns emerge within cellular fields. {I}n contrast, although in locusts pair-rule homologues may not control segmentation, some segment polarity genes and their interactions are conserved. {P}erhaps segmentation is modular, with each module autonomously expressing a characteristic intrinsic behaviour in response to transient stimuli. {I}f so, evolution could rearrange inputs to modules without changing their intrinsic behaviours. {H}ere we suggest, using computer simulations, that the {D}rosophila segment polarity genes constitute such a module, and that this module is resistant to variations in the kinetic constants that govern its behaviour.\",\n",
       " 'The evolutionary origin of complex features.. A long-standing challenge to evolutionary theory has been whether it can explain the origin of complex organismal features. {W}e examined this issue using digital organisms--computer programs that self-replicate, mutate, compete and evolve. {P}opulations of digital organisms often evolved the ability to perform complex logic functions requiring the coordinated execution of many genomic instructions. {C}omplex functions evolved by building on simpler functions that had evolved earlier, provided that these were also selectively favoured. {H}owever, no particular intermediate stage was essential for evolving complex functions. {T}he first genotypes able to perform complex functions differed from their non-performing parents by only one or two mutations, but differed from the ancestor by many mutations that were also crucial to the new functions. {I}n some cases, mutations that were deleterious when they appeared served as stepping-stones in the evolution of complex features. {T}hese findings show how complex functions can originate by random mutation and natural selection.',\n",
       " \"Early language acquisition: cracking the speech code.. Infants learn language with remarkable speed, but how they do it remains a mystery. New data show that infants use computational strategies to detect the statistical and prosodic patterns in language input, and that this leads to the discovery of phonemes and words. Social interaction with another human being affects speech learning in a way that resembles communicative learning in songbirds. The brain's commitment to the statistical and prosodic patterns that are experienced early in life might help to explain the long-standing puzzle of why infants are better language learners than adults. Successful learning by infants, as well as constraints on that learning, are changing theories of language acquisition.\",\n",
       " 'Organization, development and function of complex brain networks.  Recent research has revealed general principles in the structural and functional organization of complex networks which are shared by various natural, social and technological systems. This review examines these principles as applied to the organization, development and function of complex brain networks. Specifically, we examine the structural properties of large-scale anatomical and functional brain networks and discuss how they might arise in the course of network growth and rewiring. Moreover, we examine the relationship between the structural substrate of neuroanatomy and more dynamic functional and effective connectivity patterns that underlie human cognition. We suggest that network analysis offers new fundamental insights into global and integrative aspects of brain function, including the origin of flexible and coherent cognitive states within the neural architecture.',\n",
       " 'Motifs in brain networks.. Complex brains have evolved a highly efficient network architecture whose structural connectivity is capable of generating a large repertoire of functional states. We detect characteristic network building blocks ( structural and functional motifs) in neuroanatomical data sets and identify a small set of structural motifs that occur in significantly increased numbers. Our analysis suggests the hypothesis that brain networks maximize both the number and the diversity of functional motifs, while the repertoire of structural motifs remains small. Using functional motif number as a cost function in an optimization algorithm, we obtain network topologies that resemble real brain networks across a broad spectrum of structural measures, including small-world attributes. These results are consistent with the hypothesis that highly evolved neural architectures are organized to maximize functional repertoires and to support highly efficient integration of information.',\n",
       " 'Diffusion on Complex Networks : A way to probe their large scale topological structures. A diffusion process on complex networks is introduced in order to uncover their large scale topological structures. This is achieved by focusing on the slowest decaying diffusive modes of the network. The proposed procedure is applied to real-world networks like a friendship network of known modular structure, and an Internet routing network. For the friendship network, its known structure is well reproduced. In case of the Internet, where the structure is far less well-known, one indeed finds a modular structure, and modules can roughly be associated with individual countries. Quantitatively the modular structure of the Internet manifests itself in an approximately 10 times larger participation ratio of its slowest decaying modes as compared to the null model -- a random scale-free network. The extreme edges of the Internet are found to correspond to Russian and US military sites.',\n",
       " 'Topological Generalizations of network motifs. Biological and technological networks contain patterns, termed network motifs, which occur far more often than in randomized networks. {N}etwork motifs were suggested to be elementary building blocks that carry out key functions in the network. {I}t is of interest to understand how network motifs combine to form larger structures. {T}o address this, we present a systematic approach to define \"motif generalizations\": families of motifs of different sizes that share a common architectural theme. {T}o define motif generalizations, we first define \"roles\" in a subgraph according to structural equivalence. {F}or example, the feedforward loop triad--a motif in transcription, neuronal, and some electronic networks--has three roles: an input node, an output node, and an internal node. {T}he roles are used to define possible generalizations of the motif. {T}he feedforward loop can have three simple generalizations, based on replicating each of the three roles and their connections. {W}e present algorithms for efficiently detecting motif generalizations. {W}e find that the transcription networks of bacteria and yeast display only one of the three generalizations, the multi-output feedforward generalization. {I}n contrast, the neuronal network of {C}. elegans mainly displays the multi-input generalization. {F}orward-logic electronic circuits display a multi-input, multi-output hybrid. {T}hus, networks which share a common motif can have very different generalizations of that motif. {U}sing mathematical modeling, we describe the information processing functions of the different motif generalizations in transcription, neuronal, and electronic networks.',\n",
       " \"Collective dynamics of 'small-world' networks.. Networks of coupled dynamical systems have been used to model biological oscillators1, 2, 3, 4, Josephson junction arrays5,6, excitable media7, neural networks8, 9, 10, spatial games11, genetic control networks12 and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. But many biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks 'rewired' to introduce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them 'small-world' networks, by analogy with the small-world phenomenon13,14 (popularly known as six degrees of separation15). The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices.\",\n",
       " 'Network motifs: simple building blocks of complex networks.. Complex networks are studied across many fields of science. {T}o uncover their structural design principles, we defined \"network motifs,\" patterns of interconnections occurring in complex networks at numbers that are significantly higher than those in randomized networks. {W}e found such motifs in networks from biochemistry, neurobiology, ecology, and engineering. {T}he motifs shared by ecological food webs were distinct from the motifs shared by the genetic networks of {E}scherichia coli and {S}accharomyces cerevisiae or from those found in the {W}orld {W}ide {W}eb. {S}imilar motifs were found in networks that perform information processing, even though they describe elements as different as biomolecules within a cell and synaptic connections between neurons in {C}aenorhabditis elegans. {M}otifs may thus define universal classes of networks. {T}his approach may uncover the basic building blocks of most networks.',\n",
       " 'Authoritative sources in a hyperlinked environment. The network structure of a hyperlinked environment can be a rich source of in- formation about the content of the environment, provided we have e\\x0bective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their e\\x0bectiveness in a variety of contexts on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of \\\\authoritative\" information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of \\\\hub pages\" that join them together in the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristics for link-based analysis.  We began with the goal of discovering authoritative pages, but our approach in fact identi\\x0ces a more complex pattern of social organization on the www, in which hub pages link densely to a set of thematically related authorities. This equilibrium between hubs and authorities is a phenomenon that recurs in the context of a wide variety of topics on the www. Measures of impact and influence in bibliometrics have typically lacked, and arguably not required, an analogous formulation of the role that hubs play; the www is very di\\x0berent from the scienti\\x0cc literature, and our framework seems appropriate as a model of the way in which authority is conferred in an environment such as the Web.',\n",
       " 'The anatomy of a large-scale hypertextual Web search engine. Abstract In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http:// google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical largescale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.',\n",
       " 'Spectra and eigenvectors of scale-free networks.. We study the spectra and eigenvectors of the adjacency matrices of scale-free networks when bidirectional interaction is allowed, so that the adjacency matrix is real and symmetric. The spectral density shows an exponential decay around the center, followed by power-law long tails at both spectrum edges. The largest eigenvalue lambda1 depends on system size N as lambda1 approximately N1/4 for large N, and the corresponding eigenfunction is strongly localized at the hub, the vertex with largest degree. The component of the normalized eigenfunction at the hub is of order unity. We also find that the mass gap scales as N(-0.68).',\n",
       " \"Spectra of random graphs with given expected degrees. In the study of the spectra of power-law graphs, there are basically two  competing approaches. One is to prove analogues of Wigner's semicircle law,  whereas the other predicts that the eigenvalues follow a power-law  distribution. Although the semicircle law and the power law have nothing in  common, we will show that both approaches are essentially correct if one  considers the appropriate matrices. We will prove that (under certain mild  conditions) the eigenvalues of the (normalized) Laplacian of a random  power-law graph follow the semicircle law, whereas the spectrum of the  adjacency matrix of a power-law graph obeys the power law. Our results are  based on the analysis of random graphs with given expected degrees and their  relations to several key invariants. Of interest are a number of (new) values  for the exponent β, where phase transitions for eigenvalue distributions  occur. The spectrum distributions have direct implications to numerous graph  algorithms such as, for example, randomized algorithms that involve rapidly  mixing Markov chains.\",\n",
       " 'Functional discovery via a compendium of expression profiles.. Ascertaining the impact of uncharacterized perturbations on the cell is a fundamental problem in biology. Here, we describe how a single assay can be used to monitor hundreds of different cellular functions simultaneously. We constructed a reference database or “compendium” of expression profiles corresponding to 300 diverse mutations and chemical treatments in S. cerevisiae , and we show that the cellular pathways affected can be determined by pattern matching, even among very subtle profiles. The utility of this approach is validated by examining profiles caused by deletions of uncharacterized genes: we identify and experimentally confirm that eight uncharacterized open reading frames encode proteins required for sterol metabolism, cell wall function, mitochondrial respiration, or protein synthesis. We also show that the compendium can be used to characterize pharmacological perturbations by identifying a novel target of the commonly used drug dyclonine.',\n",
       " 'Growing and navigating the small world Web by local content.. Can we model the scale-free distribution of Web hypertext degree under realistic assumptions about the behavior of page authors? Can a Web crawler efficiently locate an unknown relevant page? These questions are receiving much attention due to their potential impact for understanding the structure of the Web and for building better search engines. Here I investigate the connection between the linkage and content topology of Web pages. The relationship between a text-induced distance metric and a link-based neighborhood probability distribution displays a phase transition between a region where linkage is not determined by content and one where linkage decays according to a power law. This relationship is used to propose a Web growth model that is shown to accurately predict the distribution of Web page degree, based on textual content and assuming only local knowledge of degree for existing pages. A qualitatively similar phase transition is found between linkage and semantic distance, with an exponential decay tail. Both relationships suggest that efficient paths can be discovered by decentralized Web navigation algorithms based on textual and/or categorical cues.',\n",
       " 'The structure of scientific collaboration networks. The structure of scientific collaboration networks is investigated. Two scientists are considered connected if they have authored a paper together and explicit networks of such connections are constructed by using data drawn from a number of databases, including MEDLINE (biomedical research), the Los Alamos e-Print Archive (physics), and NCSTRL (computer science). I show that these collaboration networks form â\\x80\\x9csmall worlds,â\\x80\\x9d in which randomly chosen pairs of scientists are typically separated by only a short path of intermediate acquaintances. I further give results for mean and distribution of numbers of collaborators of authors, demonstrate the presence of clustering in the networks, and highlight a number of apparent differences in the patterns of collaboration between the fields studied.',\n",
       " 'Classes of small-world networks. We study the statistical properties of a variety of diverse real-world networks. We present evidence of the occurrence of three classes of small-world networks: (a) scale-free networks, characterized by a vertex connectivity distribution that decays as a power law; (b) broad-scale networks, characterized by a connectivity distribution that has a power law regime followed by a sharp cutoff; and (c) single-scale networks, characterized by a connectivity distribution with a fast decaying tail. Moreover. we note for the classes of broad-scale and single-scale networks that there are constraints limiting the addition of new links. Our results suggest that the nature of such constraints may be the controlling factor for the emergence of different classes of networks.',\n",
       " 'Spectra of \"real-world\" graphs: beyond the semicircle law.. Many natural and social systems develop complex networks, that are usually modelled as random graphs. The eigenvalue spectrum of these graphs provides information about their structural properties. While the semi-circle law is known to describe the spectral density of uncorrelated random graphs, much less is known about the eigenvalues of real-world graphs, describing such complex systems as the Internet, metabolic pathways, networks of power stations, scientific collaborations or movie actors, which are inherently correlated and usually very sparse. An important limitation in addressing the spectra of these systems is that the numerical determination of the spectra for systems with more than a few thousand nodes is prohibitively time and memory consuming. Making use of recent advances in algorithms for spectral characterization, here we develop new methods to determine the eigenvalues of networks comparable in size to real systems, obtaining several surprising results on the spectra of adjacency matrices corresponding to models of real-world graphs. We find that when the number of links grows as the number of nodes, the spectral density of uncorrelated random graphs does not converge to the semi-circle law. Furthermore, the spectral densities of real-world graphs have specific features depending on the details of the corresponding models. In particular, scale-free graphs develop a triangle-like spectral density with a power law tail, while small-world graphs have a complex spectral density function consisting of several sharp peaks. These and further results indicate that the spectra of correlated graphs represent a practical tool for graph classification and can provide useful insight into the relevant structural properties of real networks.',\n",
       " 'Multistability in the lactose utilization network of Escherichia coli. Multistability, the capacity to achieve multiple internal states in response to a single set of external inputs, is the defining characteristic of a switch. Biological switches are essential for the determination of cell fate in multicellular organisms1, the regulation of cell-cycle oscillations during mitosis2, 3 and the maintenance of epigenetic traits in microbes4. The multistability of several natural1, 2, 3, 4, 5, 6 and synthetic7, 8, 9 systems has been attributed to positive feedback loops in their regulatory networks10. However, feedback alone does not guarantee multistability. The phase diagram of a multistable system, a concise description of internal states as key parameters are varied, reveals the conditions required to produce a functional switch11, 12. Here we present the phase diagram of the bistable lactose utilization network of Escherichia coli13. We use this phase diagram, coupled with a mathematical model of the network, to quantitatively investigate processes such as sugar uptake and transcriptional regulation in vivo. We then show how the hysteretic response of the wild-type system can be converted to an ultrasensitive graded response14, 15. The phase diagram thus serves as a sensitive probe of molecular interactions and as a powerful tool for rational network design.',\n",
       " 'Protein complexes and functional modules in molecular networks. Proteins, nucleic acids, and small molecules form a dense network of molecular interactions in a cell. Molecules are nodes of this network, and the interactions between them are edges. The architecture of molecular networks can reveal important principles of cellular organization and function, similarly to the way that protein structure tells us about the function and organization of a protein. Computational analysis of molecular networks has been primarily concerned with node degree [Wagner, A. & Fell, D. A. (2001) Proc. R. Soc. London Ser. B 268, 1803-1810; Jeong, H., Tombor, B., Albert, R., Oltvai, Z. N. & Barabasi, A. L. (2000) Nature 407, 651-654] or degree correlation [Maslov, S. & Sneppen, K. (2002) Science 296, 910-913], and hence focused on single/two-body properties of these networks. Here, by analyzing the multibody structure of the network of protein-protein interactions, we discovered molecular modules that are densely connected within themselves but sparsely connected with the rest of the network. Comparison with experimental data and functional annotation of genes showed two types of modules: (i) protein complexes (splicing machinery, transcription factors, etc.) and (ii) dynamic functional units (signaling cascades, cell-cycle regulation, etc.). Discovered modules are highly statistically significant, as is evident from comparison with random graphs, and are robust to noise in the data. Our results provide strong support for the network modularity principle introduced by Hartwell et al. [Hartwell, L. H., Hopfield, J. J., Leibler, S. & Murray, A. W. (1999) Nature 402, C47-C52], suggesting that found modules constitute the \"building blocks\" of molecular networks.',\n",
       " 'Finding and evaluating community structure in networks. We propose and study a set of algorithms for discovering community structure in networks\\x97natural divisions of network nodes into densely connected subgroups. Our algorithms all share two definitive features: first, they involve iterative removal of edges from the network to split it into communities, the edges removed being identified using any one of a number of possible \"betweenness\" measures, and second, these measures are, crucially, recalculated after each removal. We also propose a measure for the strength of the community structure found by our algorithms, which gives us an objective metric for choosing the number of communities into which a network should be divided. We demonstrate that our algorithms are highly effective at discovering community structure in both computer-generated and real-world network data, and show how they can be used to shed light on the sometimes dauntingly complex structure of networked systems.We propose and study a set of algorithms for discovering community structure in networks\\x97natural divisions of network nodes into densely connected subgroups. Our algorithms all share two definitive features: first, they involve iterative removal of edges from the network to split it into communities, the edges removed being identified using any one of a number of possible \"betweenness\" measures, and second, these measures are, crucially, recalculated after each removal. We also propose a measure for the strength of the community structure found by our algorithms, which gives us an objective metric for choosing the number of communities into which a network should be divided. We demonstrate that our algorithms are highly effective at discovering community structure in both computer-generated and real-world network data, and show how they can be used to shed light on the sometimes dauntingly complex structure of networked systems.',\n",
       " 'The structure and function of complex networks. Inspired by empirical studies of networked systems such as the Internet, social networks, and bio- logical networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.',\n",
       " 'Matching words and pictures. We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-moda  and correspondence extensions to Hofmann’s hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data.',\n",
       " 'An overview of audio information retrieval. Abstract.   The problem of audio information retrieval is familiar to anyone who has returned from vacation to find an answering machine full of messages. While there is not yet an ?AltaVista? for the audio data type, many workers are finding ways to automatically locate, index, and browse audio using recent advances in speech recognition and machine listening. This paper reviews the state of the art in audio information retrieval, and presents recent advances in automatic speech recognition, word spotting, speaker and music identification, and audio similarity with a view towards making audio less ?opaque?. A special section addresses intelligent interfaces for navigating and browsing audio and multimedia documents, using automatically derived information to go beyond the tape recorder metaphor.',\n",
       " \"Programmed population control by cell–cell communication and regulated killing. De novo engineering of gene circuits inside cells is extremely difficult1, 2, 3, 4, 5, 6, 7, 8, 9, and efforts to realize predictable and robust performance must deal with noise in gene expression and variation in phenotypes between cells10, 11, 12. Here we demonstrate that by coupling gene expression to cell survival and death using cell–cell communication, we can programme the dynamics of a population despite variability in the behaviour of individual cells. Specifically, we have built and characterized a 'population control' circuit that autonomously regulates the density of an Escherichia coli population. The cell density is broadcasted and detected by elements from a bacterial quorum-sensing system13, 14, which in turn regulate the death rate. As predicted by a simple mathematical model, the circuit can set a stable steady state in terms of cell density and gene expression that is easily tunable by varying the stability of the cell–cell communication signal. This circuit incorporates a mechanism for programmed death in response to changes in the environment, and allows us to probe the design principles of its more complex natural counterparts.\",\n",
       " 'Mobility in collaboration. This paper addresses an issue that has received little attention within CSCW - the requirements to support mobility within collaborative activities. By examining three quite different settings each with differing technological support, we examine the ways in which mobility is critical to collaborative work. We suggest that taking mobility seriously may not only contribute our understanding of current support for collaboration, but raise more general issues concerning the requirements for mobile and other technologies.',\n",
       " 'The sociocognitive psychology of computer-mediated communication: the present and future of technology-based interactions.. The increased diffusion of the Internet has made computer-mediated communication (CMC) very popular. However, a difficult question arises for psychologists and communication researchers: \"What are the communicative characteristics of CMC?\" According to the \"cues-filtered-out\" approach, CMC lacks the specifically relational features (social cues), which enable the interlocutors to identify correctly the kind of interpersonal situations they find themselves in. This paper counters this vision by integrating in its theoretical frame the different psycho-social approaches available in current literature. In particular, the paper describes the characteristics of the socio-cognitive processes-emotional expression, context definition, and identity creation-used by the interlocutors to make order and create relationships out of the miscommunication processes typical of CMC. Moreover, it presents the emerging forms of CMC-instant messaging, shared hypermedia, weblogs, and graphical chats-and their possible social and communicative effects.',\n",
       " 'Oscillations in NF-{kappa}B Signaling Control the Dynamics of Gene Expression. Signaling by the transcription factor nuclear factor kappa B (NF-{kappa}B) involves its release from inhibitor kappa B (I{kappa}B) in the cytosol, followed by translocation into the nucleus. NF-{kappa}B regulation of I{kappa}B{alpha} transcription represents a delayed negative feedback loop that drives oscillations in NF-{kappa}B translocation. Single-cell time-lapse imaging and computational modeling of NF-{kappa}B (RelA) localization showed asynchronous oscillations following cell stimulation that decreased in frequency with increased I{kappa}B{alpha} transcription. Transcription of target genes depended on oscillation persistence, involving cycles of RelA phosphorylation and dephosphorylation. The functional consequences of NF-{kappa}B signaling may thus depend on number, period, and amplitude of oscillations. 10.1126/science.1099962',\n",
       " 'Design of artificial cell-cell communication using gene and metabolic networks.. Artificial transcriptional networks have been used to achieve novel, nonnative behavior in bacteria. {T}ypically, these artificial circuits are isolated from cellular metabolism and are designed to function without intercellular communication. {T}o attain concerted biological behavior in a population, synchronization through intercellular communication is highly desirable. {H}ere we demonstrate the design and construction of a gene-metabolic circuit that uses a common metabolite to achieve tunable artificial cell-cell communication. {T}his circuit uses a threshold concentration of acetate to induce gene expression by acetate kinase and part of the nitrogen-regulation two-component system. {A}s one application of the cell-cell communication circuit we created an artificial quorum sensor. {E}ngineering of carbon metabolism in {E}scherichia coli made acetate secretion proportional to cell density and independent of oxygen availability. {I}n these cells the circuit induced gene expression in response to a threshold cell density. {T}his threshold can be tuned effectively by controlling {D}eltap{H} over the cell membrane, which determines the partition of acetate between medium and cells. {M}utagenesis of the enhancer sequence of the gln{A}p2 promoter produced variants of the circuit with changed sensitivity demonstrating tunability of the circuit by engineering of its components. {T}he behavior of the circuit shows remarkable predictability based on a mathematical design model.',\n",
       " 'Initial sequencing and comparative analysis of the mouse genome.. The sequence of the mouse genome is a key informational tool for understanding the contents of the human genome and a key experimental tool for biomedical research. Here, we report the results of an international collaboration to produce a high-quality draft sequence of the mouse genome. We also present an initial comparative analysis of the mouse and human genomes, describing some of the insights that can be gleaned from the two sequences. We discuss topics including the analysis of the evolutionary forces shaping the size, structure and sequence of the genomes; the conservation of large-scale synteny across most of the genomes; the much lower extent of sequence orthology covering less than half of the genomes; the proportions of the genomes under selection; the number of protein-coding genes; the expansion of gene families related to reproduction and immunity; the evolution of proteins; and the identification of intraspecies polymorphism.',\n",
       " 'Dynamic conditional random fields. Conditional random fields (CRFs) for sequence modeling have several  advantages over joint models such as HMMs, including the ability to  relax strong independence assumptions made in those models, and the  ability to incorporate arbitrary overlapping features. Previous work has  focused on linear-chain CRFs, which correspond to finite-state machines,  and have efficient exact inference algorithms. Often, however, we wish  to label sequence data in multiple interacting ways---for example,...',\n",
       " 'Large N Field Theories, String Theory and Gravity. We review the holographic correspondence between field theories and string/M theory, focusing on the relation between compactifications of string/M theory on Anti-de Sitter spaces and conformal field theories. We review the background for this correspondence and discuss its motivations and the evidence for its correctness. We describe the main results that have been derived from the correspondence in the regime that the field theory is approximated by classical or semiclassical gravity. We focus on the case of the N=4 supersymmetric gauge theory in four dimensions, but we discuss also field theories in other dimensions, conformal and non-conformal, with or without supersymmetry, and in particular the relation to QCD. We also discuss some implications for black hole physics.',\n",
       " 'The large-scale organization of metabolic networks.. In a cell or microorganism, the processes that generate mass, energy, information transfer and cell-fate specification are seamlessly integrated through a complex network of cellular constituents and reactions. {H}owever, despite the key role of these networks in sustaining cellular functions, their large-scale structure is essentially unknown. {H}ere we present a systematic comparative mathematical analysis of the metabolic networks of 43 organisms representing all three domains of life. {W}e show that, despite significant variation in their individual constituents and pathways, these metabolic networks have the same topological scaling properties and show striking similarities to the inherent organization of complex non-biological systems. {T}his may indicate that metabolic organization is not only identical for all living organisms, but also complies with the design principles of robust and error-tolerant scale-free networks, and may represent a common blueprint for the large-scale organization of interactions among all cellular constituents.',\n",
       " \"Functional and topological characterization of protein interaction networks.. The elucidation of the cell's large-scale organization is a primary challenge for post-genomic biology, and understanding the structure of protein interaction networks offers an important starting point for such studies. We compare four available databases that approximate the protein interaction network of the yeast, Saccharomyces cerevisiae, aiming to uncover the network's generic large-scale properties and the impact of the proteins' function and cellular localization on the network topology. We show how each database supports a scale-free, topology with hierarchical modularity, indicating that these features represent a robust and generic property of the protein interactions network. We also find strong correlations between the network's structure and the functional role and subcellular localization of its protein constituents, concluding that most functional and/or localization classes appear as relatively segregated subnetworks of the full protein interaction network. The uncovered systematic differences between the four protein interaction databases reflect their relative coverage for different functional and localization classes and provide a guide for their utility in various bioinformatics studies.\",\n",
       " 'XML, bioinformatics and data integration.. Motivation: The eXtensible Markup Language (XML) is an emerging standard for structuring documents, notably for the World Wide Web. In this paper, the authors present XML and examine its use as a data language for bioinformatics. In particular, XML is compared to other languages, and some of the potential uses of XML in bioinformatics applications are presented. The authors propose to adopt XML for data interchange between databases and other sources of data. Finally the discussion is illustrated by a test case of a pedigree data model in XML. Contact: Emmanuel.Barillot@infobiogen.fr',\n",
       " 'Systems Biology: A Brief Overview. To understand biology at the system level, we must examine the structure and dynamics of cellular and organismal function, rather than the char- acteristics of isolated parts of a cell or organism. Properties of systems, such as robustness, emerge as central issues, and understanding these properties may have an impact on the future of medicine. However, many breakthroughs in experimental devices, advanced software, and analytical methods are required before the achievements of systems biology can live up to their much-touted potential.',\n",
       " 'Computational systems biology. Book Description Systems Biology is concerned with the quantitative study of complex biosystems at the molecular, cellular, tissue, and systems scales. Its focus is on the function of the system as a whole, rather than on individual parts. This exciting new arena applies mathematical modeling and engineering methods to the study of biological systems. This book is the first of its kind to focus on the newly emerging field of systems biology with an emphasis on computational approaches. The work covers new concepts, methods for information storage, mining and knowledge extraction, reverse engineering of gene and metabolic networks, as well as modelling and simulation of multi-cellular systems. Central themes include strategies for predicting biological properties and methods for elucidating structure-function relationships. From the Back Cover Computational systems biology is a field that aims to develop a systems level understanding of biological processes using computational techniques. This book presents a timely compendium of the state-of-the-art in this new era of biological understanding. A distinguished group of contributors reviews bioinformation engineering and data integration technologies along with biomedical applications. Different computational approaches to model and simulate biological systems are discussed. Current computational research efforts, comprehensively covered in this volume, are focused on regulatory, signaling and metabolic networks. As an exciting outlook new concepts for computer representations on multi-scales predicting emergent properties of biological systems are introduced. Andres Kriete is Associate Professor for Bioinformation Engineering at Drexel University, Philadelphia and Director of the Biocomputing Laboratory at the Coriell Institute for Medical Research. Roland Eils is Professor of Bioinformatics at the University of Heidelberg and Director of the Division of Theoretical Bioinformatics at the German Cancer Research Center (DKFZ) in Heidelberg.',\n",
       " 'Mapping weblog communities. Websites of a particular class form increasingly complex networks, and new tools are needed to map and understand them. A way of visualizing this complex network is by mapping it. A map highlights which members of the community have similar interests, and reveals the underlying social network. In this paper, we will map a network of websites using Kohonen\\'s self-organizing map (SOM), a neural-net like method generally used for clustering and visualization of complex data sets. The set of websites considered has been the Blogalia weblog hosting site (based at <A HREF=\"http://www.blogalia.com/\">this http URL</A>), a thriving community of around 200 members, created in January 2002. In this paper we show how SOM discovers interesting community features, its relation with other community-discovering algorithms, and the way it highlights the set of communities formed over the network.',\n",
       " 'Understanding mobile contexts. Mobile urban environments present a challenge for context-aware computers because they differ from fixed indoor contexts such as offices, meeting rooms, and lecture halls in many important ways. Internal factors such as tasks and goals are different—external factors such as social resources are dynamic and unpredictable. An empirical, user-centred approach is needed to understand mobile contexts. In this paper, we present insights from an ethnomethodologically inspired study of 25 adult urbanites in Helsinki. The results describe typical phenomena in mobility: how situational and planned acts intermesh in navigation, how people construct personal and group spaces, and how temporal tensions develop and dissolve. Furthermore, we provide examples of social solutions to navigation problems, examine mobile multitasking, and consider design implications for mobile and context-aware human–computer interaction.',\n",
       " \"Shortest paths and load scaling in scale-free trees.. Szab&oacute;, Alava, and Kert&eacute;sz [Phys. Rev. E 66, 026101 (2002)] considered two questions about the scale-free random tree given by the m=1 case of the Barab&aacute;si-Albert (BA) model (identical with a random tree model introduced by Szymanski in 1987): what is the distribution of the node to node distances, and what is the distribution of node loads, where the load on a node is the number of shortest paths passing through it? They gave heuristic answers to these questions using a ``mean-field'' approximation, replacing the random tree by a certain fixed tree with carefully chosen branching ratios. By making use of our earlier results on scale-free random graphs, we shall analyze the random tree rigorously, obtaining and proving very precise answers to these questions. We shall show that, after dividing by N (the number of nodes), the load distribution converges to an integer distribution X with Pr(X=c)=2/[(2c+1)(2c+3)], c=0,1,2,..., confirming the asymptotic power law with exponent -2 predicted by Szab&oacute;, Alava, and Kert&eacute;sz. For the distribution of node-node distances, we show asymptotic normality, and give a precise form for the (far from normal) large deviation law. We note that the mean-field methods used by Szab&oacute;, Alava, and Kert&eacute;sz give very good results for this model.\",\n",
       " \"Network biology: understanding the cell's functional organization.. A key aim of postgenomic biomedical research is to systematically catalogue all molecules and their interactions within a living cell. There is a clear need to understand how these molecules and the interactions between them determine the function of this enormously complex machinery, both in isolation and when surrounded by other cells. Rapid advances in network biology indicate that cellular networks are governed by universal laws and offer a new conceptual framework that could potentially revolutionize our view of biology and disease pathologies in the twenty-first century.\",\n",
       " 'The small world of human language.. Words in human language interact in sentences in non-random ways, and allow humans to construct an astronomic variety of sentences from a limited number of discrete units. This construction process is extremely fast and robust. The co-occurrence of words in sentences reflects language organization in a subtle manner that can be described in terms of a graph of word interactions. Here, we show that such graphs display two important features recently found in a disparate number of complex systems. (i) The so called small-world effect. In particular, the average distance between two words, d (i.e. the average minimum number of links to be crossed from an arbitrary word to another), is shown to be d approximately equal to 2-3, even though the human brain can store many thousands. (ii) A scale-free distribution of degrees. The known pronounced effects of disconnecting the most connected vertices in such networks can be identified in some language disorders. These observations indicate some unexpected features of language organization that might reflect the evolutionary and social history of lexicons and the origins of their flexibility and combinatorial nature.',\n",
       " 'Metabolomics and systems biology: making sense of the soup.. Novel techniques for acquiring metabolomics data continue to emerge. Such data require proper storage in suitably configured databases, which then permit one to establish the size of microbial metabolomes (hundreds of major metabolites) and allow the nature, organisation and control of metabolic networks to be investigated. A variety of algorithms for metabolic network reconstruction coupled to suitable modelling algorithms are the ground substances for the development of metabolic network and systems biology. Even qualitative models of metabolic networks, when subject to stoichiometric constraints, can prove highly informative, and are the first step to the quantitative models, which alone can allow the true representation of complex biochemical systems.',\n",
       " 'Genomic analysis of regulatory network dynamics reveals large topological changes. Network analysis has been applied widely, providing a unifying language to describe disparate systems ranging from social interactions to power grids. It has recently been used in molecular biology, but so far the resulting networks have only been analysed statically1, 2, 3, 4, 5, 6, 7, 8. Here we present the dynamics of a biological network on a genomic scale, by integrating transcriptional regulatory information9, 10, 11 and gene-expression data12, 13, 14, 15, 16 for multiple conditions in Saccharomyces cerevisiae. We develop an approach for the statistical analysis of network dynamics, called SANDY, combining well-known global topological measures, local motifs and newly derived statistics. We uncover large changes in underlying network architecture that are unexpected given current viewpoints and random simulations. In response to diverse stimuli, transcription factors alter their interactions to varying degrees, thereby rewiring the network. A few transcription factors serve as permanent hubs, but most act transiently only during certain conditions. By studying sub-network structures, we show that environmental responses facilitate fast signal propagation (for example, with short regulatory cascades), whereas the cell cycle and sporulation direct temporal progression through multiple stages (for example, with highly inter-connected transcription factors). Indeed, to drive the latter processes forward, phase-specific transcription factors inter-regulate serially, and ubiquitously active transcription factors layer above them in a two-tiered hierarchy. We anticipate that many of the concepts presented here—particularly the large-scale topological changes and hub transience—will apply to other biological networks, including complex sub-systems in higher eukaryotes.',\n",
       " \"Fluctuations in network dynamics.. Most complex networks serve as conduits for various dynamical processes, ranging from mass transfer by chemical reactions in the cell to packet transfer on the Internet. We collected data on the time dependent activity of five natural and technological networks, finding that for each the coupling of the flux fluctuations with the total flux on individual nodes obeys a unique scaling law. We show that the observed scaling can explain the competition between the system's internal collective dynamics and changes in the external environment, allowing us to predict the relevant scaling exponents.\",\n",
       " 'Analysis of weighted networks. The connections in many networks are not merely binary entities, either present or not, but have associated weights that record their strengths relative to one another. Recent studies of networks have, by and large, steered clear of such weighted networks, which are often perceived as being harder to analyze than their unweighted counterparts. Here we point out that weighted networks can in many cases be analyzed using a simple mapping from a weighted network to an unweighted multigraph, allowing us to apply standard techniques for unweighted graphs to weighted ones as well. We give a number of examples of the method, including an algorithm for detecting community structure in weighted networks and a simple proof of the maximum-flow–minimum-cut theorem.',\n",
       " 'Inferring Network Mechanisms: The Drosophila melanogaster Protein Interaction Network. Naturally occurring networks exhibit quantitative features revealing underlying growth mechanisms. Numerous network mechanisms have recently been proposed to reproduce specific properties such as degree distributions or clustering coefficients. We present a method for inferring the mechanism most accurately capturing a given network topology, exploiting discriminative tools from machine learning. The Drosophila melanogaster protein network is confidently and robustly (to noise and training data subsampling) classified as a duplication{\\\\^a}\\x80\\x93mutation{\\\\^a}\\x80\\x93complementation network over preferential attachment, small-world, and a duplication{\\\\^a}\\x80\\x93mutation mechanism without complementation. Systematic classification, rather than statistical study of specific properties, provides a discriminative approach to understand the design of complex networks.',\n",
       " 'Metabolomics by numbers: acquiring and understanding global metabolite data. In this postgenomic era, there is a specific need to assign function to orphan genes in order to validate potential targets for drug therapy and to discover new biomarkers of disease. Metabolomics is an emerging field that is complementary to the other ‘omics and proving to have unique advantages. As in transcriptomics or proteomics, a typical metabolic fingerprint or metabolomic experiment is likely to generate thousands of data points, of which only a handful might be needed to describe the problem adequately. Extracting the most meaningful elements of these data is thus key to generating useful new knowledge with mechanistic or explanatory power.',\n",
       " 'Functional genomic hypothesis generation and experimentation by a robot scientist. The question of whether it is possible to automate the scientific process is of both great theoretical interest1, 2 and increasing practical importance because, in many scientific areas, data are being generated much faster than they can be effectively analysed. We describe a physically implemented robotic system that applies techniques from artificial intelligence3, 4, 5, 6, 7, 8 to carry out cycles of scientific experimentation. The system automatically originates hypotheses to explain observations, devises experiments to test these hypotheses, physically runs the experiments using a laboratory robot, interprets the results to falsify hypotheses inconsistent with the data, and then repeats the cycle. Here we apply the system to the determination of gene function using deletion mutants of yeast (Saccharomyces cerevisiae) and auxotrophic growth experiments9. We built and tested a detailed logical model (involving genes, proteins and metabolites) of the aromatic amino acid synthesis pathway. In biological experiments that automatically reconstruct parts of this model, we show that an intelligent experiment selection strategy is competitive with human performance and significantly outperforms, with a cost decrease of 3-fold and 100-fold (respectively), both cheapest and random-experiment selection.',\n",
       " 'Here is the evidence, now what is the hypothesis? The complementary roles of inductive and hypothesis-driven science in the post-genomic era. Abstract It is considered in some quarters that hypothesis-driven methods are the only valuable, reliable or significant means of scientific advance. Data-driven or ‘inductive’ advances in scientific knowledge are then seen as marginal, irrelevant, insecure or wrong-headed, while the development of technology—which is not of itself ‘hypothesis-led’ (beyond the recognition that such tools might be of value)—must be seen as equally irrelevant to the hypothetico-deductive scientific agenda. We argue here that data- and technology-driven programmes are not alternatives to hypothesis-led studies in scientific knowledge discovery but are complementary and iterative partners with them. Many fields are data-rich but hypothesis-poor. Here, computational methods of data analysis, which may be automated, provide the means of generating novel hypotheses, especially in the post-genomic era. BioEssays 26:99–105, 2004. © 2003 Wiley Periodicals, Inc.',\n",
       " \"High-throughput classification of yeast mutants for functional genomics using metabolic footprinting.. Many technologies have been developed to help explain the function of genes discovered by systematic genome sequencing. At present, transcriptome and proteome studies dominate large-scale functional analysis strategies. Yet the metabolome, because it is 'downstream', should show greater effects of genetic or physiological changes and thus should be much closer to the phenotype of the organism. We earlier presented a functional analysis strategy that used metabolic fingerprinting to reveal the phenotype of silent mutations of yeast genes. However, this is difficult to scale up for high-throughput screening. Here we present an alternative that has the required throughput (2 min per sample). This 'metabolic footprinting' approach recognizes the significance of 'overflow metabolism' in appropriate media. Measuring intracellular metabolites is time-consuming and subject to technical difficulties caused by the rapid turnover of intracellular metabolites and the need to quench metabolism and separate metabolites from the extracellular space. We therefore focused instead on direct, noninvasive, mass spectrometric monitoring of extracellular metabolites in spent culture medium. Metabolic footprinting can distinguish between different physiological states of wild-type yeast and between yeast single-gene deletion mutants even from related areas of metabolism. By using appropriate clustering and machine learning techniques, the latter based on genetic programming, we show that metabolic footprinting is an effective method to classify 'unknown' mutants by genetic defect.\",\n",
       " 'A systematic approach to modeling, capturing, and disseminating proteomics experimental data.. Both the generation and the analysis of proteome data are becoming increasingly widespread, and the field of proteomics is moving incrementally toward high-throughput approaches. Techniques are also increasing in complexity as the relevant technologies evolve. A standard representation of both the methods used and the data generated in proteomics experiments, analogous to that of the MIAME (minimum information about a microarray experiment) guidelines for transcriptomics, and the associated MAGE (microarray gene expression) object model and XML (extensible markup language) implementation, has yet to emerge. This hinders the handling, exchange, and dissemination of proteomics data. Here, we present a UML (unified modeling language) approach to proteomics experimental data, describe XML and SQL (structured query language) implementations of that model, and discuss capture, storage, and dissemination strategies. These make explicit what data might be most usefully captured about proteomics experiments and provide complementary routes toward the implementation of a proteome repository.',\n",
       " 'Schemes of flux control in a model of Saccharomyces cerevisiae glycolysis.. We used parameter scanning to emulate changes to the limiting rate for steps in a fitted model of glucose-derepressed yeast glycolysis. Three flux-control regimes were observed, two of which were under the dominant control of hexose transport, in accordance with various experimental studies and other model predictions. A third control regime in which phosphofructokinase exerted dominant glycolytic flux control was also found, but it appeared to be physiologically unreachable by this model, and all realistically obtainable flux control regimes featured hexose transport as a step involving high flux control.',\n",
       " \"Motifs, modules and games in bacteria.. Global explorations of regulatory network dynamics, organization and evolution have become tractable thanks to high-throughput sequencing and molecular measurement of bacterial physiology. {F}rom these, a nascent conceptual framework is developing, that views the principles of regulation in term of motifs, modules and games. {M}otifs are small, repeated, and conserved biological units ranging from molecular domains to small reaction networks. {T}hey are arranged into functional modules, genetically dissectible cellular functions such as the cell cycle, or different stress responses. {T}he dynamical functioning of modules defines the organism's strategy to survive in a game, pitting cell against cell, and cell against environment. {P}lacing pathway structure and dynamics into an evolutionary context begins to allow discrimination between those physical and molecular features that particularize a species to its surroundings, and those that provide core physiological function. {T}his approach promises to generate a higher level understanding of cellular design, pathway evolution and cellular bioengineering.\",\n",
       " 'Network dynamics and cell physiology. Complex assemblies of interacting proteins carry out most of the interesting jobs in a cell, such as metabolism, DNA synthesis, movement and information processing. These physiological properties play out as a subtle molecular dance, choreographed by underlying regulatory networks. To understand this dance, a new breed of theoretical molecular biologists reproduces these networks in computers and in the mathematical language of dynamical systems.',\n",
       " \"A synthetic oscillatory network of transcriptional regulators.. Networks of interacting biomolecules carry out many essential functions in living cells, but the 'design principles' underlying the functioning of such intracellular networks remain poorly understood, despite intensive efforts including quantitative analysis of relatively simple systems. {H}ere we present a complementary approach to this problem: the design and construction of a synthetic network to implement a particular function. {W}e used three transcriptional repressor systems that are not part of any natural biological clock to build an oscillating network, termed the repressilator, in {E}scherichia coli. {T}he network periodically induces the synthesis of green fluorescent protein as a readout of its state in individual cells. {T}he resulting oscillations, with typical periods of hours, are slower than the cell-division cycle, so the state of the oscillator has to be transmitted from generation to generation. {T}his artificial clock displays noisy behaviour, possibly because of stochastic fluctuations of its components. {S}uch 'rational network design may lead both to the engineering of new cellular behaviours and to an improved understanding of naturally occurring networks.\",\n",
       " \"Construction of a genetic toggle switch in Escherichia coli.. It has been proposed' that gene-regulatory circuits with virtually any desired property can be constructed from networks of simple regulatory elements. {T}hese properties, which include multistability and oscillations, have been found in specialized gene circuits such as the bacteriophage lambda switch and the {C}yanobacteria circadian oscillator. {H}owever, these behaviours have not been demonstrated in networks of non-specialized regulatory components. {H}ere we present the construction of a genetic toggle switch-a synthetic, bistable gene-regulatory network-in {E}scherichia coli and provide a simple theory that predicts the conditions necessary for bistability. {T}he toggle is constructed from any two repressible promoters arranged in a mutually inhibitory network. {I}t is flipped between stable states using transient chemical or thermal induction and exhibits a nearly ideal switching threshold. {A}s a practical device, the toggle switch forms a synthetic, addressable cellular memory unit and has implications for biotechnology, biocomputing and gene therapy.\",\n",
       " 'Engineering stability in gene networks by autoregulation.. The genetic and biochemical networks which underlie such things as homeostasis in metabolism and the developmental programs of living cells, must withstand considerable variations and random perturbations of biochemical parameters. {T}hese occur as transient changes in, for example, transcription, translation, and {RNA} and protein degradation. {T}he intensity and duration of these perturbations differ between cells in a population. {T}he unique state of cells, and thus the diversity in a population, is owing to the different environmental stimuli the individual cells experience and the inherent stochastic nature of biochemical processes (for example, refs 5 and 6). {I}t has been proposed, but not demonstrated, that autoregulatory, negative feedback loops in gene circuits provide stability, thereby limiting the range over which the concentrations of network components fluctuate. {H}ere we have designed and constructed simple gene circuits consisting of a regulator and transcriptional repressor modules in {E}scherichia coli and we show the gain of stability produced by negative feedback.',\n",
       " 'A functional genomics strategy that uses metabolome data to reveal the phenotype of silent mutations.. A large proportion of the 6,000 genes present in the genome of Saccharomyces cerevisiae, and of those sequenced in other organisms, encode proteins of unknown function. Many of these genes are \"silent,\" that is, they show no overt phenotype, in terms of growth rate or other fluxes, when they are deleted from the genome. We demonstrate how the intracellular concentrations of metabolites can reveal phenotypes for proteins active in metabolic regulation. Quantification of the change of several metabolite concentrations relative to the concentration change of one selected metabolite can reveal the site of action, in the metabolic network, of a silent gene. In the same way, comprehensive analyses of metabolite concentrations in mutants, providing \"metabolic snapshots,\" can reveal functions when snapshots from strains deleted for unstudied genes are compared to those deleted for known genes. This approach to functional analysis, using comparative metabolomics, we call FANCY—an abbreviation for functional analysis by co-responses in yeast.',\n",
       " 'Document co-organization in an online knowledge community. We introduce the concept of \"document co-organization\" and describe such a system. By document co-organization we mean that individuals are allowed to hierarchically organize documents personally and share their hierarchies with others, while the system generates a \"consensus\" hierarchy from these personal hierarchies, which provides a full, common, and emergent view of all documents. By allowing users to retrieve documents from their own organization (hierarchy), another user\\'s, the consensus hierarchy, or a time-based hierarchy, we provide access corresponding to different characteristics of knowledge tasks: they are personal, collective, social, and time-sensitive. In a class website experiment, we show that for a complex knowledge task, hierarchies are used more frequently than search. One surprising finding is how often students use others\\' personal hierarchies.',\n",
       " 'Inferring Web Communities from Link Topology. dag&amp;x.berkeley.edu kleinberQcs.cornell.edu The World Wide Web grows through a decentralized, almost anarchic process, and this has resulted in a large hyperlinked corpus without the kind of logical organiza-tion that can be built into more tradit,ionally-created hy-permedia. To extract, meaningful structure under such circumstances, we develop a notion of hyperlinked com-munities on the www t,hrough an analysis of the link topology. By invoking a simple, mathematically clean method for defining and exposing the structure of these communities, we are able to derive a number of themes: The communities can be viewed as containing a core of central, “authoritative ” pages linked tog&amp;her by “hub pages ” ; and they exhibit a natural type of hierarchical topic generalization that can be inferred directly from the pat,t,ern of linkage. Our investigation shows that al-though the process by which users of the Web create pages and links is very difficult to understand at a “lo-cal ” level, it results in a much greater degree of orderly high-level structure than has typically been assumed.',\n",
       " \"Footprints: History-Rich Tools for Information Foraging. Inspired by Hill and Hollan's original work [6], we have been developing a theory of interaction history and building tools to apply this theory to navigation in a complex information space. We have built a series of tools --- map, trails, annotations and signposts --- based on a physical-world navigation metaphor. These tools have been in use for over a year. Our user study involved a controlled browse task and showed that users were able to get the same amount of work done with significantly less effort.\",\n",
       " 'Information foraging. Information foraging theory is an approach to understanding how strategies and technologies for information seeking, gathering, and consumption are adapted to the flux of information in the environment. The theory assumes that people, when possible, will modify their strategies or the structure of the environment to maximize their rate of gaining valuable information. The theory is developed by (a) adaptation (rational) analysis of information foraging problems and (b) a detailed process model (adaptive control of thought in information foraging [ACT-IF]). The adaptation analysis develops (a) information patch models, which deal with time allocation and information filtering and enrichment activities in environments in which information is encountered in clusters; (b) information scent models, which address the identification of information value from proximal cues; and (c) information diet models, which address decisions about the selection and pursuit of information items. ACT-IF is instantiated as a production system model of people interacting with complex information technology. Humans actively seek, gather, share, and consume information to a degree unapproached by other organisms. Ours might properly be characterized as a species of informavores (Dennett, 1991). Our adaptive success depends to a large extent on a vast and complex',\n",
       " 'Exploiting generative models in discriminative classifiers. Generative probability models such as hidden Markov models providea principled way of treating missing information and dealingwith variable length sequences. On the other hand, discriminativemethods such as support vector machines enable us to constructflexible decision boundaries and often result in classification performancesuperior to that of the model based approaches. An idealclassifier should combine these two complementary approaches. Inthis paper, we develop a natural way of achieving this combinationby deriving kernel functions for use in discriminative methodssuch as support vector machines from generative probability models.We provide a theoretical justification for this combination aswell as demonstrate a substantial improvement in the classificationperformance in the context of DNA and protein sequence analysis.1 IntroductionSpeech, vision, text and biosequence data can be difficult to deal with in the contextof simple statistical classification problems. B...',\n",
       " 'Spreading Activation Models for Trust Propagation. Semantic Web endeavors have mainly focused on issues pertaining to knowledge representation and ontology design. However, besides understanding information metadata stated by subjects, knowing about their credibility becomes equally crucial. Hence, trust and trust metrics, conceived as computational means to evaluate trust relationships between individuals, come into play. Our major contributions to semantic Web trust management are twofold. First, we introduce our classification scheme for trust metrics along various axes and discuss advantages and drawbacks of existing approaches for semantic Web scenarios. Hereby, we devise our advocacy for local group trust metrics, guiding us to the second part which presents Appleseed, our novel proposal for local group trust computation. Compelling in its simplicity, Appleseed borrows many ideas from spreading activation models in psychology and relates their concepts to trust evaluation in an intuitive fashion.',\n",
       " \"Design and implementation of microarray gene expression markup language (MAGE-ML).. BACKGROUND: Meaningful exchange of microarray data is currently difficult because it is rare that published data provide sufficient information depth or are even in the same format from one publication to another. Only when data can be easily exchanged will the entire biological community be able to derive the full benefit from such microarray studies. RESULTS: To this end we have developed three key ingredients towards standardizing the storage and exchange of microarray data. First, we have created a minimal information for the annotation of a microarray experiment (MIAME)-compliant conceptualization of microarray experiments modeled using the unified modeling language (UML) named MAGE-OM (microarray gene expression object model). Second, we have translated MAGE-OM into an XML-based data format, MAGE-ML, to facilitate the exchange of data. Third, some of us are now using MAGE (or its progenitors) in data production settings. Finally, we have developed a freely available software tool kit (MAGE-STK) that eases the integration of MAGE-ML into end users' systems. CONCLUSIONS: MAGE will help microarray data producers and users to exchange information by providing a common platform for data exchange, and MAGE-STK will make the adoption of MAGE easier.\",\n",
       " 'Network reachability of real-world contact sequences. We use real-world contact sequences, time-ordered lists of contacts from one person to another, to study how fast information or disease can spread across network of contacts. Specifically we measure the reachability time -- the average shortest time for a series of contacts to spread information between a reachable pair of vertices (a pair where a chain of contacts exists leading from one person to the other) -- and the reachability ratio -- the fraction of reachable vertex pairs. These measures are studied using conditional uniform graph tests. We conclude, among other things, that the network reachability depends much on a core where the path lengths are short and communication frequent, that clustering of the contacts of an edge in time tend to decrease the reachability, and that the order of the contacts really do make sense for dynamical spreading processes.',\n",
       " 'A Theory of Problem-Solving Behavior. In this paper we develop a formal, testable theory of problem-solving behavior with special relevance to individuals and small groups. The theory is consistent with principles drawn from operant behavior and social exchange theories but also incorporates elements of cognitive psychology. Problem solving is defined as a nonroutine activity oriented toward changing an undesirable state of affairs. The focus on change differentiates problem solving from coping, which is oriented toward relieving feelings of stress. A decision-making model is presented, which takes the problem-solving process through its latter stages. The theory is based on two axioms and three theorems pertaining to the process of decision making. These axioms and theorems serve as the foundation for deriving 14 theorems that establish the antecedent conditions affecting decisions relevnat to each of four stages in the problem-solving process. This theory is distinguished from other problem-solving theories in its effort to account for conditions leading to awareness of problems and in its emphasis on generic problem-solving processes rather than on the effectiveness of problem-solving outcomes.',\n",
       " 'Sticky Information and the Locus of Problem Solving: Implications for Innovation. To solve a problem, needed information and problem-solving capabilities must be brought together. Often the information used in technical problem solving is costly to acquire, transfer, and use in a new location-is, in our terms, sticky. In this paper we explore the impact of information stickiness on the locus of innovation-related problem solving. We find, first, that when sticky information needed by problem solvers is held at one site only, problem solving will be carried out at that locus, other things being equal. Second, when more than one locus of sticky information is called upon by problem solvers, the locus of problem solving may iterate among these sites as problem solving proceeds. When the costs of such iteration are high, then, third, problems that draw upon multiple sites of sticky information will sometimes be task partitioned into subproblems that each draw on only one such locus, and/or, fourth, investments will be made to reduce the stickiness of information at some locations. Information stickiness appears to affect a number of issues of importance to researchers and practitioners. Among these are patterns in the diffusion of information, the specialization of firms, the locus of innovation, and the nature of problems selected by problem solvers.',\n",
       " 'Information diffusion through blogspace. We study the dynamics of information propagation in environments of low-overhead personal publishing, using a large collection of weblogs over time as our example domain. We characterize and model this collection at two levels. First, we present a macroscopic characterization of topic propagation through our corpus, formalizing the notion of long-running “chatter” topics consisting recursively of “spike” topics generated by outside world events, or more rarely, by resonances within the community. Second, we present a microscopic characterization of propagation from individual to individual, drawing on the theory of infectious diseases to model the flow. We propose, validate, and employ an algorithm to induce the underlying propagation network from a sequence of posts, and report on the results.',\n",
       " 'Problems with Fitting to the Power-Law Distribution. Abstract. This short communication uses a simple experiment to show that fitting to a power law distribution by using graphical methods based on linear fit on the log-log scale is biased and inaccurate. It shows that using maximum likelihood estimation (MLE) is far more robust. Finally, it presents a new table for performing the Kolmogorov-Smirnov test for goodness-of-fit tailored to power-law distributions in which the power-law exponent is estimated using MLE. The techniques presented here will advance the application of complex network theory by allowing reliable estimation of power-law models from data and further allowing quantitative assessment of goodness-of-fit of proposed power-law models to empirical data. PACS. 02.50.Ng Distribution theory and Monte Carlo studies -- 05.10.Ln Monte Carlo methods -- 89.75.-k',\n",
       " \"The shortest path to complex networks. 1. The birth of network science. 2. What are random networks? 3. Adjacency matrix. 4. Degree distribution. 5. What are simple networks? Classical random graphs. 6. Birth of the giant component. 7. Topology of the Web. 8.Uncorrelated networks. 9. What are small worlds? 10. Real networks are mesoscopic objects. 11. What are complex networks? 12. The configuration model. 13. The absence of degree--degree correlations. 14.Networks with correlated degrees.15.Clustering. 16. What are small-world networks? 17. `Small worlds' is not the same as `small-world networks'. 18. Fat-tailed degree distributions. 19.Reasons for the fat-tailed degree distributions. 20. Preferential linking. 21. Condensation of edges. 22. Cut-offs of degree distributions. 23. Reasons for correlations in networks. 24. Classical random graphs cannot be used for comparison with real networks. 25. How to measure degree--degree correlations. 26. Assortative and disassortative mixing. 27. Disassortative mixing does not mean that vertices of high degrees rarely connect to each other. 28. Reciprocal links in directed nets. 29. Ultra-small-world effect. 30. Tree ansatz. 31.Ultraresilience against random failures. 32. When correlated nets are ultraresilient. 33. Vulnerability of complex networks. 34. The absence of an epidemic threshold. 35. Search based on local information. 36.Ultraresilience disappears in finite nets. 37.Critical behavior of cooperative models on networks. 38. Berezinskii-Kosterlitz-Thouless phase transitions in networks. 39.Cascading failures. 40.Cliques &amp; communities. 41. Betweenness. 42.Extracting communities. 43. Optimal paths. 44.Distributions of the shortest-path length &amp; of the loop's length are narrow. 45. Diffusion on networks. 46. What is modularity? 47.Hierarchical organization of networks. 48. Convincing modelling of real-world networks:Is it possible? 49. The small Web..\",\n",
       " 'Coevolution of dynamical states and interactions in dynamic networks. We explore the coupled dynamics of the internal states of a set of interacting elements and the network of interactions among them. Interactions are modeled by a spatial game and the network of interaction links evolves adapting to the outcome of the game. As an example, we consider a model of cooperation in which the adaptation is shown to facilitate the formation of a hierarchical interaction network that sustains a highly cooperative stationary state. The resulting network has the characteristics of a small world network when a mechanism of local neighbor selection is introduced in the adaptive network dynamics. The highly connected nodes in the hierarchical structure of the network play a leading role in the stability of the network. Perturbations acting on the state of these special nodes trigger global avalanches leading to complete network reorganization.',\n",
       " 'Coarse-Graining and Self-Dissimilarity of Complex Networks. Can complex engineered and biological networks be coarse-grained into smaller and more understandable versions in which each node represents an entire pattern in the original network? To address this, we define coarse-graining units as connectivity patterns which can serve as the nodes of a coarse-grained network and present algorithms to detect them. We use this approach to systematically reverse-engineer electronic circuits, forming understandable high-level maps from incomprehensible transistor wiring: first, a coarse-grained version in which each node is a gate made of several transistors is established. Then the coarse-grained network is itself coarse-grained, resulting in a high-level blueprint in which each node is a circuit module made of many gates. We apply our approach also to a mammalian protein signal-transduction network, to find a simplified coarse-grained network with three main signaling channels that resemble multi-layered perceptrons made of cross-interacting MAP-kinase cascades. We find that both biological and electronic networks are \"self-dissimilar,\" with different network motifs at each level. The present approach may be used to simplify a variety of directed and nondirected, natural and designed networks.',\n",
       " 'MIPS: a database for genomes and protein sequences. The Munich Information Center for Protein Sequences (MIPS-GSF, Neuherberg, Germany) continues to provide genome-related information in a systematic way. MIPS supports both national and European sequencing and functional analysis projects, develops and maintains automatically generated and manually annotated genome-specific databases, develops systematic classification schemes for the functional annotation of protein sequences, and provides tools for the comprehensive analysis of protein sequences. This report updates the information on the yeast genome (CYGD), the Neurospora crassa genome (MNCDB), the databases for the comprehensive set of genomes (PEDANT genomes), the database of annotated human EST clusters (HIB), the database of complete cDNAs from the DHGP (German Human Genome Project), as well as the project specific databases for the GABI (Genome Analysis in Plants) and HNB (Helmholtz-Netzwerk Bioinformatik) networks. The Arabidospsis thaliana database (MATDB), the database of mitochondrial proteins (MITOP) and our contribution to the PIR International Protein Sequence Database have been described elsewhere [Schoof et al. (2002) Nucleic Acids Res., 30, 91-93; Scharfe et al. (2000) Nucleic Acids Res., 28, 155-158; Barker et al. (2001) Nucleic Acids Res., 29, 29-32]. All databases described, the protein analysis tools provided and the detailed descriptions of our projects can be accessed through the MIPS World Wide Web server (http://mips.gsf.de).',\n",
       " 'MIPS: analysis and annotation of proteins from whole genomes. The Munich Information Center for Protein Sequences (MIPSâ\\x80\\x90GSF), Neuherberg, Germany, provides protein sequenceâ\\x80\\x90related information based on wholeâ\\x80\\x90genome analysis. The main focus of the work is directed toward the systematic organization of sequenceâ\\x80\\x90related attributes as gathered by a variety of algorithms, primary information from experimental data together with information compiled from the scientific literature. MIPS maintains automatically generated and manually annotated genomeâ\\x80\\x90specific databases, develops systematic classification schemes for the functional annotation of protein sequences and provides tools for the comprehensive analysis of protein sequences. This report updates the information on the yeast genome (CYGD), the Neurospora crassa genome (MNCDB), the database of complete cDNAs (German Human Genome Project, NGFN), the database of mammalian proteinâ\\x80\\x93protein interactions (MPPI), the database of FASTA homologies (SIMAP), and the interface for the fast retrieval of proteinâ\\x80\\x90associated information (QUIPOS). The Arabidopsis thaliana database, the rice database, the plant EST databases (MATDB, MOsDB, SPUTNIK), as well as the databases for the comprehensive set of genomes (PEDANT genomes) are described elsewhere in the 2003 and 2004 NAR database issues, respectively. All databases described, and the detailed descriptions of our projects can be accessed through the MIPS web server (http://mips.gsf.de).',\n",
       " 'Evolving a Stigmergic Self-Organized Data-Mining. Self-organizing complex systems typically are comprised of a large number of frequently similar components or events. Through their process, a pattern at the global-level of a system emerges solely from numerous interactions among the lower-level components of the system. Moreover, the rules specifying interactions among the system’s components are executed using only local information, without reference to the global pattern, which, as in many real-world problems is not easily accessible or possible to be found. Stigmergy, a kind of indirect communication and learning by the environment found in social insects is a well know example of self-organization, providing not only vital clues in order to understand how the components can interact to produce a complex pattern, as can pinpoint simple biological non-linear rules and methods to achieve improved artificial intelligent adaptive categorization systems, critical for Data-Mining. On the present work it is our intention to show that a new type of Data-Mining can be designed based on Stigmergic paradigms, taking profit of several natural features of this phenomenon. By hybridizing bio-inspired Swarm Intelligence with Evolutionary Computation we seek for an entire distributed, adaptive, collective and cooperative self-organized Data-Mining. As a real-world/real-time test bed for our proposal, World-Wide-Web Mining will be used.  Having that purpose in mind, Web usage Data was collected from the Monash University’s Web site (Australia), with over 7 million hits every week. Results are compared to other recent systems, showing that the system presented is by far promising.',\n",
       " 'Random Networks with Tunable Degree Distribution and Clustering. We present an algorithm for generating random networks with arbitrary degree distribution and Clustering (frequency of triadic closure). We use this algorithm to generate networks with exponential, power law, and poisson degree distributions with variable levels of clustering. Such networks may be used as models of social networks and as a testable null hypothesis about network structure. Finally, we explore the effects of clustering on the point of the phase transition where a giant component forms in a random network, and on the size of the giant component. Some analysis of these effects is presented.',\n",
       " 'The statistical mechanics of networks. We study the family of network models derived by requiring the expected properties of a graph ensemble to match a given set of measurements of a real-world network, while maximizing the entropy of the ensemble. Models of this type play the same role in the study of networks as is played by the Boltzmann distribution in classical statistical mechanics; they offer the best prediction of network properties subject to the constraints imposed by a given set of observations. We give exact solutions of models within this class that incorporate arbitrary degree distributions and arbitrary but independent edge probabilities. We also discuss some more complex examples with correlated edges that can be solved approximately or exactly by adapting various familiar methods, including mean-field theory, perturbation theory, and saddle-point expansions.',\n",
       " 'Bridging Epistemologies: The Generative Dance between Organizational Knowledge and Organizational Knowing. Much current work on organizational knowledge, intellectual capital, knowledge-creating organizations, knowledge work, and the like rests on a single, traditional understanding of the nature of knowledge. We call this understanding the \"epistemology of possession,\" since it treats knowledge as something people possess. Yet, this epistemology cannot account for the knowing found in individual and group practice. Knowing as action calls for an \"epistemology of practice.\" Moreover, the epistemology of possession tends to privilege explicit over tacit knowledge, and knowledge possessed by individuals over that possessed by groups. Current work on organizations is limited by this privileging and by the scant attention given to knowing in its own right. Organizations are better understood if explicit, tacit, individual and group knowledge are treated as four distinct and coequal forms of knowledge (each doing work the others cannot), and if knowledge and knowing are seen as mutually enabling (not competing). We hold that knowledge is a tool of knowing, that knowing is an aspect of our interaction with the social and physical world, and that the interplay of knowledge and knowing can generate new knowledge and new ways of knowing. We believe this generative dance between knowledge and knowing is a powerful source of organizational innovation. Harnessing this innovation calls for organizational and technological infrastructures that support the interplay of knowledge and knowing. Ultimately, these concepts make possible a more robust framing of such epistemologically-centered concerns as core competencies, the management of intellectual capital, etc. We explore these views through three brief case studies drawn from recent research. 10.1287/orsc.10.4.381',\n",
       " 'SGD: Saccharomyces Genome Database.. The Saccharomyces Genome Database (SGD) provides Internet access to the complete Saccharomyces cerevisiae genomic sequence, its genes and their products, the phenotypes of its mutants, and the literature supporting these data. The amount of information and the number of features provided by SGD have increased greatly following the release of the S.cerevisiae genomic sequence, which is currently the only complete sequence of a eukaryotic genome. SGD aids researchers by providing not only basic information, but also tools such as sequence similarity searching that lead to detailed information about features of the genome and relationships between genes. SGD presents information using a variety of user-friendly, dynamically created graphical displays illustrating physical, genetic and sequence feature maps. SGD can be accessed via the World Wide Web at http://genome-www.stanford.edu/Saccharomyces/',\n",
       " \"On the uniform generation of random graphs with prescribed degree sequences. Random graphs with prescribed degree sequences have been widely used as a model of complex networks. Comparing an observed network to an ensemble of such graphs allows one to detect deviations from randomness in network properties. Here we briefly review two existing methods for the generation of random graphs with arbitrary degree sequences, which we call the ``switching'' and ``matching'' methods, and present a new method based on the ``go with the winners'' Monte Carlo method. The matching method may suffer from non uniform sampling, while the switching method has no general theoretical bound on its mixing time. The ``go with the winners'' method has neither of these drawbacks, but is slow. It can however be used to evaluate there liability of the other two methods and, by doing this, we demonstrate that the deviations of the switching and matching algorithms under realistic conditions are small compared to the ``go with the winners'' algorithm. Because of its combination of speed and accuracy we recommend the use of the switching method for most calculations.\",\n",
       " 'How can we think the complex?. This chapter does not deal with specific tools and techniques for managing complex systems, but proposes some basic concepts that help us to think and speak about complexity. We review classical thinking and its intrinsic drawbacks when dealing with complexity. We then show how complexity forces us to take build models with indeterminacy and unpredictability. However, we can still deal with the problems created in this way by being adaptive, and profiting from a complex system’s capability for selforganization, and the distributed intelligence this may produce.',\n",
       " \"What's in a name?. Among the several findings deriving from the application of complex network formalism to the investigation of natural phenomena, the fact that linguistic constructions follow power laws presents special interest for its potential implications for psychology and brain science. By corresponding to one of the most essentially human manifestations, such language-related properties suggest that similar dynamics may also be inherent to the brain areas related to language and associative memory, and perhaps even consciousness. The present work reports a preliminary experimental investigation aimed at characterizing and modeling the flow of sequentially induced associations between words from the English language in terms of complex networks. The data is produced through a psychophysical experiment where a word is presented to the subject, who is requested to associate another word. Complex network and graph theory formalism and measurements are applied in order to characterize the experimental data. Several interesting results are identified, including the characterization of attraction basins, association asymmetries, context biasing, as well as a possible power-law underlying word associations, which could be explained by the appearance of strange loops along the hierarchical structure underlying word categories.\",\n",
       " \"Zipf's law and the creation of musical context. This article discusses the extension of the notion of context from linguistics to the domain of music. In language, the statistical regularity known as Zipf's law - which concerns the frequency of usage of different words - has been quantitatively related to the process of text generation. This connection is established by Simon's model, on the basis of a few assumptions regarding the accompanying creation of context. Here, it is shown that the statistics of note usage in musical compositions are compatible with the predictions of Simon's model. This result, which gives objective support to the conceptual likeness of context in language and music, is obtained through automatic analysis of the digital versions of several compositions. As a by-product, a quantitative measure of context definiteness is introduced and used to compare tonal and atonal works.\",\n",
       " 'Modeling the evolution of weighted networks. We present a general model for the growth of weighted networks in which the structural growth is coupled with the edges’ weight dynamical evolution. The model is based on a simple weight-driven dynamics and a weights’ reinforcement mechanism coupled to the local network growth. That coupling can be generalized in order to include the effect of additional randomness and nonlinearities which can be present in real-world networks. The model generates weighted graphs exhibiting the statistical properties observed in several real-world systems. In particular, the model yields a nontrivial time evolution of vertices’ properties and scale-free behavior with exponents depending on the microscopic parameters characterizing the coupling rules. Very interestingly, the generated graphs spontaneously achieve a complex hierarchical architecture characterized by clustering and connectivity correlations varying as a function of the vertices’ degree.',\n",
       " 'Social Structure and Opinion Formation. We present a dynamical theory of opinion formation that takes explicitly into account the structure of the social network in which in- dividuals are embedded. The theory predicts the evolution of a set of opinions through the social network and establishes the existence of a martingale property, i.e. that the expected weighted fraction of the population that holds a given opinion is constant in time. Most importantly, this weighted fraction is not either zero or one, but corresponds to a non-trivial distribution of opinions in the long time limit. This co-existence of opinions within a social network is in agreement with the often observed locality effect, in which an opinion or a fad is localized to given groups without infecting the whole society. We verified these predictions, as well as those concerning the fragility of opinions and the importance of highly connected individuals in opinion formation, by performing computer experiments on a number of social networks.',\n",
       " 'Conservation and Evolution of Cis-Regulatory Systems in Ascomycete Fungi. Relatively little is known about the mechanisms through which gene expression regulation evolves. To investigate this, we systematically explored the conservation of regulatory networks in fungi by examining the cis -regulatory elements that govern the expression of coregulated genes. We first identified groups of coregulated Saccharomyces cerevisiae genes enriched for genes with known upstream or downstream cis -regulatory sequences. Reasoning that many of these gene groups are coregulated in related species as well, we performed similar analyses on orthologs of coregulated S. cerevisiae genes in 13 other ascomycete species. We find that many species-specific gene groups are enriched for the same flanking regulatory sequences as those found in the orthologous gene groups from S. cerevisiae , indicating that those regulatory systems have been conserved in multiple ascomycete species. In addition to these clear cases of regulatory conservation, we find examples of cis -element evolution that suggest multiple modes of regulatory diversification, including alterations in transcription factor-binding specificity, incorporation of new gene targets into an existing regulatory system, and cooption of regulatory systems to control a different set of genes. We investigated one example in greater detail by measuring the in vitro activity of the S. cerevisiae transcription factor Rpn4p and its orthologs from Candida albicans and Neurospora crassa . Our results suggest that the DNA binding specificity of these proteins has coevolved with the sequences found upstream of the Rpn4p target genes and suggest that Rpn4p has a different function in N. crassa .',\n",
       " 'Computational identification of developmental enhancers: conservation and function of transcription factor binding-site clusters in Drosophila melanogaster and Drosophila pseudoobscura.. BACKGROUND: The identification of sequences that control transcription in metazoans is a major goal of genome analysis. In a previous study, we demonstrated that searching for clusters of predicted transcription factor binding sites could discover active regulatory sequences, and identified 37 regions of the Drosophila melanogaster genome with high densities of predicted binding sites for five transcription factors involved in anterior-posterior embryonic patterning. Nine of these clusters overlapped known enhancers. Here, we report the results of in vivo functional analysis of 27 remaining clusters. RESULTS: We generated transgenic flies carrying each cluster attached to a basal promoter and reporter gene, and assayed embryos for reporter gene expression. Six clusters are enhancers of adjacent genes: giant, fushi tarazu, odd-skipped, nubbin, squeeze and pdm2; three drive expression in patterns unrelated to those of neighboring genes; the remaining 18 do not appear to have enhancer activity. We used the Drosophila pseudoobscura genome to compare patterns of evolution in and around the 15 positive and 18 false-positive predictions. Although conservation of primary sequence cannot distinguish true from false positives, conservation of binding-site clustering accurately discriminates functional binding-site clusters from those with no function. We incorporated conservation of binding-site clustering into a new genome-wide enhancer screen, and predict several hundred new regulatory sequences, including 85 adjacent to genes with embryonic patterns. CONCLUSIONS: Measuring conservation of sequence features closely linked to function--such as binding-site clusterin--makes better use of comparative sequence data than commonly used methods that examine only sequence identity.',\n",
       " 'The Economic Implications of Learning by Doing. Demonstrates that technical change is attributable to experience. The cumulative production of capital goods is used as the index of experience.  New capital goods are assumed to completely embody technical change. The assumption is made that the model will be operating in an environment of full employment although reference is made throughout to the case of capital shortage.         The implications of this model on wage earners are discussed, and profits and investments are examined. The rate of return is determined by the expected rate of increase in wages, current labor costs per unit output, and the physical lifetime of the investment. Learning is an act of investment that benefits future investors. Further analysis shows that the socially optimal ratio of gross investment to output is higher than the competitive level.   (SRD)',\n",
       " \"Shannon Information and Kolmogorov Complexity. We compare the elementary theories of Shannon information and Kolmogorov complexity, the extent to which they have a common purpose, and where they are fundamentally different. We discuss and relate the basic notions of both theories: Shannon entropy versus Kolmogorov complexity, the relation of both to universal coding, Shannon mutual information versus Kolmogorov (`algorithmic') mutual information, probabilistic sufficient statistic versus algorithmic sufficient statistic (related to lossy compression in the Shannon theory versus meaningful information in the Kolmogorov theory), and rate distortion theory versus Kolmogorov's structure function. Part of the material has appeared in print before, scattered through various publications, but this is the first comprehensive systematic comparison. The last mentioned relations are new.\",\n",
       " 'MUSCLE: a multiple sequence alignment method with reduced time and space complexity. BACKGROUND: In a previous paper, we introduced MUSCLE, a new program for creating multiple alignments of protein sequences, giving a brief summary of the algorithm and showing MUSCLE to achieve the highest scores reported to date on four alignment accuracy benchmarks. Here we present a more complete discussion of the algorithm, describing several previously unpublished techniques that improve biological accuracy and / or computational complexity. We introduce a new option, MUSCLE-fast, designed for high-throughput applications. We also describe a new protocol for evaluating objective functions that align two profiles. RESULTS: We compare the speed and accuracy of MUSCLE with CLUSTALW, Progressive POA and the MAFFT script FFTNS1, the fastest previously published program known to the author. Accuracy is measured using four benchmarks: BAliBASE, PREFAB, SABmark and SMART. We test three variants that offer highest accuracy (MUSCLE with default settings), highest speed (MUSCLE-fast), and a carefully chosen compromise between the two (MUSCLE-prog). We find MUSCLE-fast to be the fastest algorithm on all test sets, achieving average alignment accuracy similar to CLUSTALW in times that are typically two to three orders of magnitude less. MUSCLE-fast is able to align 1,000 sequences of average length 282 in 21 seconds on a current desktop computer. CONCLUSIONS: MUSCLE offers a range of options that provide improved speed and / or alignment accuracy compared with currently available programs. MUSCLE is freely available at http://www.drive5.com/muscle.',\n",
       " 'EDUTELLA: A P2P Networking Infrastructure Based on RDF. Metadata for the World Wide Web is important, but metadata for Peer-to-Peer (P2P) networks is absolutely crucial. In this paper we discuss the open source project Edutella which builds upon metadata standards defined for the WWW and aims to provide an RDF-based metadata infrastructure for P2P applications, building on the recently announced JXTA Framework. We describe the goals and main services this infrastructure will provide and the architecture to connect Edutella Peers based on exchange of RDF metadata. As the query service is one of the core services of Edutella, upon which other services are built, we specify in detail the Edutella Common Data Model (ECDM) as basis for the Edutella query exchange language (RDF-QEL-i) and format implementing distributed queries over the Edutella network. Finally, we shortly discuss registration and mediation services, and introduce the prototype and application scenario for our current Edutella aware peers.',\n",
       " \"The long memory of the efficient market. For the London Stock Exchange we demonstrate that the signs of orders obey a long-memory process. The autocorrelation function decays roughly as $\\\\tau^{-\\\\alpha}$ with $\\\\alpha \\\\approx 0.6$, corresponding to a Hurst exponent $H \\\\approx 0.7$. This implies that the signs of future orders are quite predictable from the signs of past orders; all else being equal, this would suggest a very strong market inefficiency. We demonstrate, however, that fluctuations in order signs are compensated for by anti-correlated fluctuations in transaction size and liquidity, which are also long-memory processes. This tends to make the returns whiter. We show that some institutions display long-range memory and others don't.\",\n",
       " 'Complexity vs stability in small-world networks. According to the May-Wigner stability theorem, increasing the complexity of a network inevitably leads to its destabilization, such that a small perturbation will be able to disrupt the entire system. One of the principal arguments against this observation is that it is valid only for random networks, and therefore does not apply to real-world networks, which presumably are structured. Here, we examine how the introduction of small-world topological structure into networks affects their stability. Our results indicate that, in structured networks, the parameter values at which the stability-instability transition occurs with increasing complexity is identical to that predicted by the May-Wigner criteria. However, the nature of the transition, as measured by the finite-size scaling exponent, appears to change as the network topology transforms from regular to random, with the small-world regime as the cross-over region. This behavior is related to the localization of the largest eigenvalues along the real axis in the eigenvalue plain with increasing regularity in the network.',\n",
       " 'The spatial structure of networks. Abstract.&nbsp;&nbsp; We study networks that connect points in geographic space, such as transportation networks and the Internet. We ﬁnd that there are strong signatures in these networks of topography and use patterns, giving the networks shapes that are quite distinct from one another and from non-geographic networks. We oﬀer an explanation of these diﬀerences in terms of the costs and beneﬁts of transportation and communication, and give a simple model based on the Monte Carlo optimization of these costs and beneﬁts that reproduces well the qualitative features of the networks studied.',\n",
       " 'Citation Statistics From More Than a Century of Physical Review. We study the statistics of citations from all Physical Review journals for the 110-year period 1893 until 2003. In addition to characterizing the citation distribution and identifying publications with the highest citation impact, we investigate how citations evolve with time. There is a positive correlation between the number of citations to a paper and the average age of citations. Citations from a publication have an exponentially decaying age distribution; that is, old papers tend to not get cited. In contrast, the citations to a publication are consistent with a power-law age distribution, with an exponent close to -1 over a time range of 2 -- 20 years. We also identify a number of strongly-correlated citation bursts and other dramatic features in the time history of citations to individual publications.',\n",
       " 'Analysis of Relative Gene Expression Data Using Real-Time Quantitative PCR and the 2−ΔΔCT Method. The two most commonly used methods to analyze data from real-time, quantitative PCR experiments are absolute quantification and relative quantification. Absolute quantification determines the input copy number, usually by relating the PCR signal to a standard curve. Relative quantification relates the PCR signal of the target transcript in a treatment group to that of another sample such as an untreated control. The 2−ΔΔCT method is a convenient way to analyze the relative changes in gene expression from real-time quantitative PCR experiments. The purpose of this report is to present the derivation, assumptions, and applications of the 2−ΔΔCT method. In addition, we present the derivation and applications of two variations of the 2−ΔΔCT method that may be useful in the analysis of real-time, quantitative PCR data.',\n",
       " 'Document Similarity Using a Phrase Indexing Graph Model. Document clustering techniques mostly rely on single term analysis of text, such as the vector space model. To better capture the structure of documents, the underlying data model should be able to represent the phrases in the document as well as single terms. We present a novel data model, the Document Index Graph, which indexes Web documents based on phrases rather than on single terms only. The semistructured Web documents help in identifying potential phrases that when matched with other documents indicate strong similarity between the documents. The Document Index Graph captures this information, and finding significant matching phrases between documents becomes easy and efficient with such model. The model is flexible in that it could revert to a compact representation of the vector space model if we choose not to index phrases. However, using phrase indexing yields more accurate document similarity calculations. The similarity between documents is based on both single term weights and matching phrase weights. The combined similarities are used with standard document clustering techniques to test their effect on the clustering quality. Experimental results show that our phrase-based similarity, combined with single-term similarity measures, gives a more accurate measure of document similarity and thus significantly enhances Web document clustering quality. [PUBLICATION ABSTRACT]',\n",
       " \"From Kuramoto to Crawford: exploring the onset of synchronization in populations of coupled oscillators. The Kuramoto model describes a large population of coupled limit-cycle oscillators whose natural frequencies are drawn from some prescribed distribution. If the coupling strength exceeds a certain threshold, the system exhibits a phase transition: some of the oscillators spontaneously synchronize, while others remain incoherent. The mathematical analysis of this bifurcation has proved both problematic and fascinating. We review 25 years of research on the Kuramoto model, highlighting the false turns as well as the successes, but mainly following the trail leading from Kuramoto's work to Crawford's recent contributions. It is a lovely winding road, with excursions through mathematical biology, statistical physics, kinetic theory, bifurcation theory, and plasma physics. (C) 2000 Elsevier Science B.V. All rights reserved.\",\n",
       " 'The systems biology markup language (SBML): a medium for representation and exchange of biochemical network models. Motivation: Molecular biotechnology now makes it possible to build elaborate systems models, but the systems biology community needs information standards if models are to be shared, evaluated and developed cooperatively.  Results: We summarize the Systems Biology Markup Language (SBML) Level 1, a free, open, XML-based format for representing biochemical reaction networks. SBML is a software-independent language for describing models common to research in many areas of computational biology, including cell signaling pathways, metabolic pathways, gene regulation, and others.  Availability: The specification of SBML Level 1 is freely available from http://www.sbml.org/  Contact: sysbio-team@caltech.edu 10.1093/bioinformatics/btg015',\n",
       " 'A Fast Elitist Non-Dominated Sorting Genetic Algorithm for Multi-Objective Optimization: NSGA-II. Multi-objective evolutionary algorithms which use non-dominated sorting and sharing have been mainly criticized for their (i) O(MN 3) computational complexity (where M is the number of objectives and N is the population size), (ii) non-elitism approach, and (iii) the need for specifying a sharing parameter. In this paper, we suggest a non-dominated sorting based multi-objective evolutionary algorithm (we called it the Non-dominated Sorting GA-II or NSGA-II) which alleviates all the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN 2) computational complexity is presented. Second, a selection operator is presented which creates a mating pool by combining the parent and child populations and selecting the best (with respect to fitness and spread) N solutions. Simulation results on five difficult test problems show that the proposed NSGA-II, in most problems, is able to find much better spread of solutions and better convergence near the true Pareto-optimal front compared to PAES and SPEAâ\\x80\\x94two other elitist multi-objective EAs which pay special attention towards creating a diverse Pareto-optimal front. Because of NSGA-IIâ\\x80\\x99s low computational requirements, elitist approach, and parameter-less sharing approach, NSGA-II should find increasing applications in the years to come.',\n",
       " 'The similarity metric. A new class of metrics appropriate for measuring effective similarity relations between sequences, say one type of similarity per metric, is studied. We propose a new \"normalized information distance\", based on the noncomputable notion of Kolmogorov complexity, and show that it minorizes every metric in the class (that is, it is universal in that it discovers all effective similarities). We demonstrate that it too is a metric and takes values in [0, 1]; hence it may be called the  similarity metric . This is a theory foundation for a new general practical tool. We give two distinctive applications in widely divergent areas (the experiments by necessity use just computable approximations to the target notions). First, we computationally compare whole mitochondrial genomes and infer their evolutionary history. This results in a first completely automatic computed whole mitochondrial phylogeny tree. Secondly, we give fully automatically computed language tree of 52 different language based on translated versions of the \"Universal Declaration of Human Rights\".',\n",
       " 'Introduction to Random Boolean Networks. The goal of this tutorial is to promote interest in the study of random Boolean networks (RBNs). These can be very interesting models, since one does not have to assume any functionality or particular connectivity of the networks to study their generic properties. Like this, RBNs have been used for exploring the configurations where life could emerge. The fact that RBNs are a generalization of cellular automata makes their research a very important topic. The tutorial, intended for a broad audience, presents the state of the art in RBNs, spanning over several lines of research carried out by different groups. We focus on research done within artificial life, as we cannot exhaust the abundant research done over the decades related to RBNs.',\n",
       " \"A Generalized Approach to Complex Networks. This work reports on how the formalization of complex network concepts in terms of discrete mathematics, especially mathematical morphology, allows a series of generalizations and important results ranging from new measurements of the network topology to new network growth models. First, the concepts of node degree and clustering coefficient are extended in order to characterize not only specific nodes, but any generic subnetwork. Second, the consideration of distance transform and rings are used to further extend those concepts in order to obtain a signature, instead of a single scalar measurement, ranging from the single node to whole graph scales. The use of the closing operation is also proposed as a means to analyze the proximity of 3-cycles along a complex network. The potential of such concepts is illustrated with respect to the random and Barab\\\\'asi-Albert models, as well as distance-based preferential-attachment networks. A generalization of the Barab\\\\'asi-Albert network, characterized by preferential attachment with respect to the generalized degree, is also proposed and shown to exhibit properties which are intermediate between the Barab\\\\'asi-Albert and random models. Each of these cases is characterized and analyzed with respect to particularly relevant subnetworks contained in the original complex network, namely hubs, 3-cycles and outmost nodes.\",\n",
       " 'Generation of uncorrelated random scale-free networks. Uncorrelated random scale-free networks are useful null models to check the accuracy and the analytical solutions of dynamical processes defined on complex networks. We propose and analyze a model capable of generating random uncorrelated scale-free networks with no multiple and self-connections. The model is based on the classical configuration model, with an additional restriction on the maximum possible degree of the vertices. We check numerically that the proposed model indeed generates scale-free networks with no two- and three-vertex correlations, as measured by the average degree of the nearest neighbors and the clustering coefficient of the vertices of degree k , respectively.',\n",
       " \"Robustness in bacterial chemotaxis.. Networks of interacting proteins orchestrate the responses of living cells to a variety of external stimuli, but how sensitive is the functioning of these protein networks to variations in their biochemical parameters? One possibility is that to achieve appropriate function, the reaction rate constants and enzyme concentrations need to be adjusted in a precise manner, and any deviation from these 'fine-tuned' values ruins the network's performance. An alternative possibility is that key properties of biochemical networks are robust; that is, they are insensitive to the precise values of the biochemical parameters. Here we address this issue in experiments using chemotaxis of Escherichia coli, one of the best-characterized sensory systems. We focus on how response and adaptation to attractant signals vary with systematic changes in the intracellular concentration of the components of the chemotaxis network. We find that some properties, such as steady-state behaviour and adaptation time, show strong variations in response to varying protein concentrations. In contrast, the precision of adaptation is robust and does not vary with the protein concentrations. This is consistent with a recently proposed molecular mechanism for exact adaptation, where robustness is a direct consequence of the network's architecture.\",\n",
       " 'Global prevalence of diabetes: estimates for the year 2000 and projections for 2030.. OBJECTIVE: The goal of this study was to estimate the prevalence of diabetes and the number of people of all ages with diabetes for years 2000 and 2030. RESEARCH DESIGN AND METHODS: Data on diabetes prevalence by age and sex from a limited number of countries were extrapolated to all 191 World Health Organization member states and applied to United Nations\\' population estimates for 2000 and 2030. Urban and rural populations were considered separately for developing countries. RESULTS: The prevalence of diabetes for all age-groups worldwide was estimated to be 2.8% in 2000 and 4.4% in 2030. The total number of people with diabetes is projected to rise from 171 million in 2000 to 366 million in 2030. The prevalence of diabetes is higher in men than women, but there are more women with diabetes than men. The urban population in developing countries is projected to double between 2000 and 2030. The most important demographic change to diabetes prevalence across the world appears to be the increase in the proportion of people >65 years of age. CONCLUSIONS: These findings indicate that the \"diabetes epidemic\" will continue even if levels of obesity remain constant. Given the increasing prevalence of obesity, it is likely that these figures provide an underestimate of future diabetes prevalence.',\n",
       " 'What makes biochemical networks tick?. In view of the increasing number of reported concentration oscillations in living cells, methods are needed that can identify the causes of these oscillations. These causes always derive from the influences that concentrations have on reaction rates. The influences reach over many molecular reaction steps and are defined by the detailed molecular topology of the network. So-called ‘autoinfluence paths’, which quantify the influence of one molecular species upon itself through a particular path through the network, can have positive or negative values. The former bring a tendency towards instability. In this molecular context a new graphical approach is presented that enables the classification of network topologies into oscillophoretic and nonoscillophoretic, i.e. into ones that can and ones that cannot induce concentration oscillations. The network topologies are formulated in terms of a set of uni-molecular and bi-molecular reactions, organized into branched cycles of directed reactions, and presented as graphs. Subgraphs of the network topologies are then classified as negative ones (which can) and positive ones (which cannot) give rise to oscillations. A subgraph is oscillophoretic (negative) when it contains more positive than negative autoinfluence paths. Whether the former generates oscillations depends on the values of the other subgraphs, which again depend on the kinetic parameters. An example shows how this can be established. By following the rules of our new approach, various oscillatory kinetic models can be constructed and analyzed, starting from the classified simplest topologies and then working towards desirable complications. Realistic biochemical examples are analyzed with the new method, illustrating two new main classes of oscillophore topologies.',\n",
       " 'Robustness of Cellular Functions. Robustness, the ability to maintain performance in the face of perturbations and uncertainty, is a long-recognized key property of living systems. Owing to intimate links to cellular complexity, however, its molecular and cellular basis has only recently begun to be understood. Theoretical approaches to complex engineered systems can provide guidelines for investigating cellular robustness because biology and engineering employ a common set of basic mechanisms in different combinations. Robustness may be a key to understanding cellular complexity, elucidating design principles, and fostering closer interactions between experimentation and theory.',\n",
       " 'The Protein Folding Network. The conformation space of a 20 residue antiparallel beta-sheet peptide, sampled by molecular dynamics simulations, is mapped to a network. Snapshots saved along the trajectory are grouped according to secondary structure into nodes of the network and the transitions between them are links. The conformation space network describes the significant free energy minima and their dynamic connectivity without requiring arbitrarily chosen reaction coordinates. As previously found for the Internet and the World-Wide Web as well as for social and biological networks, the conformation space network is scale-free and contains highly connected hubs like the native state which is the most populated free energy basin. Furthermore, the native basin exhibits a hierarchical organization, which is not found for a random heteropolymer lacking a predominant free-energy minimum. The network topology is used to identify conformations in the folding transition state (TS) ensemble, and provides a basis for understanding the heterogeneity of the TS and denatured state ensemble as well as the existence of multiple pathways.',\n",
       " 'Efficient sampling algorithm for estimating subgraph concentrations and detecting network motifs. Summary: Biological and engineered networks have recently been shown to display network motifs: a small set of characteristic patterns that occur much more frequently than in randomized networks with the same degree sequence. Network motifs were demonstrated to play key information processing roles in biological regulation networks. Existing algorithms for detecting network motifs act by exhaustively enumerating all subgraphs with a given number of nodes in the network. The runtime of such algorithms increases strongly with network size. Here, we present a novel algorithm that allows estimation of subgraph concentrations and detection of network motifs at a runtime that is asymptotically independent of the network size. This algorithm is based on random sampling of subgraphs. Network motifs are detected with a surprisingly small number of samples in a wide variety of networks. Our method can be applied to estimate the concentrations of larger subgraphs in larger networks than was previously possible with exhaustive enumeration algorithms. We present results for high-order motifs in several biological networks and discuss their possible functions.  Availability: A software tool for estimating subgraph concentrations and detecting network motifs (mfinder 1.1) and further information is available at http://www.weizmann.ac.il/mcb/UriAlon/ 10.1093/bioinformatics/bth163',\n",
       " \"Design as bricolage: anthropology meets design thinking. We identify a metaphor for the design activity: we view design as bricolage. We start from describing bricolage, and we proceed to the relationship of design to art. We obtain a characterisation of design that enables us to show that both traditional and contemporary design are forms of bricolage. We examine the consequences of 'design as bricolage' for the relationship between design and science and for the extent of the design activity. © 1999 Elsevier Science Ltd. All rights reserved.\",\n",
       " 'Semantic blogging and decentralized knowledge management. In the Semantic Web research group at Hewlett-Packard Laboratories, Bristol, we frequently circulate items of interest (such as news articles, software tools, links to Web sites, and competitor information). We call them snippets, or information nuggets, we would like to store, annotate, and share. Email is not the ideal medium for these tasks; its transient nature means the snippets are effectively lost over time. Yet the risk from using a more formal process, like a centralized database, is that it is both cumbersome to use (a barrier to entry) and overly rigid in its data model (not amenable to storing different types of information). Our need illustrates what I call decentralized, informal knowledge management.',\n",
       " 'Serendipity and information seeking: an empirical study. \"Serendipity\" has both a classical origin in literature and a more modern manifestation where it is found in the descriptions of the problem solving and knowledge acquisition of humanities and science scholars. Studies of information retrieval and information seeking have also discussed the utility of the notion of serendipity. Some have implied that it may be stimulated, or that certain people may \"encounter\" serendipitous information more than others. All to some extent accept the classical definition of serendipity as a \"fortuitous\" accident. The analysis presented here is part of a larger study concerning the information-seeking behaviour of interdisciplinary scholars. This paper considers the nature of serendipity in information-seeking contexts, and reinterprets the notion of serendipity as a phenomenon arising from both conditions and strategies - as both a purposive and a non-purposive component of information seeking and related knowledge acquisition.',\n",
       " 'Re-appraising information seeking behaviour in a digital environment: Bouncers, checkers, returnees and the like. Collating data from a number of log and questionnaire studies conducted largely into the use of a range of consumer health digital information platforms, Centre for Information Behaviour and the Evaluation of Research (Ciber) researchers describe some new thoughts on characterising (and naming) information seeking behaviour in the digital environment, and in so doing, suggest a new typology of digital users. The characteristic behaviour found is one of bouncing in which users seldom penetrate a site to any depth, tend to visit a number of sites for any given information need and seldom return to sites they once visited. They tend to feed for information horizontally, and whether they search a site of not depends heavily on digital visibility, which in turn creates all the conditions for bouncing. The question whether this type of information seeking represents a form of dumbing down or up, and what it all means for publishers, librarians and information providers, who might be working on other, possible outdated usage paradigms, is discussed.',\n",
       " 'Detection of multistability, bifurcations, and hysteresis in a large class of biological positive-feedback systems.. It is becoming increasingly clear that bistability (or, more generally, multistability) is an important recurring theme in cell signaling. {B}istability may be of particular relevance to biological systems that switch between discrete states, generate oscillatory responses, or \"remember\" transitory stimuli. {S}tandard mathematical methods allow the detection of bistability in some very simple feedback systems (systems with one or two proteins or genes that either activate each other or inhibit each other), but realistic depictions of signal transduction networks are invariably much more complex. {H}ere, we show that for a class of feedback systems of arbitrary order the stability properties of the system can be deduced mathematically from how the system behaves when feedback is blocked. {P}rovided that this open-loop, feedback-blocked system is monotone and possesses a sigmoidal characteristic, the system is guaranteed to be bistable for some range of feedback strengths. {W}e present a simple graphical method for deducing the stability behavior and bifurcation diagrams for such systems and illustrate the method with two examples taken from recent experimental studies of bistable systems: a two-variable {C}dc2/{W}ee1 system and a more complicated five-variable mitogen-activated protein kinase cascade.',\n",
       " 'Emergence of Complex Dynamics in a Simple Model of Signaling Networks. 10.1073/pnas.0404843101 Various physical, social, and biological systems generate complex fluctuations with correlations across multiple time scales. In physiologic systems, these long-range correlations are altered with disease and aging. Such correlated fluctuations in living systems have been attributed to the interaction of multiple control systems; however, the mechanisms underlying this behavior remain unknown. Here, we show that a number of distinct classes of dynamical behaviors, including correlated fluctuations characterized by 1/ scaling of their power spectra, can emerge in networks of simple signaling units. We found that, under general conditions, complex dynamics can be generated by systems fulfilling the following two requirements, () a “small-world” topology and () the presence of noise. Our findings support two notable conclusions. First, complex physiologic-like signals can be modeled with a minimal set of components; and second, systems fulfilling conditions  and  are robust to some degree of degradation (i.e., they will still be able to generate 1/ dynamics).',\n",
       " 'Semantic Web services. hose properties, capabilities,  interfaces, and effects are encoded in an unambiguous,  machine-understandable form.  The realization of the Semantic Web is underway  with the development of new AI-inspired content  markup languages, such as OIL,  3  DAML+OIL  (www.daml.org/2000/10/daml-oil), and DAML-L (the  last two are members of the DARPA Agent Markup  Language (DAML) family of languages).  4  These languages  have a well-defined semantics and enable the  markup and manipulation of complex taxonomic and  logical relations between entities on the Web. A fundamental  component of the Semantic Web will be the markup of Web services to make them computer-interpretable, use-apparent, and agent-ready. This article addresses precisely this component.  We present an approach to Web service markup that provides an agent-independent declarative API capturing the data and metadata associated with a service together with specifications of its pro',\n",
       " 'Genomeâ\\x80\\x90wide detection of tissueâ\\x80\\x90specific alternative splicing in the human transcriptome. We have developed an automated method for discovering tissueâ\\x80\\x90specific regulation of alternative splicing through a genomeâ\\x80\\x90wide analysis of expressed sequence tags (ESTs). Using this approach, we have identified 667 tissueâ\\x80\\x90specific alternative splice forms of human genes. We validated our muscleâ\\x80\\x90specific and brainâ\\x80\\x90specific splice forms for known genes. A high fraction (8/10) were reported to have a matching tissue specificity by independent studies in the published literature. The number of tissueâ\\x80\\x90specific alternative splice forms is highest in brain, while eye_retina, muscle, skin, testis and lymph have the greatest enrichment of tissueâ\\x80\\x90specific splicing. Overall, 10â\\x80\\x9330% of human alternatively spliced genes in our data show evidence of tissueâ\\x80\\x90specific splice forms. Seventyâ\\x80\\x90eight percent of our tissueâ\\x80\\x90specific alternative splices appear to be novel discoveries. We present bioinformatics analysis of several tissueâ\\x80\\x90specific splice forms, including automated protein isoform sequence and domain prediction, showing how our data can provide valuable insights into gene function in different tissues. For example, we have discovered a novel kidneyâ\\x80\\x90specific alternative splice form of the WNK1 gene, which appears to specifically disrupt its Nâ\\x80\\x90terminal kinase domain and may play a role in PHAII hypertension. Our database greatly expands knowledge of tissueâ\\x80\\x90specific alternative splicing and provides a comprehensive dataset for investigating its functional roles and regulation in different human tissues.',\n",
       " 'Inferring quantitative models of regulatory networks from expression data.. Motivation: Genetic networks regulate key processes in living cells. Various methods have been suggested to reconstruct network architecture from gene expression data. However, most approaches are based on qualitative models that provide only rough approximations of the underlying events, and lack the quantitative aspects that are critical for understanding the proper function of biomolecular systems.  Results: We present fine-grained dynamical models of gene transcription and develop methods for reconstructing them from gene expression data within the framework of a generative probabilistic model. Unlike previous works, we employ quantitative transcription rates, and simultaneously estimate both the kinetic parameters that govern these rates, and the activity levels of unobserved regulators that control them. We apply our approach to expression datasets from yeast and show that we can learn the unknown regulator activity profiles, as well as the binding affinity parameters. We also introduce a novel structure learning algorithm, and demonstrate its power to accurately reconstruct the regulatory network from those datasets. 10.1093/bioinformatics/bth941',\n",
       " 'Using Bayesian networks to analyze expression data.. DNA hybridization arrays simultaneously measure the expression level for thousands of genes. These measurements provide a “snapshot ” of transcription levels within the cell. A major challenge in computational biology is to uncover, from such measurements, gene/protein interactions and key biological features of cellular systems. In this paper, we propose a new framework for discovering interactions between genes based on multiple expression measurements. This framework builds on the use of Bayesian networks for representing statistical dependencies. A Bayesian network is a graph-based model of joint multivariate probability distributions that captures properties of conditional independence between variables. Such models are attractive for their ability to describe complex stochastic processes and because they provide a clear methodology for learning from (noisy) observations. We start by showing how Bayesian networks can describe interactions between genes. We then describe a method for recovering gene interactions from microarray data using tools for learning Bayesian networks. Finally, we demonstrate this method on the S. cerevisiae cell-cycle measurements of Spellman et al. (1998). Key words: gene expression, microarrays, Bayesian methods. 1.',\n",
       " 'Combining location and expression data for principled discovery of genetic regulatory network models.. We develop principled methods for the automatic induction (discovery) of genetic regulatory network models from multiple data sources and data modalities. Models of regulatory networks are represented as Bayesian networks, allowing the models to compactly and robustly capture probabilistic multivariate statistical dependencies between the various cellular factors in these networks. We build on previous Bayesian network validation results by extending the validation framework to the context of model induction, leveraging heuristic simulated annealing search algorithms and posterior model averaging. Using expression data in isolation yields results inconsistent with location data so we incorporate genomic location data to guide the model induction process. We combine these two data modalities by allowing location data to influence the model prior and expression data to influence the model likelihood. We demonstrate the utility of this approach by discovering genetic regulatory models of thirty-three variables involved in S. cerevisiae pheromone response. The models we automatically generate are consistent with the current understanding regarding this regulatory network, but also suggest new directions for future experimental investigation. 1',\n",
       " 'Using graphical models and genomic expression data to statistically validate models of genetic regulatory networks.. We propose a model-driven approach for analyzing genomic expression data that permits genetic regulatory networks to be represented in a biologically interpretable computational form. Our models permit latent variables capturing unobserved factors, describe arbitrarily complex (more than pair-wise) relationships at varying levels of refinement, and can be scored rigorously against observational data. The models that we use are based on Bayesian networks and their extensions. As a demonstration of this approach, we utilize 52 genomes worth of Affymetrix GeneChip expression data to correctly differentiate between alternative hypotheses of the galactose regulatory network in S. cerevisiae. When we extend the graph semantics to permit annotated edges, we are able to score models describing relationships at a finer degree of specification. 1 Introduction The vast quantity of data generated by genomic expression arrays affords researchers a significant opportunity to transform biology, medicine, and pharmacology using systematic computational methods. The availability of genomic (and eventually proteomic) expression data promises to have a profound impact on the understanding of basic cellular processes, the diagnosis and treatment of disease, and the efficacy of designing and delivering targeted therapeutics. Particularly relevant to these objectives is the development of a deeper understanding of the various mechanisms by which cells control and regulate the transcription of their genes. In this paper, we present a principled method for using genomic expression data to elucidate these genetic regulatory networks. While the potential utility of expression data is immense, some obstacles',\n",
       " 'Advances to Bayesian network inference for generating causal networks from observational biological data. Motivation: Network inference algorithms are powerful computational tools for identifying putative causal interactions among variables from observational data. Bayesian network inference algorithms hold particular promise in that they can capture linear, non-linear, combinatorial, stochastic and other types of relationships among variables across multiple levels of biological organization. However, challenges remain when applying these algorithms to limited quantities of experimental data collected from biological systems. Here, we use a simulation approach to make advances in our dynamic Bayesian network (DBN) inference algorithm, especially in the context of limited quantities of biological data.  Results: We test a range of scoring metrics and search heuristics to find an effective algorithm configuration for evaluating our methodological advances. We also identify sampling intervals and levels of data discretization that allow the best recovery of the simulated networks. We develop a novel influence score for DBNs that attempts to estimate both the sign (activation or repression) and relative magnitude of interactions among variables. When faced with limited quantities of observational data, combining our influence score with moderate data interpolation reduces a significant portion of false positive interactions in the recovered networks. Together, our advances allow DBN inference algorithms to be more effective in recovering biological networks from experimentally collected data.  Availability: Source code and simulated data are available upon request.  Supplementary information: http://www.jarvislab.net/Bioinformatics/BNAdvances/ 10.1093/bioinformatics/bth448',\n",
       " 'A survey of approaches to automatic schema matching. Abstract. Schema matching is a basic problem in many database application domains, such as data integration, E-business, data warehousing, and semantic query processing. In current implementations, schema matching is typically performed manually, which has significant limitations. On the other hand, previous research papers have proposed many techniques to achieve a partial automation of the match operation for specific application domains. We present a taxonomy that covers many of these existing approaches, and we describe the approaches in some detail. In particular, we distinguish between schema-level and instance-level, element-level and structure-level, and language-based and constraint-based matchers. Based on our classification we review some previous match implementations thereby indicating which part of the solution space they cover. We intend our taxonomy and review of past work to be useful when comparing different approaches to schema matching, when developing a new match algorithm, and when implementing a schema matching component.',\n",
       " \"C-OWL: Contextualizing ontologies. Ontologies are shared models of a domain that encode a view which is common to a set of different parties. Contexts are local models that encode a party's subjective view of a domain. In this paper we show how ontologies can be contextualized, thus acquiring certain useful properties that a pure shared approach cannot provide. We say that an ontology is contextualized or, also, that it is a contextual ontology, when its contents are kept local, and therefore not shared with other ontologies, and mapped with the contents of other ontologies via explicit (context) mappings. The result is Context OWL (C-OWL), a language whose syntax and semantics have been obtained by extending the OWL syntax and semantics to allow for the representation of contextual ontologies.\",\n",
       " 'Integrated Semantic-Syntactic Video Modeling for Search and Browsing. Video processing and computer vision communities usually employ shot-based or object-based structural video models and associate low-level (color, texture, shape, and motion) and semantic descriptions (textual annotations) with these structural (syntactic) elements. Database and information retrieval communities, on the other hand, employ entity-relation or object-oriented models to model the semantics of multimedia documents. This paper proposes a new generic integrated semantic-syntactic video model to include all of these elements within a single framework to enable structured video search and browsing combining textual and low-level descriptors. The proposed model includes semantic entities (video objects and events) and the relations between them. We introduce a new \"actor\" entity to enable grouping of object roles in specific events. This context-dependent classification of attributes of an object allows for more efficient browsing and retrieval. The model also allows for decomposition of events into elementary motion units and elementary reaction/interaction units in order to access mid-level semantics and low-level video features. The instantiations of the model are expressed as graphs. Users can formulate flexible queries that can be translated into such graphs. Alternatively, users can input query graphs by editing an abstract model (model template). Search and retrieval is accomplished by matching the query graph with those instantiated models in the database. Examples and experimental results are provided to demonstrate the effectiveness of the proposed integrated modeling and querying framework.',\n",
       " 'Understanding and Using Context. Context is a poorly used source of information in our computing environments. As a result, we have an impoverished understanding of what context is and how it can be used. In this paper, we provide an operational definition of context and discuss the different ways that context can be used by context-aware applications. We also present the Context Toolkit, an architecture that supports the building of these context-aware applications. We discuss the features and abstractions in the toolkit that make the task of building applications easier. Finally, we introduce a new abstraction, a situation, which we believe will provide additional support to application designers.  1. Introduction  Humans are quite successful at conveying ideas to each other and reacting appropriately. This is due to many factors: the richness of the language they share, the common understanding of how the world works, and an implicit understanding of everyday situations. When humans talk with humans, they are able...',\n",
       " 'Time as essence for photo browsing through personal digital libraries. We developed two photo browsers for collections with thousands of time-stamped digital images. Modern digital cameras record photo shoot times, and semantically related photos tend to occur in bursts. Our browsers exploit the timing information to structure the collections and to automatically generate meaningful summaries. The browsers differ in how users navigate and view the structured collections. We conducted user studies to compare the two browsers and an un-summarized image browser. Our results show that exploiting the time dimension and appropriately summarizing collections can lead to significant improvements. For example, for one task category, one of our browsers enabled a 33\\\\\\\\ improvement in speed of finding given images compared to the commercial browser. Similarly, users were able to complete 29% more tasks when using this same browser.',\n",
       " 'Automatic organization for digital photographs with geographic coordinates. Uses time and location information in the EXIF header to automatically organise a personal photograph collection. Foruses on auto-genereating a structure that can be used WITHOUT A MAP. Makes a nice point about the inneficient use of screen real estate when using maps, and that zooming an paning could be cumbersome. The paper presents the algorithm for grouping by time and location for event detection. The tool generates a text caption to describe (by location) each grouping--this is crucial since no map. Description/evaluation of the user interface is left for future work. Evaluation: the algorithms are tested on three image collections (2580, 1192, 1823 images) . This could be interesting to look at again when writing the machine learning stuff.',\n",
       " 'Two supervised learning approaches for name disambiguation in author citations. Due to name abbreviations, identical names, name misspellings,  and pseudonyms in publications or bibliographies (citations), an  author may have multiple names and multiple authors may share  the same name. Such name ambiguity affects the performance  of document retrieval, web search, database integration, and may  cause improper attribution to authors. This paper investigates two  supervised learning approaches to disambiguate authors in the citations.   One approach uses the naive Bayes probability model, a  generative model; the other uses Support Vector Machines(SVMs)  [The Nature of Statistical Learning Theory] and the vector space representation of citations,   a discriminative model. Both approaches utilize three types of citation attributes:  co-author names, the title of the paper, and the title of  the journal or proceeding. We illustrate these two approaches on  two types of data, one collected from the web, mainly publication  lists from homepages, the other collected from the DBLP citation databases.',\n",
       " 'Toward an Understanding of the Motivation of Open Source Software Developers. An Open Source Software (OSS) project is unlikely to  be successful unless there is an accompanied community  that provides the platform for developers and users to  collaborate. Members of such communities are volunteers  whose motivation to participate and contribute is of  essential importance to the success of OSS projects. In  this paper, we aim to create an understanding of what  motivates people to participate in OSS communities. We  theorize that learning is one of the motivational forces.  Our theory is grounded in the learning theory of  Legitimate Peripheral Participation, and is supported by  analyzing the social structure of OSS communities and the  co-evolution between OSS systems and communities. We  also discuss practical implications of our theory for  creating and maintaining sustainable OSS communities as  well as for software engineering research and education.',\n",
       " \"Bibliometrics and beyond: some thoughts on web-based citation analysis. The idea of a unified citation index to the literature of science was first outlined by Eugene Garfield [1] in 1955 in the journal Science. Science  Citation Index has since established itself as the gold standard for scientific information retrieval. It has also become the database of choice for citation analysts and evaluative bibliometricians worldwide. As scientific publication moves to the web, and novel approaches to scholarly communication  and peer review establish themselves, new methods of citation and link analysis will emerge to capture often liminal expressions of peer esteem, influence and approbation. The web thus affords bibliometricians rich opportunities to apply   and adapt their techniques to new contexts and content: the age of  bibliometric spectroscopy' [2] is dawning. 10.1177/016555150102700101\",\n",
       " 'Adding semantics to web services standards. With the increasing growth in popularity of Web services, discovery of relevant Web services becomes a significant challenge. One approach is to develop semantic Web services where by the Web services are annotated based on shared ontologies, and use these annotations for semantics-based discovery of relevant Web services. We discuss one such approach that involves adding semantics to WSDL using DAML+OIL ontologies. Our approach also uses UDDI to store these semantic annotations and search for...',\n",
       " 'Analysis of a very large web search engine query log. In this paper we present an analysis of an AltaVista Search Engine query log consisting of approximately 1 billion entries for search requests over a period of six weeks. This represents almost 285 million user sessions, each an attempt to fill a single information need. We present an analysis of individual queries, query duplication, and query sessions. We also present results of a correlation analysis of the log entries, studying the interaction of terms within queries. Our data supports the conjecture that web users differ significantly from the user assumed in the standard information retrieval literature. Specifically, we show that web users type in short queries, mostly look at the first 10 results only, and seldom modify the query. This suggests that traditional information retrieval techniques may not work well for answering web search requests. The correlation analysis showed that the most highly correlated items are constituents of phrases. This result indicates it may be useful for search engines to consider search terms as parts of phrases even if the user did not explicitly specify them as such.',\n",
       " \"Relevant term suggestion in interactive web search based on contextual information in query session logs. Abstract 10.1002/asi.10256.abs This paper proposes an effective term suggestion approach to interactive Web search. Conventional approaches to making term suggestions involve extracting co-occurring keyterms from highly ranked retrieved documents. Such approaches must deal with term extraction difficulties and interference from irrelevant documents, and, more importantly, have difficulty extracting terms that are conceptually related but do not frequently co-occur in documents. In this paper, we present a new, effective log-based approach to relevant term extraction and term suggestion. Using this approach, the relevant terms suggested for a user query are those that co-occur in similar query sessions from search engine logs, rather than in the retrieved documents. In addition, the suggested terms in each interactive search step can be organized according to its relevance to the entire query session, rather than to the most recent single query as in conventional approaches. The proposed approach was tested using a proxy server log containing about two million query transactions submitted to search engines in Taiwan. The obtained experimental results show that the proposed approach can provide organized and highly relevant terms, and can exploit the contextual information in a user's query session to make more effective suggestions.\",\n",
       " \"Agglomerative clustering of a search engine query log. This paper introduces a technique for mining a collection of user transactions with an Internet search engine to discover clusters of similar queries and similar URLs. The information we exploit is clickthrough data: each record consists of a user's query to a search engine along with the URL which the user selected from among the candidates offered by the search engine. By viewing this dataset as a bipartite graph, with the vertices on one side corresponding to queries and on the other side...\",\n",
       " 'Region-Based Memory Management. This paper describes a memory management discipline for programs that perform dynamic memory allocation and de-allocation. At runtime, all values are put into regions. The store consists of a stack of regions. All points of region allocation and deallocation are inferred automatically, using a type and effect based program analysis. The scheme does not assume the presence of a garbage collector. The scheme was first presented by Tofte and Talpin (1994); subsequently, it has been tested in The ML Kit with Regions, a region-based, garbage-collection free implementation of the Standard ML Core language, which includes recursive datatypes, higher-order functions and updatable references (Birkedal et al. 96, Elsman and Hallenberg 95). This paper defines a region-based dynamic semantics for a skeletal programming language extracted from Standard ML. We present the inference system which specifies where regions can be allocated and de-allocated and a detailed proof that the system is sound wi...',\n",
       " \"Language Support for Regions. Region-based memory management systems structure memory by grouping objects in regions under program control. Memory is reclaimed by deleting regions, freeing all objects stored therein. Our compiler for C with regions, RC, prevents unsafe region deletions by keeping a count of references to each region. Using type annotations that make the structure of a program's regions more explicit, we reduce the overhead of reference counting from a maximum of 27\\\\% to a maximum of 11\\\\% on a suite of realistic benchmarks. We generalise these annotations in a region type system whose main novelty is the use of existentially quantified abstract regions to represent pointers to objects whose region is partially or totally unknown. A distribution of RC is available at http://www.cs.berkeley.edu/~dgay/rc.tar.gz.\",\n",
       " 'A Syntactic Approach to Type Soundness. We present a new approach to proving type soundness for Hindley/Milner-style polymorphic type systems. The keys to our approach are (1) an adaptation of subject reduction theorems from combinatory logic to programming languages, and (2) the use of rewriting techniques for the specification of the language semantics. The approach easily extends from polymorphic functional languages to imperative languages that provide references, exceptions, continuations, and similar features. We illustrate the technique with a type soundness theorem for the core of Standard ML, which includes the first type soundness proof for polymorphic exceptions and continuations.',\n",
       " 'Expertise recommender: a flexible recommendation system and architecture. Locating the expertise necessary to solve difficult problems is a nuanced social and collaborative problem. In organizations, some people assist others in locating expertise by making referrals. People who make referrals fill key organizational roles that have been identified by CSCW and affiliated research. Expertise locating systems are not designed to replace people who fill these key organizational roles. Instead, expertise locating systems attempt to decrease workload and support people who have no other options. Recommendation systems are collaborative software that can be applied to expertise locating. This work describes a general recommendation architecture that is grounded in a field study of expertise locating. Our expertise recommendation system details the work necessary to fit expertise recommendation to a work setting. The architecture and implementation begin to tease apart the technical aspects of providing good recommendations from social and collaborative concerns.',\n",
       " \"FotoFile: a consumer multimedia organization and retrieval system. FotoFile blends automatic and human annotation methods. Users express frustration re: organizing and retrieving digital images. Conducted focus groups to gain insight into the percieved tradeoffs between: manual vs. automated annotattion and direct search vs. browsing. Different focus groups were held for business and home participants. Keyword based search easiest to grasp, but time requuirements seen as daunting. Automated indexing (content based) desired, but home users thought they'd do keywords more, browsing more desired by home users than business. The primary orgaizational metaphor is an album, and useres can specify the representative image to aid in recognition. Batch annotation is supported, annotation and search use the same basic mechanism. Storytelling is discussed as a means of facilitating annotation--allows for small grouping of photos, part of a story--called scraplets, to be named and annotated. Using the same photos in mutliple scraplets links them implicitly--links are shown during album playback to remind of multiple possible story lines. Face recognition is implemented and black rectangles are placed on thumbnails to indicate recognized faces--users are prompted to confirm person's identity, all images auto annoted w/ the person's name. Metadata is visualized using the hyperbolic tree. Good discussion of the difficulties in designing meaningful studies in this area.\",\n",
       " \"Composable and compilable macros:: you want it when?. Many macro systems, especially for Lisp and Scheme, allow macro transformers to perform general computation. Moreover, the language for implementing compile-time macro transformers is usually the same as the language for implementing run-time functions. As a side effect of this sharing, implementations tend to allow the mingling of compile-time values and run-time values, as well as values from separate compilations. Such mingling breaks programming tools that must parse code without executing it. Macro implementors avoid harmful mingling by obeying certain macro-definition protocols and by inserting phase-distinguishing annotations into the code. However, the annotations are fragile, the protocols are not enforced, and programmers can only reason about the result in terms of the compiler's implementation. MzScheme---the language of the PLT Scheme tool suite---addresses the problem through a macro system that separates compilation without sacrificing the expressiveness of macros.\",\n",
       " 'Assessing Agreement on Classification Tasks: The Kappa Statistic. Currently, computational linguists and cognitive scientists working in the area of discourse and dialogue argue that their subjective judgments are reliable using several different statistics, none of which are easily interpretable or comparable to each other. Meanwhile, researchers in content analysis have already experienced the same difficulties and come up with a solution in the kappa statistic. We discuss what is wrong with reliability measures as they are currently used for discourse and dialogue work in computational linguistics and cognitive science, and argue that we would be better off as a field adopting techniques from content analysis.',\n",
       " 'A deoxyribozyme-based molecular automaton. We describe a molecular automaton, called MAYA, which encodes a version of the game of tic-tac-toe and interactively competes against a human opponent. The automaton is a Boolean network of deoxyribozymes that incorporates 23 molecular-scale logic gates and one constitutively active deoxyribozyme arrayed in nine wells (3x3) corresponding to the game board. To make a move, MAYA carries out an analysis of the input oligonucleotide keyed to a particular move by the human opponent and indicates a move by fluorescence signaling in a response well. The cycle of human player input and automaton response continues until there is a draw or a victory for the automaton. The automaton cannot be defeated because it implements a perfect strategy.',\n",
       " 'Refactoring object-oriented frameworks. This thesis de\\x0cnes a set of program restructuring operations (refactorings) that support the design, evolution and reuse of object-oriented application frameworks. The focus of the thesis is on automating the refactorings in a way that preserves the behavior of a program. The refactorings are de\\x0cned to be behavior preserving, provided that their preconditions are met. Most of the refactorings are simple to implement and it is almost trivial to show that they are behavior preserving. However, for a few refactorings, one or more of their preconditions are in general undecidable. Fortunately, for some cases it can be determined whether these refactorings can be applied safely. Three of the most complex refactorings are de\\x0cned in detail: generalizing the inheritance hierarchy, specializing the inheritance hierarchy and using aggregations to model the relationships among classes. These operations are decomposed into more primitive parts, and the power of these operations is discussed from the perspectives of automatability and usefulness in supporting design. Two design constraints needed in refactoring are class invariants and exclusive components. These constraints are needed to ensure that behavior is preserved across some refactorings. This thesis gives some conservative algorithms for determining whether a program satis\\x0ces these constraints, and describes how to use this design information to refactor a program.',\n",
       " 'Imperative Functional Programming. We present a new model, based on monads, for performing input/output in a non-strict, purely functional language. It is composable, extensible, efficient, requires no extensions to the type system, and extends smoothly to incorporate mixed-language working and in-place array updates. 1 Introduction Input/output has always appeared to be one of the less satisfactory features of purely functional languages: fitting action into the functional paradigm feels like fitting a square block into a round ...',\n",
       " 'Meeting the needs of remote library users. Many higher education institutions now offer virtual delivery of academic programs via the Internet and the World Wide Web, and cater to the needs of growing numbers of distance education students. Before an academic library can adequately address the needs, preferences, and expectations of its remote patrons, however, it needs to understand the peculiarities and challenges of serving patrons at a distance. Penn State University\\\\&#039;s World Campus (a virtual campus) is one of the leading distance education institutions in the USA, and its libraries are among those actively engaged in ascertaining the needs of remote library users and developing services and resources to meet these needs. A portrait of library resources and services available to World Campus students is presented, and results of a survey conducted to assess Penn State World Campus students\\\\&#039; perceptions, expectations and use of Web-based library resources are reported. A key finding is that students are pleased with the quality and availability of library services, but not fully partaking of the vast array of services and collections.',\n",
       " 'Library weblogs. A total of 55 weblogs maintained by libraries were identified in late 2003 using Internet search engines and directories. The weblogs were studied using content analysis techniques. Library weblogs were found in just three countries, with the majority being in the USA. Public and academic libraries were more likely to have a weblog than other types of libraries. The most common aim or purpose was to provide news, information and links to Internet resources for library users. Few provided interactive facilities, and when provided, there was little evidence that the facilities were used to any extent. Only one-fifth of the weblogs had been updated within the past day and only half within the previous week. Less than half provided an RSS feed. Given the small number of library weblogs in the study, the question of why so few? is discussed. Finally, the article addresses the implications of the findings for library managers.',\n",
       " \"Five reasons for scenario-based design. Scenarios of human-computer interaction help us to understand and to create computer systems and applications as artifacts of human activity&#209; as things to learn from, as tools to use in one's work, as media for interacting with other people. Scenario-based design of information technology addresses five technical challenges: Scenarios evoke reflection in the content of design work, helping developers coordinate design action and reflection. Scenarios are at once concrete and flexible, helping developers manage the fluidy of design situations. Scenarios afford multiple views of an interaction, diverse kinds and amounts of detailing, helping developers manage the many consequences entailed by any given design move. Scenarios can also be abstracted and categorized, helping designers to recognize, capture, and reuse generalizations, and to address the challenge that technical knowledge often lags the needs of technical design. Finally, scenarios promote work-oriented communication among stakeholders, helping to make design activities more accessible to the great variety of expertise that can contribute to design, and addressing the challenge that external constraints designers and clients often distract attention from the needs and concerns of the people who will use the technology.\",\n",
       " 'CiteSeer-API: towards seamless resource location and interlinking for digital libraries. We introduce CiteSeer-API, a public API to CiteSeer-like services. CiteSeer-API is SOAP/WSDL based and allows for easy programmatical access to all the specific functionalities offered by CiteSeer services, including full text search of documents and citations and citation-based document discovery. In order to enable operability and interlinking with arbitrary software agents and digital library systems, CiteSeer-API uses digital content signatures to create system-independent handles for the...',\n",
       " 'Modeling network dynamics: the lac operon, a case study. We use the lac operon in Escherichia coli as a prototype system to illustrate the current state, applicability, and limitations of modeling the dynamics of cellular networks. We integrate three different levels of description (molecular, cellular, and that of cell population) into a single model, which seems to capture many experimental aspects of the system. [Journal Article, Review, Review, Tutorial; 26 Refs; In English; United States]',\n",
       " 'Library instruction and information literacy - 2003. Purpose - The purpose of this paper is to provide a selected bibliography of recent resources on library instruction and information literacy. Design/methodology/approach - The paper introduces and annotates periodical articles, monographs, and exhibition catalogues examining library instruction and information literacy. Findings - The paper provides information about each source, discusses the characteristics of current scholarship, and describes sources that contain unique scholarly contributions and quality reproductions. Originality/value - The information may be used by librarians and interested parties as a quick reference to literature on library instruction and information literacy.',\n",
       " 'Robust perfect adaptation in bacterial chemotaxis through integral feedback control. Integral feedback control is a basic engineering strategy for ensuring that the output of a system robustly tracks its desired value independent of noise or variations in system parameters. In biological systems, it is common for the response to an extracellular stimulus to return to its prestimulus value even in the continued presence of the signal—a process termed adaptation or desensitization. Barkai, Alon, Surette, and Leibler have provided both theoretical and experimental evidence that the precision of adaptation in bacterial chemotaxis is robust to dramatic changes in the levels and kinetic rate constants of the constituent proteins in this signaling network [Alon, U., Surette, M. G., Barkai, N. & Leibler, S. (1998) Nature (London) 397, 168–171]. Here we propose that the robustness of perfect adaptation is the result of this system possessing the property of integral feedback control. Using techniques from control and dynamical systems theory, we demonstrate that integral control is structurally inherent in the Barkai–Leibler model and identify and characterize the key assumptions of the model. Most importantly, we argue that integral control in some form is necessary for a robust implementation of perfect adaptation. More generally, integral control may underlie the robustness of many homeostatic mechanisms.',\n",
       " 'Information Literacy: The Meta-Competency of the Knowledge Economy? An Exploratory Paper. Information literacy is a meta-competency that encapsulates the generic skills of defining, locating and accessing information. It is an essential and integral competency for both the knowledge worker and effective knowledge management. Librarians need to realign their roles from providers and organizers of information, to facilitators and educators of clients&#146; information access and process. This requires librarians to develop partnerships with workplace communities so as to understand the role that information plays in the knowledge economy; and to develop a new language that is relevant to workplace communities and ties information literacy instruction to the authentic situations of the workplace. Information literacy is a meta-competency: it is the currency of the knowledge economy. The themes introduced in this paper are drawn from the current doctoral research of the author. The research examines what it means to individuals to be information literate in a workplace context, and how information literacy manifests and transfers in to workplace practice.',\n",
       " \"Turning techno-savvy into info-savvy: authentically integrating information literacy into the college curriculum. It is no longer effective to provide information literacy instruction that is thought to be ''good for'' college students, but rather, instruction must focus on the learning styles and preferences of the target population. This case study reports a series of hands-on/minds-on information literacy activities that dissolve student's misconception that ''techno-savvy'' is synonymous with information literate. Careful and thorough instruction in the mining of popular Internet search engines for authoritative information was coupled with instruction in the use of traditional library resources. It was found that the college students studied possess a high need for clarity and a low tolerance for ambiguity, and therefore any activities assigned must be thoroughly, yet succinctly, described in order to achieve success. Combining traditional information literacy instruction with novel approaches appeals to the confidence in and reliance on Internet search engines that college students exhibit, while it moves this microcosm toward a higher level of information literacy and commitment to life long learning.\",\n",
       " \"Research agenda for library instruction and information literacy. Editor's note:  The Association for College and Research Libraries (ACRL) made the following research agenda available in the February 2003 issue of  College & Research Libraries News  (Vol. 64, No. 2, pp. 108–113; reprinted with permission). Although this agenda has already been published, we believe it still merits reprinting, especially within the pages of an internationally focused research journal. The questions raised in ACRL's agenda indeed do deserve investigation, and the results of those investigations should appear in the scholarly literature. We encourage anyone who addresses those questions to first examine  Developing Research & Communication Skills: Guidelines for Information Literacy in the Curriculum  (Philadelphia, PA: Middle States Commission on Higher Education, 2003). This important report links information literacy with assessment and demonstrates the importance of that topic to an educational accreditation body. Furthermore, that linkage adds to (and rounds out) the research agenda. Clearly, information literacy as defined in that report is a topic of interest to anyone in education—higher education or other.\",\n",
       " 'Information-saturated yet ignorant: information mediation as social empowerment in the knowledge economy. In today&#039;s information society, the information citizen must face a variety of challenges in order to make the most of their role in the knowledge economy. The role of information as knowledge capital means that there is a danger of inappropriate commercialisation of information, which can militate against the optimal social use of this resource. Similarly, low levels of information literacy can exclude the individual from full membership of the information society. Information professionals are in a prime position to address these problems, since the information mediator can both act against inappropriate commercialisation of information and offset the social disadvantages of information illiteracy. If the information professional does not rise to the challenge of leadership within the new information order, society become information-saturated and simultaneously ignorant.',\n",
       " 'Case-based, problem-based learning - Information literacy for the real world. Describes case-based, problem-based learning (CBPBL), a student-centered approach that uses tightly focused minicases to help students demonstrate their ability to identify their information needs. Topics include the role of the instructor; uses of CBPBL; librarian as facilitator or liaison for a CBPBL session; examples of minicases for library instruction; and group size. (Author/LRW)',\n",
       " 'Information and digital literacies: a review of concepts. The concepts of &#096;information literacy&#039; and &#096;digital literacy&#039; are described, and reviewed, by way of a literature survey and analysis. Related concepts, including computer literacy, library literacy, network literacy, Internet literacy and hyper-literacy are also discussed, and their relationships elucidated. After a general introduction, the paper begins with the basic concept of &#096;literacy&#039;, which is then expanded to include newer forms of literacy, more suitable for complex information environments. Some of these, for example library, media and computer literacies, are based largely on specific skills, but have some extension beyond them. They lead togeneral concepts, such as information literacy and digital literacy which are based on knowledge, perceptions and attitudes, though reliant on the simpler skills-based literacies',\n",
       " 'Integrating information literacy using problem-based learning. Teaching information literacy skills is increasingly difficult as the number of students entering the university demonstrate an extraordinary confidence using technology. Students and subject area faculty often do not grasp the subtle difference between being technology proficient and being information literate. Some faculty are even beginning to dismiss library instruction by saying my students already know how to use the Internet. This paper introduces a new method for teaching essential information literacy skills, combined with problem solving techniques, to develop, promote, and assess critical and analytical thinking of students further (and faculty) using information technologies today.',\n",
       " 'Robustness in simple biochemical networks.. Cells use complex networks of interacting molecular components to transfer and process information. These \"computational devices of living cells\"1 are responsible for many important cellular processes, including cell-cycle regulation and signal transduction. Here we address the issue of the sensitivity of the networks to variations in their biochemical parameters. We propose a mechanism for robust adaptation in simple signal transduction networks. We show that this mechanism applies in particular to bacterial chemotaxis2, 3, 4, 5, 6, 7. This is demonstrated within a quantitative model which explains, in a unified way, many aspects of chemotaxis, including proper responses to chemical gradients8, 9, 10, 11, 12. The adaptation property10, 13, 14, 15, 16 is a consequence of the network\\'s connectivity and does not require the \\'fine-tuning\\' of parameters. We argue that the key properties of biochemical networks should be robust in order to ensure their proper functioning.',\n",
       " 'Association of genes to genetically inherited diseases using data mining. Although approximately one-quarter of the roughly 4,000 genetically inherited diseases currently recorded in respective databases (LocusLink1, OMIM2) are already linked to a region of the human genome, about 450 have no known associated gene. Finding disease-related genes requires laborious examination of hundreds of possible candidate genes (sometimes, these are not even annotated; see, for example, refs 3,4). The public availability of the human genome5 draft sequence has fostered new strategies to map molecular functional features of gene products to complex phenotypic descriptions, such as those of genetically inherited diseases. Owing to recent progress in the systematic annotation of genes using controlled vocabularies6, we have developed a scoring system for the possible functional relationships of human genes to 455 genetically inherited diseases that have been mapped to chromosomal regions without assignment of a particular gene. In a benchmark of the system with 100 known disease-associated genes, the disease-associated gene was among the 8 best-scoring genes with a 25% chance, and among the best 30 genes with a 50% chance, showing that there is a relationship between the score of a gene and its likelihood of being associated with a particular disease. The scoring also indicates that for some diseases, the chance of identifying the underlying gene is higher.',\n",
       " \"The evolution of Lisp. Lisp is the world's greatest programming language&mdash;or so its proponents think. The structure of Lisp makes it easy to extend the language or even to implement entirely new dialects without starting from scratch. Overall, the evolution of Lisp has been guided more by institutional rivalry, one-upsmanship, and the glee born of technical cleverness that is characteristic of the &ldquo;hacker culture&rdquo; than by sober assessments of technical requirements. Nevertheless this process has eventually produced both an industrial-strength programming language, messy but powerful, and a technically pure dialect, small but powerful, that is suitable for use by programming-language theoreticians.   We pick up where McCarthy's paper in the first HOPL conference left off. We trace the development chronologically from the era of the PDP-6, through the heyday of Interlisp and MacLisp, past the ascension and decline of special purpose Lisp machines, to the present era of standardization activities. We then examine the technical evolution of a few representative language features, including both some notable successes and some notable failures, that illuminate design issues that distinguish Lisp from other programming languages. We also discuss the use of Lisp as a laboratory for designing other programming languages. We conclude with some reflections on the forces that have driven the evolution of Lisp.\",\n",
       " 'Template meta-programming for Haskell. We propose a new extension to the purely functional programming language Haskell that supports  compile-time meta-programming.  The purpose of the system is to support the  algorithmic  construction of programs at compile-time.The ability to generate code at compile time allows the programmer to implement such features as polytypic programs, macro-like expansion, user directed optimization (such as inlining), and the generation of supporting data structures and functions from existing data structures and functions.Our design is being implemented in the Glasgow Haskell Compiler, ghc.',\n",
       " 'Revised report on the algorithmic language scheme. The report gives a defining description of the programming language Scheme. Scheme is a statically scoped and properly tail-recursive dialect of the Lisp programming language invented by Guy Lewis Steele, Jr. and Gerald Jay Sussman. It was designed to have an exceptionally clear and simple semantics and few different ways to form expressions. A wide variety of programming paradigms, including imperative, functional, and message passing styles, find convenient expression in Scheme.',\n",
       " 'Proper tail recursion and space efficiency. The IEEE/ANSI standard for Scheme requires implementations to be properly tail recursive. This ensures that portable code can rely upon the space efficiency of continuation-passing style and other idioms. On its face, proper tail recursion concerns the efficiency of procedure calls that occur within a tail context. When examined closely, proper tail recursion also depends upon the fact that garbage collection can be asymptotically more space-efficient than Algol-like stack allocation. Proper...',\n",
       " 'Control flow analysis in scheme. Traditional flow analysis techniques, such as the ones typically employed by optimizing Fortran compilers, do not work for Scheme-like languages. This paper presents a flow analysis technique &mdash;  control flow analysis  &mdash; which is applicable to Scheme-like languages. As a demonstration application, the information gathered by control flow analysis is used to perform a traditional flow analysis problem, induction variable elimination. Extensions and limitations are discussed.   The techniques presented in this paper are backed up by working code. They are applicable not only to Scheme, but also to related languages, such as Common Lisp and ML.',\n",
       " 'The essence of functional programming. This paper explores the use monads to structure functional programs. No prior knowledge of monads or category theory is required. Monads increase the ease with which programs may be modified. They can mimic the effect of impure features such as exceptions, state, and continuations; and also provide effects not easily achieved with such features. The types of a program reflect which effects occur. Monads increase the ease with which programs may be modified. They can mimic the effect of impure features such as exceptions, state, and continuations; and also provide effects not easily achieved with such features. The types of a program reflect which effects occur. The first section is an extended example of the use of monads. A simple interpreter is modified to support various extra features: error messages, state, output, and non-deterministic choice. The second section describes the relation between monads and the continuation-passing style. The third section sketches how monads are used in a compiler for Haskell that is written in Haskell.',\n",
       " 'Contracts for higher-order functions. Assertions play an important role in the construction of robust software. Their use in programming languages dates back to the 1970s. Eiffel, an object-oriented programming language, wholeheartedly adopted assertions and developed the \"Design by Contract\" philosophy. Indeed, the entire object-oriented community recognizes the value of assertion-based contracts on methods.In contrast, languages with higher-order functions do not support assertion-based contracts. Because predicates on functions are, in general, undecidable, specifying such predicates appears to be meaningless. Instead, the functional languages community developed type systems that statically approximate interesting predicates.In this paper, we show how to support higher-order function contracts in a theoretically well-founded and practically viable manner. Specifically, we introduce &#955; con , a typed lambda calculus with assertions for higher-order functions. The calculus models the assertion monitoring system that we employ in DrScheme. We establish basic properties of the model (type soundness, etc.) and illustrate the usefulness of contract checking with examples from DrScheme\\'s code base.We believe that the development of an assertion system for higher-order functions serves two purposes. On one hand, the system has strong practical potential because existing type systems simply cannot express many assertions that programmers would like to state. On the other hand, an inspection of a large base of invariants may provide inspiration for the direction of practical future type system research.',\n",
       " \"Units: cool modules for HOT languages. A module system ought to enable  assembly-line programming  using separate compilation and an expressive linking language. Separate compilation allows programmers to develop parts of a program independently. A linking language gives programmers precise control over the assembly of parts into a whole. This paper presents models of  program units , MzScheme's module language for assembly-line programming. Units support separate compilation, independent module reuse, cyclic dependencies, hierarchical structuring, and dynamic linking. The models explain how to integrate units with untyped and typed languages such as Scheme and ML.\",\n",
       " 'A call-by-need lambda calculus. The mismatch between the operational semantics of the lambda calculus and the actual behavior of implementations is a major obstacle for compiler writers. They cannot explain the behavior of their evaluator in terms of source level syntax, and they cannot easily compare distinct implementations of different lazy strategies. In this paper we derive an equational characterization of call-by-need and prove it correct with respect to the original lambda calculus. The theory is a strictly smaller theory than the lambda calculus. Immediate applications of the theory concern the correctness proofs of a number of implementation strategies,  e.g. , the call-by-need continuation passing transformation and the realization of sharing via assignments.',\n",
       " 'On the expressive power of programming languages. The literature on programming languages contains an abundance of informal claims on the relative expressive power of programming languages, but there is no framework for formalizing such statements nor for deriving interesting consequences. As a first step in this direction, we develop a formal notion of expressiveness and investigate its properties. To validate the theory, we analyze some widely held beliefs about the expressive power of several extensions of functional languages. Based on these results, we believe that our system correctly captures many of the informal ideas on expressiveness, and that it constitutes a foundation for further research in this direction. 1 Comparing Programming Languages The literature on programming languages contains an abundance of informal claims on the expressive power of programming languages. Arguments in these contexts typically assert the expressibility or non-expressibility of programming constructs relative to a language. Unfortunately, pro...',\n",
       " 'Catching bugs in the web of program invariants. MrSpidey is a user-friendly, interactive static debugger for Scheme. A static debugger supplements the standard debugger by analyzing the program and pinpointing those program operations that may cause run-time errors such as dereferencing the null pointer or applying non-functions. The program analysis of MrSpidey computes value set descriptions for each term in the program and constructs a value flow graph connecting the set descriptions. Using the set descriptions, MrSpidey can identify and highlight potentially erroneous program operations, whose cause the programmer can then explore by selectively exposing portions of the value flow graph.',\n",
       " 'Synthesizing Object-Oriented and Functional Design to Promote Re-Use. Many problems require recursively specified types of data and a collection of tools that operate on those data. Over time, these problems evolve so that the programmer must extend the toolkit or extend the types and adjust the existing tools accordingly. Ideally, this should be done without modifying existing code. Unfortunately, the prevailing program design strategies do not support both forms of extensibility: functional programming accommodates the addition of tools, while object-oriented programming supports either adding new tools or extending the data set, but not both. In this paper, we present a composite design pattern that synthesizes the best of both approaches and in the process resolves the tension between the two design strategies. We also show how this protocol suggests a new set of linguistic facilities for languages that support class systems.',\n",
       " 'Modular object-oriented programming with units and mixins. Module and class systems have evolved to meet the demand for reuseable software components. Considerable effort has been invested in developing new module and class systems, and in demonstrating how each promotes code reuse. However, relatively little has been said about the interaction of these constructs, and how using modules and classes together  can improve programs. In this paper, we demonstrate the synergy of a particular form of modules and classes---called units and mixins,...',\n",
       " \"How to design programs: an introduction to programming and computing. {This introduction to programming places computer science in the core of a liberal arts education. Unlike other introductory books, it focuses on the program design process. This approach fosters a variety of skills--critical reading, analytical thinking, creative synthesis, and attention to detail--that are important for everyone, not just future computer programmers.<br /> <br /> The book exposes readers to two fundamentally new ideas. First, it presents program design guidelines that show the reader how to analyze a problem statement; how to formulate concise goals; how to make up examples; how to develop an outline of the solution, based on the analysis; how to finish the program; and how to test. Each step produces a well-defined intermediate product. Second, the book comes with a novel programming environment, the first one explicitly designed for beginners. The environment grows with the readers as they master the material in the book until it supports a full-fledged language for the whole spectrum of programming tasks.<br /> <br /> All the book's support materials are available for free on the Web. The Web site includes the environment, teacher guides, exercises for all levels, solutions, and additional projects.}\",\n",
       " 'Macros as multi-stage computations: type-safe, generative, binding macros in MacroML. With few exceptions, macros have traditionally been viewed as operations on syntax trees or even on plain strings. This view makes macros seem ad hoc, and is at odds with two desirable features of contemporary typed functional languages: static typing and static scoping. At a deeper level, there is a need for a simple, usable semantics for macros. This paper argues that these problems can be addressed by formally viewing macros as multi-stage computations. This view eliminates the need for freshness conditions and tests on variable names, and provides a compositional interpretation that can serve as a basis for designing a sound type system for languages supporting macros, or even for compilation. To illustrate our approach, we develop and present MacroML, an extension of ML that supports inlining, recursive macros, and the definition of new binding constructs. The latter is subtle, and is the most novel addition in a statically typed setting. The semantics of a core subset of MacroML is given by an interpretation into MetaML, a statically-typed multi-stage programming language. It is then easy to show that MacroML is stage- and type-safe: macro expansion does not depend on runtime evaluation, and both stages do not \"go wrong.',\n",
       " \"A general definition of metabolic pathways useful for systematic organization and analysis of complex metabolic networks. A set of linear pathways often does not capture the full range of behaviors of a metabolic network. {T}he concept of 'elementary flux modes' provides a mathematical tool to define and comprehensively describe all metabolic routes that are both stoichiometrically and thermodynamically feasible for a group of enzymes. {W}e have used this concept to analyze the interplay between the pentose phosphate pathway ({PPP}) and glycolysis. {T}he set of elementary modes for this system involves conventional glycolysis, a futile cycle, all the modes of {PPP} function described in biochemistry textbooks, and additional modes that are a priori equally entitled to pathway status. {A}pplications include maximizing product yield in amino acid and antibiotic synthesis, reconstruction and consistency checks of metabolism from genome data, analysis of enzyme deficiencies, and drug target identification in metabolic networks.\",\n",
       " 'Metabolic pathways in the post-genome era.. Metabolic pathways are a central paradigm in biology. Historically, they have been defined on the basis of their step-by-step discovery. However, the genome-scale metabolic networks now being reconstructed from annotation of genome sequences demand new network-based definitions of pathways to facilitate analysis of their capabilities and functions, such as metabolic versatility and robustness, and optimal growth rates. This demand has led to the development of a new mathematically based analysis of complex, metabolic networks that enumerates all their unique pathways that take into account all requirements for cofactors and byproducts. Applications include the design of engineered biological systems, the generation of testable hypotheses regarding network structure and function, and the elucidation of properties that can not be described by simple descriptions of individual components (such as product yield, network robustness, correlated reactions and predictions of minimal media). Recently, these properties have also been studied in genome-scale networks. Thus, network-based pathways are emerging as an important paradigm for analysis of biological systems.',\n",
       " 'A hybrid approach for searching in the semantic web. This paper presents a search architecture that combines classical search techniques with spread activation techniques applied to a semantic model of a given domain. Given an ontology, weights are assigned to links based on certain properties of the ontology, so that they measure the strength of the relation. Spread activation techniques are used to find related concepts in the ontology given an initial set of concepts and corresponding initial activation values. These initial values are obtained from the results of classical search applied to the data associated with the concepts in the ontology. Two test cases were implemented, with very positive results. It was also observed that the proposed hybrid spread activation, combining the symbolic and the sub-symbolic approaches, achieved better results when compared to each of the approaches alone.',\n",
       " \"Adaptive web search based on user profile constructed without any effort from users. Web search engines help users find useful information on the World Wide Web (WWW). However, when the same query is submitted by different users, typical search engines return the same result regardless of who submitted the query. Generally, each user has different information needs for his/her query. Therefore, the search result should be adapted to users with different information needs. In this paper, we first propose several approaches to adapting search results according to each user's need for relevant information without any user effort, and then verify the effectiveness of our proposed approaches. Experimental results show that search systems that adapt to each user's preferences can be achieved by constructing user profiles based on modified collaborative filtering with detailed analysis of user's browsing history in one day.\",\n",
       " 'A Corpus-Based Approach to Language Learning. A CORPUS-BASED APPROACH TO LANGUAGE LEARNING Eric Brill Supervisor: Mitchell Marcus One goal of computational linguistics is to discover a method for assigning a rich structural annotation to sentences that are presented as simple linear strings of words; meaning can be much more readily extracted from a structurally annotated sentence than from a sentence with no structural information. Also, structure allows for a more in-depth check of the well-formedness of a sentence. There are two phases...',\n",
       " \"N-Gram-Based Text Categorization. Text categorization is a fundamental task in document processing, allowing the automated handling of enormous streams of documents in electronic form. One difficulty in handling some classes of documents is the presence of different kinds of textual errors, such as spelling and grammatical errors in email, and character recognition errors in documents that come through OCR. Text categorization must work reliably on all input, and thus must tolerate some level of these kinds of problems. We describe here an N-gram-based approach to text categorization that is tolerant of textual errors. The system is small, fast and robust. This system worked very well for language classification, achieving in one test a 99.8% correct classification rate on Usenet newsgroup articles written in different languages. The system also worked reasonably well for classifying articles from a number of different computer-oriented newsgroups according to subject, achieving as high as an 80% correct classification rate. There are also several obvious directions for improving the system's classification performance in those cases where it did not do as well. The system is based on calculating and comparing profiles of N-gram frequencies. First, we use the system to compute profiles on training set data that represent the various categories, e.g.,language samples or newsgroup content samples. Then the system computes a profile for a particular document that is to be classified. Finally, the system computes a distance measure between the document's profile and each of the category profiles. The system selects the category whose profile has the smallest distance to the document's profile. The profiles involved are quite small, typically 10K bytes for a category training set, and less than 4K bytes for an individual document. Using N-gram frequency profiles provides a simple and reliable way to categorize documents in a wide range of classification tasks.\",\n",
       " 'Blogging by the rest of us. Weblogs (or blogs) are frequently updated webpages with posts typically in reverse-chronological order. Blogging is the latest form of online communication to gain widespread popularity and it is rapidly becoming mainstream. Media attention tends to focus on \"heavy-hitting\" blogs devoted to politics, punditry and technology, but it has recently become apparent that vast majority of blogs are written by ordinary people for much smaller audiences, and on largely personal themes. Surprisingly little is known about this \"blogging by the rest of us\", especially from the blogger\\'s point of view. This paper presents the preliminary results of an ethnographic study of blogging as a form of personal expression and communication. We characterize a number of blogging practices, and then consider blogging as personal journaling. We find blogging to be a surprisingly versatile medium, with uses similar to an online diary, personal chronicle or newsletter, and much more. The next few years should provide a fascinating opportunity for research and design as blogging tools improve and blog usage evolves and flourishes.',\n",
       " 'The use of popular science articles in teaching scientific literacy. This article considers the use of popular science articles in teaching scientific literacy. Comparing the discourse features of popular science with research article and textbook science - the last two being target forms for students - it argues that popular science articles cannot serve as models for scientific writing. It does, however, suggest that popular articles can make science more accessible to students, and so can play a useful role in the teaching of scientific writing as well as in the teaching of science. This is because popular science articles view scientific findings as provisional rather than as incontrovertible fact as they are presented in textbooks or as they appear to be presented in research articles. Another feature of popular articles is that they are peopled with large numbers of specific scientists, thus representing scientists as ordinary people rather than as a few exceptional people of iconic status as is the case in textbooks.',\n",
       " \"Assessing the Effects of Library Instruction. The current study sought to measure the influence of a one-hour library training/orientation session on college students' library use and library skill development. Data analysis revealed a statistically significant increase in student library use while there was no statistically significant increase in library skill development. Some issues, however, limited the utility of the findings. This paper also critiques the study's design with the intention of helping future researchers plan for occurrences that may limit the accuracy of their findings.\",\n",
       " \"Using Blackboard in Library Instruction: Addressing the Learning Styles of Generations X and Y. Studies show that recent generations of college students have a learning style with identifiable characteristics. Library instruction efforts must adapt to these learning styles. Course management software (CMS), such as Blackboard, is one resource available to academic librarians to meet the challenges posed by the ``Net Generation.'' At Stetson University, the use of Blackboard courseware in library instruction sessions successfully addressed the unique learning styles of students.\",\n",
       " 'TextTiling: segmenting text into multi-paragraph subtopic passages. TextTiling is a technique for subdividing texts into multi-paragraph units that represent passages, or subtopics. The discourse cues for identifying major subtopic shifts are patterns of lexical co-occurrence and distribution. The algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts. Multi-paragraph subtopic segmentation should be useful for many text analysis tasks, including information retrieval and summarization.',\n",
       " 'On Defining Library and Information Science as Applied Philosophy of Information. This paper analyses the relations between philosophy of information (PI), library and information science (LIS) and social epistemology (SE). In the first section, it is argued that there is a natural relation between philosophy and LIS but that SE cannot provide a satisfactory foundation for LIS. SE should rather be seen as sharing with LIS a common ground, represented by the study of information, to be investigated by a new discipline, PI. In the second section, the nature of PI is outlined as the philosophical area that studies the conceptual nature of information, its dynamics and problems. In the third section, LIS is defined as a form of applied PI. The hypothesis supported is that PI should replace SE as the philosophical discipline that can best provide the conceptual foundation for LIS. In the conclusion, it is suggested that the &#039;identity&#039; crisis undergone by LIS has been the natural outcome of a justified but precocious search for a philosophical counterpart that has emerged only recently: namely, PI. The development of LIS should not rely on some borrowed, pre-packaged theory. As applied PI, LIS can fruitfully contribute to the growth of basic theoretical research in PI itself and thus provide its own foundation.',\n",
       " 'Embodying information systems: the contribution of phenomenology. This paper presents a case for embodying information systems. That is, for recognizing the fundamental importance of the body in human cognition and social action, and exploring the consequences for information systems and artificial intelligence. Current work within philosophy, biology, cognitive science, and social theory demonstrates that the Cartesian dualism of mind and body is no longer tenable, and points to the embodied and enactive nature of thought and language. Three different approaches to cognition are identified and their underlying philosophies are exemplified by Husserl, Heidegger, and Merleau-Ponty respectively. Sections of the paper cover: a philosophical and biological framework for embodied cognition; the main arguments in favor of the approach; and the implications for information systems and artificial intelligence.',\n",
       " \"Learning to cluster web search results. Organizing Web search results into clusters facilitates users' quick browsing through search results. Traditional clustering techniques are inadequate since they don't generate clusters with highly readable names. In this paper, we reformalize the clustering problem as a salient phrase ranking problem. Given a query and the ranked list of documents (typically a list of titles and snippets) returned by a certain Web search engine, our method first extracts and ranks salient phrases as candidate cluster names, based on a regression model learned from human labeled training data. The documents are assigned to relevant salient phrases to form candidate clusters, and the final clusters are generated by merging these candidate clusters. Experimental results verify our method's feasibility and effectiveness.\",\n",
       " \"A common open representation of mass spectrometry data and its application to proteomics research. A broad range of mass spectrometers are used in mass spectrometry (MS)-based proteomics research. Each type of instrument possesses a unique design, data system and performance specifications, resulting in strengths and weaknesses for different types of experiments. Unfortunately, the native binary data formats produced by each type of mass spectrometer also differ and are usually proprietary. The diverse, nontransparent nature of the data structure complicates the integration of new instruments into preexisting infrastructure, impedes the analysis, exchange, comparison and publication of results from different experiments and laboratories, and prevents the bioinformatics community from accessing data sets required for software development. Here, we introduce the 'mzXML' format, an open, generic XML (extensible markup language) representation of MS data. We have also developed an accompanying suite of supporting programs. We expect that this format will facilitate data management, interpretation and dissemination in proteomics research.\",\n",
       " 'Making sense of it all: bacterial chemotaxis.. Bacteria must be able to respond to a changing environment, and one way to respond is to move. The transduction of sensory signals alters the concentration of small phosphorylated response regulators that bind to the rotary flagellar motor and cause switching. This simple pathway has provided a paradigm for sensory systems in general. However, the increasing number of sequenced bacterial genomes shows that although the central sensory mechanism seems to be common to all bacteria, there is added complexity in a wide range of species.',\n",
       " 'Mobile Ambients. We introduce a calculus describing the movement of processes and devices, including movement through administrative domains. Keywords: Agents, process calculi, mobility, wide-area computation. 1 Introduction  There are two distinct areas of work in mobility: mobile computing, concerning computation that is carried out in mobile devices (laptops, personal digital assistants, etc.), and mobile computation, concerning mobile code that moves between devices (applets, agents, etc.). We aim to...',\n",
       " 'Generalized Venn diagrams: a new method of visualizing complex genetic set relations.. MOTIVATION: Microarray experiments generate vast amounts of data. The unknown or only partially known functional context of differentially expressed genes may be assessed by querying the Gene Ontology database via GOMiner. Resulting tree representations are difficult to interpret and are not suited for visualization of this type of data. Methods are needed to effectively visualize these complex set relationships. RESULTS: We present a visualization approach for set relationships based on Venn diagrams. The proposed extension enhances the usual notion of Venn diagrams by incorporating set size information. The cardinality of the sets and intersection sets is represented by their corresponding circle (polygon) sizes. To avoid local minima, solutions to this problem are sought by evolutionary optimization. This generalized Venn diagram approach has been implemented as an interactive Java application (VennMaster) specifically designed for use with GOMiner in the context of the Gene Ontology database. AVAILABILITY: VennMaster is platform-independent (Java 1.4.2) and has been tested on Windows (XP, 2000), Mac OS X, and Linux. Supplementary information and the software (free for non-commercial use) are available at http://www.informatik.uni-ulm.de/ni/mitarbeiter/HKestler/vennm together with a user documentation. CONTACT: hans.kestler@medizin.uni-ulm.de.',\n",
       " 'Integrated Genomic and Proteomic Analyses of a Systematically Perturbed Metabolic Network. We demonstrate an integrated approach to build, test, and refine a model of a cellular pathway, in which perturbations to critical pathway components are analyzed using DNA microarrays, quantitative proteomics, and databases of known physical interactions. Using this approach, we identify 997 messenger RNAs responding to 20 systematic perturbations of the yeast galactose-utilization pathway, provide evidence that approximately 15 of 289 detected proteins are regulated posttranscriptionally, and identify explicit physical interactions governing the cellular response to each perturbation. We refine the model through further iterations of perturbation and global measurements, suggesting hypotheses about the regulation of galactose utilization and physical interactions between this and a variety of other metabolic pathways.',\n",
       " 'Transcriptional Regulatory Networks in Saccharomyces cerevisiae. We have determined how most of the transcriptional regulators encoded in the eukaryote {S}accharomyces cerevisiae associate with genes across the genome in living cells. {J}ust as maps of metabolic networks describe the potential pathways that may be used by a cell to accomplish metabolic processes, this network of regulator-gene interactions describes potential pathways yeast cells can use to regulate global gene expression programs. {W}e use this information to identify network motifs, the simplest units of network architecture, and demonstrate that an automated process can use motifs to assemble a transcriptional regulatory network structure. {O}ur results reveal that eukaryotic cellular functions are highly connected through networks of transcriptional regulators that regulate other transcriptional regulators.',\n",
       " 'Programmable Syntax Macros. Lisp has shown that a programmable syntax macro system acts as an adjunct to the compiler that gives the programmer important and powerful abstraction facilities not provided by the language. Unlike simple token substitution macros, such as are provided by CPP #the C preprocessor#, syntax macros operate on Abstract Syntax Trees #ASTs#. Programmable syntax macro systems have not yet been developed for syntactically rich languages such as C because rich concrete syntax requires the manual...',\n",
       " 'Network structure and the diffusion of knowledge. This paper models knowledge diffusion as a barter process in which agents exchange different types of knowledge. This is intended to capture the observed practice of informal knowledge trading. Agents are located on a network and are directly connected with a small number of other agents. Agents repeatedly meet those with whom direct connections exist and trade if mutually profitable trades exist. In this way knowledge diffuses throughout the economy. We examine the relationship between network architecture and diffusion performance. We consider the space of structures that fall between, at one extreme, a network in which every agent is connected to  n  nearest neighbours, and at the other extreme a network with each agent being connected to, on average,  n  randomly chosen agents. We find that the performance of the system exhibits clear ‘small world’ properties, in that the steady-state level of average knowledge is maximal when the structure is a small world (that is, when most connections are local, but roughly 10 percent of them are long distance). The variance of knowledge levels among agents is maximal in the small world region, whereas the coefficient of variation is minimal. We explain these results as reflecting the dynamics of knowledge transmission as affected by the architecture of connections among agents.',\n",
       " 'Gene regulatory network growth by duplication. We are beginning to elucidate transcriptional regulatory networks on a large scale and to understand some of the structural principles of these networks, but the evolutionary mechanisms that form these networks are still mostly unknown. {H}ere we investigate the role of gene duplication in network evolution. {G}ene duplication is the driving force for creating new genes in genomes: at least 50\\\\% of prokaryotic genes and over 90\\\\% of eukaryotic genes are products of gene duplication. {T}he transcriptional interactions in regulatory networks consist of multiple components, and duplication processes that generate new interactions would need to be more complex. {W}e define possible duplication scenarios and show that they formed the regulatory networks of the prokaryote {E}scherichia coli and the eukaryote {S}accharomyces cerevisiae. {G}ene duplication has had a key role in network evolution: more than one-third of known regulatory interactions were inherited from the ancestral transcription factor or target gene after duplication, and roughly one-half of the interactions were gained during divergence after duplication. {I}n addition, we conclude that evolution has been incremental, rather than making entire regulatory circuits or motifs by duplication with inheritance of interactions.',\n",
       " 'A proposed framework for the description of plant metabolomics experiments and their results. The study of the metabolite complement of biological samples, known as metabolomics, is creating large amounts of data, and support for handling these data sets is required to facilitate meaningful analyses that will answer biological questions. We present a data model for plant metabolomics known as ArMet (architecture for metabolomics). It encompasses the entire experimental time line from experiment definition and description of biological source material, through sample growth and preparation to the results of chemical analysis. Such formal data descriptions, which specify the full experimental context, enable principled comparison of data sets, allow proper interpretation of experimental results, permit the repetition of experiments and provide a basis for the design of systems for data storage and transmission. The current design and example implementations are freely available (http://www.armet.org/). We seek to advance discussion and community adoption of a standard for metabolomics, which would promote principled collection, storage and transmission of experiment data.',\n",
       " 'Improved monomeric red, orange and yellow fluorescent proteins derived from Discosoma sp. red fluorescent protein. Fluorescent proteins are genetically encoded, easily imaged reporters crucial in biology and biotechnology1, 2. When a protein is tagged by fusion to a fluorescent protein, interactions between fluorescent proteins can undesirably disturb targeting or function3. Unfortunately, all wild-type yellow-to-red fluorescent proteins reported so far are obligately tetrameric and often toxic or disruptive4, 5. The first true monomer was mRFP1, derived from the Discosoma sp. fluorescent protein “DsRed” by directed evolution first to increase the speed of maturation6, then to break each subunit interface while restoring fluorescence, which cumulatively required 33 substitutions7. Although mRFP1 has already proven widely useful, several properties could bear improvement and more colors would be welcome. We report the next generation of monomers. The latest red version matures more completely, is more tolerant of N-terminal fusions and is over tenfold more photostable than mRFP1. Three monomers with distinguishable hues from yellow-orange to red-orange have higher quantum efficiencies.',\n",
       " 'Foundations for the study of software architecture. The purpose of this paper is to build the foundation for software architecture. We first develop an intuition for software architecture by appealing to several well-established architectural disciplines. On the basis of this intuition, we present a model of software architec-ture that consists of three components: elements, form, and rationale. Elements are either processing, data, or connecting elements. Form is defined in terms of the properties of, and the relationships among, the elements-- that is, the constraints on the elements. The ratio-nale provides the underlying basis for the architecture in terms of the system constraints, which most often derive from the system:requirements. We discuss the compo-nents of the model in the context of both architectures and architectural styles and present an extended exam-ple to illustrate some important architecture and style considerations. We conclude by presenting some of the benefits of our approach to software architecture, sum-marizing our contributions, and relating our approach to other current work. 1',\n",
       " 'Location‐Aided Routing (LAR) in mobile ad hoc networks. Abstract&nbsp;&nbsp;A mobile ad hoc network consists of wireless hosts that may move often. Movement of hosts results in a change in routes, requiring some mechanism for determining new routes. Several routing protocols have already been proposed for ad hoc networks. This paper suggests an approach to utilize location information (for instance, obtained using the global positioning system) to improve performance of routing protocols for ad hoc networks. By using location information, the proposed Location‐Aided Routing (LAR) protocols limit the search for a new route to a smaller “request zone” of the ad hoc network. This results in a significant reduction in the number of routing messages. We present two algorithms to determine the request zone, and also suggest potential optimizations to our algorithms.',\n",
       " 'Genetic networks with canalyzing Boolean rules are always stable. We determine stability and attractor properties of random Boolean genetic network models with canalyzing rules for a variety of architectures. For all power law, exponential, and flat in-degree distributions, we find that the networks are dynamically stable. Furthermore, for architectures with few inputs per node, the dynamics of the networks is close to critical. In addition, the fraction of genes that are active decreases with the number of inputs per node. These results are based upon investigating ensembles of networks using analytical methods. Also, for different in-degree distributions, the numbers of fixed points and cycles are calculated, with results intuitively consistent with stability analysis; fewer inputs per node implies more cycles, and vice versa. There are hints that genetic networks acquire broader degree distributions with evolution, and hence our results indicate that for single cells, the dynamics should become more stable with evolution. However, such an effect is very likely compensated for by multicellular dynamics, because one expects less stability when interactions among cells are included. We verify this by simulations of a simple model for interactions among cells.',\n",
       " 'Patterns of Search: Analyzing and Modeling Web Query Refinement. We discuss the construction of probabilistic models centering on temporal patterns of query refinement. Our analyses are derived from a large corpus of Web search queries extracted from server logs recorded by a popular Internet search service. We frame the modeling task in terms of pursuing an understanding of probabilistic relationships among temporal patterns of activity, informational goals, and classes of query refinement. We construct Bayesian networks that predict search behavior, with a focus on the progression of queries over time. We review a methodology for abstracting and tagging user queries. After presenting key statistics on query length, query frequency, and informational goals, we describe user models that capture the dynamics of query refinement.',\n",
       " 'Traits: Composable Units of Behavior. Inheritance is the fundamental reuse mechanism in object-oriented programming languages; its most prominent variants are single inheritance, multiple inheritance, and mixin inheritance. In the first part of this paper, we identify and illustrate the conceptual and practical reusability problems that arise with these forms of inheritance. We then present a simple compositional model for structuring object-oriented programs, which we call traits. Traits are essentially groups of methods that serve as building blocks for classes and are primitive units of code reuse. In this model, classes are composed from a set of traits by specifying glue code that connects the traits together and accesses the necessary state. We demonstrate how traits overcome the problems arising with the different variants of inheritance, we discuss how traits can be implemented effectively, and we summarize our experience applying traits to refactor an existing class hierarchy.',\n",
       " 'Efficient Identification of Web Communities. We define a community on the web as a set of sites that have more links (in either direction) to members of the community than to non-members. Members of such a community can be efficiently identified in a maximum flow / minimum cut framework, where the source is composed of known members, and the sink consists of well-known non-members. A focused crawler that crawls to a fixed depth can approximate community membership by augmenting the graph induced by the crawl with links to a virtual sink...',\n",
       " 'Real life, real users, and real needs: a study and analysis of user queries on the web. . We analyzed transaction logs containing 51,473 queries posed by 18,113 users of Excite, a major Internet search service. We provide data on: (i) sessions - changes in queries during a session, number of pages viewed, and use of relevance feedback, (ii) queries - the number of search terms, and the use of logic and modifiers, and (iii) terms - their rank/frequency distribution and the most highly used search terms. We then shift the focus of analysis from the query to the user to gain insight to the characteristic of the Web user. With these characteristics as a basis, we then conducted a failure analysis, identifying trends among user mistakes. We conclude with a summary of findings and a discussion of the implications of these finding.  INTRODUCTION  A panel session at the 1997 ACM Special Interest Group on Research Issues In Information Retrieval conference entitled &#034;Real Life Information Retrieval: Commercial Search Engines&#034; included representatives from several Internet search se...',\n",
       " \"The “Green” and “Gold” Roads to Open Access: The Case for Mixing and Matching. Recent discussions on Open Access (OA) have tended to treat OA journals and self-archiving as two distinct routes. Some supporters of self-archiving even suggest that it alone can bring about full Open Access to the world's scientific literature. In this paper, it is argued that each route actually corresponds to a phase in the movement toward Open Access; that the mere fact of self-archiving is not enough; that providing some branding ability to the repositories is needed. However, doing so will eventually bring about the creation of overlay (or database) journals. The two roads, therefore, will merge to create a mature OA landscape.\",\n",
       " 'Distance Teaching: Comparing Two Online Information Literacy Courses. This article explores the similarities and differences between two asynchronous online information literacy courses. Details of the courses and how the ACRL information literacy standards are incorporated will be outlined. In exploring distance learning and distance teaching, the article will discuss issues related to online information literacy learning experiences and suggest ways to address those issues and improve teaching and learning.',\n",
       " \"C4.5: programs for machine learning. {<P>Classifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available for download (see below).</p><br><br><p>C4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies.</p><br><br><p>This book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.</p>}\",\n",
       " 'Growing a language. is not the first till it comes to nought; then the first count is the sum.  4    2    5        0    Four plus two is the same as five plus one, which is the same as six plus nought, which is six.  We shall take the word many to mean more than two in number.  Think of a machine that can keep track of two numbers, and count each one up or down, and test if a number be nought and by such a test choose to do this or that. The list of things that it can do and of choices that it can make must be...',\n",
       " 'Packrat Parsing: Simple, Powerful, Lazy, Linear Time. Packrat parsing is a novel technique for implementing parsers in a lazy functional programming language. A packrat parser provides the power and flexibility of top-down parsing with backtracking and unlimited lookahead, but nevertheless guarantees linear parse time. Any language defined by an LL(k) or LR(k) grammar can be recognized by a packrat parser, in addition to many languages that conventional linear-time algorithms do not support. This additional power simplifies the handling of common syntactic idioms such as the widespread but troublesome longest-match rule, enables the use of sophisticated disambiguation strategies such as syntactic and semantic predicates, provides better grammar composition properties, and allows lexical analysis to be integrated seamlessly into parsing. Yet despite its power, packrat parsing shares the same simplicity and elegance as recursive descent parsing; in fact converting a backtracking recursive descent parser into a linear-time packrat parser often involves only a fairly straightforward structural change. This paper describes packrat parsing informally with emphasis on its use in practical applications, and explores its advantages and disadvantages with respect to the more conventional alternatives.',\n",
       " 'Parsing expression grammars: a recognition-based syntactic foundation. For decades we have been using Chomsky&#039;s generative system of grammars, particularly context-free grammars (CFGs) and regular expressions (REs), to express the syntax of programming languages and protocols. The power of generative grammars to express ambiguity is crucial to their original purpose of modelling natural languages, but this very power makes it unnecessarily difficult both to express and to parse machine-oriented languages using CFGs. Parsing Expression Grammars (PEGs) provide an alternative, recognition-based formal foundation for describing machineoriented syntax, which solves the ambiguity problem by not introducing ambiguity in the first place. Where CFGs express nondeterministic choice between alternatives, PEGs instead use prioritized choice. PEGs address frequently felt expressiveness limitations of CFGs and REs, simplifying syntax definitions and making it unnecessary to separate their lexical and hierarchical components. A linear-time parser can be built for any PEG, avoiding both the complexity and fickleness of LR parsers and the inefficiency of generalized CFG parsing. While PEGs provide a rich set of operators for constructing grammars, they are reducible to two minimal recognition schemas developed around 1970, TS/TDPL and gTS/GTDPL, which are here proven equivalent in effective recognition power.',\n",
       " 'Phobos: A front-end approach to extensible compilers. This paper describes a practical approach for implementing domain-specific languages with extensible compilers. Given a compiler with one or more front-end languages, we introduce the idea of a generic front-end that allows the syntactic and semantic specification of domainspecific languages. Phobos, our generic front-end, offers modular language specification, allowing the programmer to define new syntax and semantics incrementally.',\n",
       " 'Searching the World Wide Web. The coverage and recency of the major World Wide Web search engines was analyzed, yielding some surprising results. The coverage of any one engine is significantly limited: No single engine indexes more than about one-third of the &#034;indexable Web,&#034; the coverage of the six engines investigated varies by an order of magnitude, and combining the results of the six engines yields about 3.5 times as many documents on average as compared with the results from only one engine. Analysis of the overlap between pairs of engines gives an estimated lower bound on the size of the indexable Web of 320 million pages.',\n",
       " 'The PageRank Citation Ranking: Bringing Order to the Web. The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a method for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation. 1',\n",
       " 'Random Boolean network models and the yeast transcriptional network. 10.1073/pnas.2036429100 The recently measured yeast transcriptional network is analyzed in terms of simplified Boolean network models, with the aim of determining feasible rule structures, given the requirement of stable solutions of the generated Boolean networks. We find that, for ensembles of generated models, those with canalyzing Boolean rules are remarkably stable, whereas those with random Boolean rules are only marginally stable. Furthermore, substantial parts of the generated networks are frozen, in the sense that they reach the same state, regardless of initial state. Thus, our ensemble approach suggests that the yeast network shows highly ordered dynamics.',\n",
       " 'Organizing Programs Without Classes. . All organizational functions carried out by classes can be accomplished in a simple and natural way by object inheritance in classless languages, with no need for special mechanisms. A single model---dividing types into prototypes and traits---supports sharing of behavior and extending or replacing representations. A natural extension, dynamic object inheritance, can model behavioral modes. Object inheritance can also be used to provide structured name spaces for well-known objects. Classless ...',\n",
       " \"Phylogenetic estimation of context-dependent substitution rates by maximum likelihood.. {Nucleotide substitution in both coding and noncoding regions is context-dependent, in the sense that substitution rates depend on the identity of neighboring bases. Context-dependent substitution has been modeled in the case of two sequences and an unrooted phylogenetic tree, but it has only been accommodated in limited ways with more general phylogenies. In this article, extensions are presented to standard phylogenetic models that allow for better handling of context-dependent substitution, yet still permit exact inference at reasonable computational cost. The new models improve goodness of fit substantially for both coding and noncoding data. Considering context dependence leads to much larger improvements than does using a richer substitution model or allowing for rate variation across sites, under the assumption of site independence. The observed improvements appear to derive from three separate properties of the models: their explicit characterization of context-dependent substitution within N-tuples of adjacent sites, their ability to accommodate overlapping N-tuples, and their rich parameterization of the substitution process. Parameter estimation is accomplished using an expectation maximization algorithm, with a quasi-Newton algorithm for the maximization step; this approach is shown to be preferable to ordinary Newton methods for parameter-rich models. Overlapping tuples are efficiently handled by assuming Markov dependence of the observed bases at each site on those at the N - 1 preceding sites, and the required conditional probabilities are computed with an extension of Felsenstein's algorithm. Estimated substitution rates based on a data set of about 160,000 noncoding sites in mammalian genomes indicate a pronounced CpG effect, but they also suggest a complex overall pattern of context-dependent substitution, comprising a variety of subtle effects. Estimates based on about 3 million sites in coding regions demonstrate that amino acid substitution rates can be learned at the nucleotide level, and suggest that context effects across codon boundaries are significant.}\",\n",
       " 'Types and programming languages. {A type system is a syntactic method for automatically checking the absence of certain erroneous behaviors by classifying program phrases according to the kinds of values they compute. The study of type systems--and of programming languages from a type-theoretic perspective-has important applications in software engineering, language design, high-performance compilers, and security.<br /> <br /> This text provides a comprehensive introduction both to type systems in computer science and to the basic theory of programming languages. The approach is pragmatic and operational; each new concept is motivated by programming examples and the more theoretical sections are driven by the needs of implementations. Each chapter is accompanied by numerous exercises and solutions, as well as a running implementation, available via the Web. Dependencies between chapters are explicitly identified, allowing readers to choose a variety of paths through the material.<br /> <br /> The core topics include the untyped lambda-calculus, simple type systems, type reconstruction, universal and existential polymorphism, subtyping, bounded quantification, recursive types, kinds, and type operators. Extended case studies develop a variety of approaches to modeling the features of object-oriented languages.}',\n",
       " \"The derivative of a regular type is its type of one-hole contexts. Polymorphic regular types are tree-like datatypes generated by polynomial type expressions over a set of free variables and closed under least ﬁxed point. The 'equality types' of Core ML can be expressed in this form. Given such a type expression T with x free, this paper shows a way to represent the one-hole contexts for elements of x within elements of T, together with an operation which will plug an element of x into the hole of such a context. One-hole contexts are given as inhabitants of a regular type dxT, computed generically from the syntactic structure of T by a mechanism better known as partial differentiation. The relevant notion of containment is shown to be appropriately characterized in terms of derivatives and plugging in. The technology is then exploited to give the one-hole contexts for sub-elements of recursive types in a manner similar to Huet's 'zippers.'\",\n",
       " 'Resource usage analysis. It is an important criterion of program correctness that a program accesses resources in a valid manner. For example, a memory region that has been allocated should be eventually deallocated, and after the deallocation, the region should no longer be accessed. A file that has been opened should be eventually closed. So far, most of the methods to analyze this kind of property have been proposed in rather specific contexts (like studies of memory management and verification of usage of lock primitives), and it was not so clear what is the essence of those methods or how methods proposed for individual problems are related. To remedy this situation, we formalize a general problem of analyzing resource usage as a resource usage analysis problem, and propose a type-based method as a solution to the problem.',\n",
       " 'From system F to typed assembly language. We motivate the design of a statically typed assembly language (TAL) and present a typepreserving translation from System F to TAL. The TAL we present is based on a conventional RISC assembly language, but its static type system provides support for enforcing high-level language abstractions, such as closures, tuples, and user-defined abstract data types. The type system ensures that well-typed programs cannot violate these abstractions. In addition, the typing constructs admit most low-level...',\n",
       " 'Mining specifications. Program verification is a promising approach to improving program quality, because it can search all possible program executions for specific errors. However, the need to formally describe correct behavior or errors is a major barrier to the widespread adoption of program verification, since programmers historically have been reluctant to write formal specifications. Automating the process of formulating specifications would remove a barrier to program verification and enhance its practicality.This paper describes  specification mining , a machine learning approach to discovering formal specifications of the protocols that code must obey when interacting with an application program interface or abstract data type. Starting from the assumption that a working program is well enough debugged to reveal strong hints of correct protocols, our tool infers a specification by observing program execution and concisely summarizing the frequent interaction patterns as state machines that capture both temporal and data dependences. These state machines can be examined by a programmer, to refine the specification and identify errors, and can be utilized by automatic verification tools, to find bugs.Our preliminary experience with the mining tool has been promising. We were able to learn specifications that not only captured the correct protocol, but also discovered serious bugs.',\n",
       " \"DSL implementation using staging and monads. The impact of Domain Specific Languages (DSLs) on software design is considerable. They allow programs to be more concise than equivalent programs written in a high-level programming languages. They relieve programmers from making decisions about data-structure and algorithm design, and thus allows solutions to be constructed quickly. Because DSL's are at a higher level of abstraction they are easier to maintain and reason about than equivalent programs written in a highlevel language, and...\",\n",
       " 'Distributed snapshots: determining global states of distributed systems. This paper presents an algorithm by which a process in a distributed system determines a global state of the system during a computation. Many problems in distributed systems can be cast in terms of the problem of detecting global states. For instance, the global state detection algorithm helps to solve an important class of problems: stable property detection. A stable property is one that persists: once a stable property becomes true it remains true thereafter. Examples of stable properties are &ldquo;computation has terminated,&rdquo; &ldquo; the system is deadlocked&rdquo; and &ldquo;all tokens in a token ring have disappeared.&rdquo; The stable property detection problem is that of devising algorithms to detect a given stable property. Global state detection can also be used for checkpointing.',\n",
       " 'Dimensions of relevance. Relevance has become a major area of research in the field of Information Retrieval, despite the fact that the concept relevance is not well understood. This paper models manifestations of relevance within a system of relevance attributes to show that the attributes function in different dimensions for the different manifestations of relevance. It is shown that motivational relevance, as a manifestation of relevance, should not be viewed as part of a linear, objective–subjective scale of relevances, but rather as an attribute of relevance. Similarly, that the manifestation of affective relevance should not be viewed as a discrete category of relevance manifestation, but rather as an influencing factor on the other subjective relevance types. The paper argues a consolidated model of relevance manifestations which includes the notion of socio-cognitive relevance.',\n",
       " 'Open Rating Systems. In the offline world, we look to the people we trust and those they trust for reliable information. In this paper, we present a computational model of this phenomenon and show how it can be used to identify high quality content in an Open Rating System, i.e., a system in which any user can rate content. We present a case study (Epinions.com) of a system based on this model and describe a new platform called PeopleNet for harnessing this phenomenon in an open distributed fashion.',\n",
       " \"Propagation of Trust and Distrust. A (directed) network of people connected by ratings or trust scores, and a model for propagating those trust scores, is a fundamental building block in many of today's most successful e-commerce and recommendation systems. We develop a framework of trust propagation schemes, each of which may be appropriate in certain circumstances, and evaluate the schemes on a large trust network consisting of 800K trust scores expressed among 130K people. We show that a small number of expressed trusts/distrust per individual allows us to predict trust between any two people in the system with high accuracy. Our work appears to be the first to incorporate distrust in a computational trust propagation setting.\",\n",
       " 'A conceptual framework and a toolkit for supporting the rapid prototyping of context-aware applications. Computing devices and applications are now used beyond the desktop, in diverse environments, and this trend toward ubiquitous computing is accelerating. One challenge that remains in this emerging research field is the ability to enhance the behavior of any application by informing it of the context of its use. By context, we refer to any information that characterizes a situation related to the interaction between humans, applications, and the surrounding environment. Context-aware applications promise richer and easier interaction, but the current state of research in this field is still far removed from that vision. This is due to 3 main problems: (a) the notion of context is still ill defined, (b) there is a lack of conceptual models and methods to help drive the design of context-aware applications, and (c) no tools are available to jump-start the development of context-aware applications. In this anchor article, we address these 3 problems in turn. We first define context, identify categories of contextual information, and characterize context-aware application behavior. Though the full impact of context-aware computing requires understanding very subtle and high-level notions of context, we are focusing our efforts on the pieces of context that can be inferred automatically from sensors in a physical environment. We then present a conceptual framework that separates the acquisition and representation of context from the delivery and reaction to context by a context-aware application. We have built a toolkit, the Context Toolkit, that instantiates this conceptual framework and supports the rapid development of a rich space of context-aware applications. We illustrate the usefulness of the conceptual framework by describing a number of context-aware applications that have been prototyped using the Context Toolkit. We also demonstrate how such a framework can support the investigation of important research challenges in the area of context-aware computing.',\n",
       " \"Relevance Feedback Techniques in Interactive Content-Based Image Retrieval. Content-based image retrieval (CBIR) has become one of the most active research areas in the past few years. Many visual feature representations have been explored and many systems built. While these research efforts establish the basis of CBIR, the usefulness of the proposed approaches is limited. Specifically, these efforts have relatively ignored two distinct characteristics of CBIR systems: (1) the gap between high level concepts and low level features; (2) subjectivity of human perception of visual content. This paper proposes a relevance feedback based interactive retrieval approach, which effectively takes into account the above two characteristics in CBIR. During the retrieval process, the user's high level query and perception subjectivity are captured by dynamically updated weights based on the user's relevance feedback. The experimental results show that the proposed approach greatly reduces the user's effort of composing a query and captures the user's information need more precisely.\",\n",
       " 'Image retrieval: current techniques, promising directions and open issues. This paper provides a comprehensive survey of the technical achievements in the research area of image retrieval, especially content-based image retrieval, an area that has been so active and prosperous in the past few years. The survey includes 100+ papers covering the research aspects of image feature representation and extraction, multidimensional indexing, and system design, three of the fundamental bases of content-based image retrieval. Furthermore, based on the state-of-the-art technology available now and the demand from real-world applications, open research issues are identified and future promising research directions are suggested. Copyright 1999 Academic Press.',\n",
       " 'GroupLens: An Open Architecture for Collaborative Filtering of Netnews. Collaborative filters help people make choices based on the opinions of other people. GroupLens is a system for collaborative filtering of netnews, to help people find articles they will like in the huge stream of available articles. News reader clients display predicted scores and make it easy for users to rate articles after they read them. Rating servers, called Better Bit Bureaus, gather and disseminate the ratings. The rating servers predict scores based on the heuristic that people who agreed in the past will probably agree again. Users can protect their privacy by entering ratings under pseudonyms, without reducing the effectiveness of the score prediction. The entire architecture is open: alternative software for news clients and Better Bit Bureaus can be developed independently and can interoperate with the components we have developed. KEYWORDS: Collaborative filtering, information filtering, electronic bulletin boards, social filtering, Usenet, netnews, user model, selective...',\n",
       " \"Letizia: An Agent That Assists Web Browsing. Letizia is a user interface agent that assists a user browsing the World Wide Web. As the user operates a conventional Web browser such as Netscape, the agent tracks user behavior and attempts to anticipate items of interest by doing concurrent, autonomous exploration of links from the user's current position. The agent automates a browsing strategy consisting of a best-first search augmented by heuristics inferring user interest from browsing behavior.  1 Introduction  Letizia Álvarez de...\",\n",
       " 'Simulating protein evolution in sequence and structure space. Naturally occurring proteins comprise a special subset of all plausible sequences and structures selected through evolution. Simulating protein evolution with simplified and all-atom models has shed light on the evolutionary dynamics of protein populations, the nature of evolved sequences and structures, and the extent to which today’s proteins are shaped by selection pressures on folding, structure and function. Extensive mapping of the native structure, stability and folding rate in sequence space using lattice proteins has revealed organizational principles of the sequence/structure map important for evolutionary dynamics. Evolutionary simulations with lattice proteins have highlighted the importance of fitness landscapes, evolutionary mechanisms, population dynamics and sequence space entropy in shaping the generic properties of proteins. Finally, evolutionary-like simulations with all-atom models, in particular computational protein design, have helped identify the dominant selection pressures on naturally occurring protein sequences and structures.',\n",
       " 'Scatter/Gather: A Cluster-based Approach to Browsing Large Document Collections. Document clustering has not been well received as an information retrieval tool. Objections to its use fall into two main categories: first, that clustering is too slow for large corpora (with running time often quadratic in the number of documents); and second, that clustering does not appreciably improve retrieval. We argue that these problems arise only when clustering is used in an attempt to improve conventional search techniques. However, looking at clustering as an information access tool in its own right obviates these objections, and provides a powerful new access paradigm. We present a document browsing technique that employs document clustering as its primary operation. We also present fast (linear time) clustering algorithms which support this interactive browsing paradigm.',\n",
       " 'Data clustering: a review. Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify  cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.',\n",
       " 'Initialization of Iterative Refinement Clustering Algorithms. Iterative refinement clustering algorithms (e.g. K-Means, EM) converge to one of numerous local minima. It is known that they are especially sensitive to initial conditions. We present a procedure for computing a refined starting condition from a given initial one that is based on an efficient technique for estimating the modes of a distribution. The refined initial starting condition leads to convergence to &amp;quot;better &amp;quot; local minima. The procedure is applicable to a wide class of clustering algorithms for both discrete and continuous data. We demonstrate the application of this method to the Expectation Maximization (EM) clustering algorithm and show that refined initial points indeed lead to improved solutions. Refinement run time is considerably lower than the time required to cluster the full database. The method is scalable and can be coupled with a scalable clustering algorithm to address the large-scale clustering in data mining.',\n",
       " 'Mining Association Rules between Sets of Items in Large Databases. We are given a large database of customer transactions. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database. The algorithm incorporates buffer management and novel estimation and pruning techniques. We also present results of applying this algorithm to sales data obtained from a large retailing company, which shows the effectiveness of the algorithm.',\n",
       " 'The Pfam protein families database. {Pfam is a large collection of protein multiple sequence alignments and profile hidden Markov models. Pfam is available on the World Wide Web in the UK at http://www.sanger.ac.uk/Software/Pfam/, in Sweden at http://www.cgb.ki.se/Pfam/, in France at http://pfam.jouy.inra.fr/ and in the US at http://pfam.wustl.edu/. The latest version (6.6) of Pfam contains 3071 families, which match 69\\\\% of proteins in SWISS-PROT 39 and TrEMBL 14. Structural data, where available, have been utilised to ensure that Pfam families correspond with structural domains, and to improve domain-based annotation. Predictions of non-domain regions are now also included. In addition to secondary structure, Pfam multiple sequence alignments now contain active site residue mark-up. New search tools, including taxonomy search and domain query, greatly add to the functionality and usability of the Pfam resource.}',\n",
       " \"Aibo's first words : The social learning of language and meaning. This paper explores the hypothesis that language communication in its very first stage is bootstrapped in a social learning process under the strong influence of culture. A concrete framework for social learning has been de- veloped based on the notion of a language game. Autonomous robots have been programmed to behave according to this framework. We show ex- periments that demonstrate why there has to be a causal role of language on category acquisition; partly by showing that it leads effectively to the bootstrapping of communication and partly by showing that other forms of learning do not generate categories usable in communication or make infor- mation assumptions which cannot be satisfied.\",\n",
       " 'Biological robustness.. Robustness is a ubiquitously observed property of biological systems. {I}t is considered to be a fundamental feature of complex evolvable systems. {I}t is attained by several underlying principles that are universal to both biological organisms and sophisticated engineering systems. {R}obustness facilitates evolvability and robust traits are often selected by evolution. {S}uch a mutually beneficial process is made possible by specific architectural features observed in robust systems. {B}ut there are trade-offs between robustness, fragility, performance and resource demands, which explain system behaviour, including the patterns of failure. {I}nsights into inherent properties of robust systems will provide us with a better understanding of complex diseases and a guiding principle for therapy design.',\n",
       " 'DENIM: finding a tighter fit between tools and practice for Web site design. Through a study of web site design practice, we observed that web site designers design sites at different levels of refinement&mdash;site map, storyboard, and individual page&mdash;and that designers sketch at all levels during the early stages of design. However, existing web design tools do not support these tasks very well. Informed by these observations, we created DENIM, a system that helps web site designers in the early stages of design. DENIM supports sketching input, allows design at different refinement levels, and unifies the levels through zooming. We performed an informal evaluation with seven professional designers and found that they reacted positively to the concept and were interested in using such a system in their work.',\n",
       " 'Network motifs in integrated cellular networks of transcription–regulation and protein–protein interaction. 10.1073/pnas.0306752101 Genes and proteins generate molecular circuitry that enables the cell to process information and respond to stimuli. A major challenge is to identify characteristic patterns in this network of interactions that may shed light on basic cellular mechanisms. Previous studies have analyzed aspects of this network, concentrating on either transcription–regulation or protein–protein interactions. Here we search for composite network motifs: characteristic network patterns consisting of both transcription–regulation and protein–protein interactions that recur significantly more often than in random networks. To this end we developed algorithms for detecting motifs in networks with two or more types of interactions and applied them to an integrated data set of protein–protein interactions and transcription regulation in . We found a two-protein mixed-feedback loop motif, five types of three-protein motifs exhibiting coregulation and complex formation, and many motifs involving four proteins. Virtually all four-protein motifs consisted of combinations of smaller motifs. This study presents a basic framework for detecting the building blocks of networks with multiple types of interactions.',\n",
       " 'Software infrastructure for natural language processing. We classify and review current approaches to software infrastructure for research, development and delivery of NLP systems. The task is motivated by a discussion of current trends in the field of NLP and Language Engineering. We describe a system called GATE (a General Architecture for Text Engineering) that provides a software infrastructure on top of which heterogeneous NLP processing modules may be evaluated and refined individually, or may be combined into larger application systems. GATE...',\n",
       " 'Virtual screening of chemical libraries. Virtual screening uses computer-based methods to discover new ligands on the basis of biological structures. Although widely heralded in the 1970s and 1980s, the technique has since struggled to meet its initial promise, and drug discovery remains dominated by empirical screening. Recent successes in predicting new ligands and their receptor-bound structures, and better rates of ligand discovery compared to empirical screening, have re-ignited interest in virtual screening, which is now widely used in drug discovery, albeit on a more limited scale than empirical screening.',\n",
       " \"Navigating chemical space for biology and medicine.. Despite over a century of applying organic synthesis to the search for drugs, we are still far from even a cursory examination of the vast number of possible small molecules that could be created. Indeed, a thorough examination of all 'chemical space' is practically impossible. Given this, what are the best strategies for identifying small molecules that modulate biological targets? And how might such strategies differ, depending on whether the primary goal is to understand biological systems or to develop potential drugs?\",\n",
       " 'Natural and engineered nucleic acids as tools to explore biology. RNA and DNA molecules can form complex, three-dimensional folded structures that have surprisingly sophisticated functions, including catalysing chemical reactions and controlling gene expression. Although natural nucleic acids make occasional use of these advanced functions, the true potential for sophisticated function by these biological polymers is far greater. An important challenge for biochemists is to take RNA and DNA beyond their proven use as polymers that form double-helical structures. Molecular engineers are beginning to harness the power of nucleic acids that form more complex three-dimensional structures, and apply them as tools for exploring biological systems and as therapeutics.',\n",
       " \"Chemical space and biology. Chemical space — which encompasses all possible small organic molecules, including those present in biological systems — is vast. So vast, in fact, that so far only a tiny fraction of it has been explored. Nevertheless, these explorations have greatly enhanced our understanding of biology, and have led to the development of many of today's drugs. The discovery of new bioactive molecules, facilitated by a deeper understanding of the nature of the regions of chemical space that are relevant to biology, will advance our knowledge of biological processes and lead to new strategies to treat disease.\",\n",
       " \"Informed consent for population-based research involving genetics.. Bridging the gap between gene discovery and our ability to use genetic information to benefit health requires population-based knowledge about the contribution of common gene variants and gene-environment interactions to the risk of disease. The risks and benefits associated with population-based research involving genetics, especially lower-penetrance gene variants, can differ in nature from those associated with family-based research. In response to the urgent need for appropriate guidelines, the Centers for Disease Control and Prevention formed a multidisciplinary group to develop an informed consent approach for integrating genetic variation into population-based research. The group used expert opinion and federal regulations, the National Bioethics Advisory Commission's report on research involving human biological materials, existing consent forms, and literature on informed consent to create suggested language for informed consent documents and a supplemental brochure. This language reflects the premise that the probability and magnitude of harm, as well as possible personal benefits, are directly related to the meaning of the results for the health of the participant and that appropriate disclosures and processes for obtaining consent should be based on an assessment at the outset of the likelihood that the results will generate information that could lead directly to an evidence-based intervention. This informed consent approach is proposed to promote discussion about how best to enable potential participants to make informed decisions about population-based research involving genetics and to suggest issues for consideration by research sponsors, institutional review boards, and investigators.\",\n",
       " 'The Bayou architecture: Support for data sharing among mobile users. The Bayou System is a platform of replicated, highly available, variable-consistency, mobile databases on which to build collaborative applications. This paper presents the preliminary system architecture along with the design goals that influenced it. We take a fresh, bottom-up and critical look at the requirements of mobile computing applications and carefully pull together both new and existing techniques into an overall architecture that meets these requirements. Our emphasis is on supporting application-specific conflict detection and resolution and on providing application-controlled inconsistency',\n",
       " 'Taking email to task: the design and evaluation of a task management centered email tool. Email has come to play a central role in task management, yet email tool features have remained relatively static in recent years, lagging behind users’ evolving practices. The Taskmaster system narrows this gap by recasting email as task management and embedding task-centric resources directly in the client. In this paper, we describe the field research that inspired Taskmaster and the principles behind its design. We then describe how user studies conducted with “live” email data over a two-week period revealed the value of a task-centric approach to email system design and its potential benefits for overloaded users.',\n",
       " 'Collection development for new librarians: Advice from the trenches. There are many challenges facing new librarians in the academic environment, including collection development. This article analyzes the topic of collection development and how it relates to new professionals in the field of librarianship. The article contains a literature review of papers discussing the collection development curriculum in library and information science programs, expected skills required of collection development offices, and library training programs for new librarians. The article also provides practical advise by recent graduates and their collection development experiences. Topics of discussion include acclimation to a new environment, collection development policies and procedures, liaison work, resource selection, and time management.',\n",
       " 'Fast Algorithms for Mining Association Rules. We consider the problem of discovering association rules between items in a large database of sales transactions. We present two new algorithms for solving this problem that are fundamentally different from the known algorithms. Experiments with synthetic as well as real-life data show that these algorithms outperform the known algorithms by factors ranging from three for small problems to more than an order of magnitude for large problems. We also show how the best features of the two proposed algorithms can be combined into a hybrid algorithm, called AprioriHybrid. Scale-up experiments show that AprioriHybrid scales linearly with the number of transactions. AprioriHybrid also has excellent scale-up properties with respect to the transaction size and the number of items in the database. 1',\n",
       " \"Placing Knowledge Management in Context. We welcome the increased emphasis on practice-based theories of knowing as an alternative to the more representational, knowledge-as-object approaches which have characterised many organizational attempts at 'knowledge management' to date. Building on the findings of a short empirical study into the 'knowledge management' initiatives of a global software organization, which highlighted the value of rich context in the generation of meaning, we seek to shed some light on a perceived confusion about the nature of organizational context. We show such context to be an inseparable part of knowing, which it creates and by which it is defined, and re-use Blackler's (1995) taxonomy of 'knowledge types' to illustrate the relational interaction between shared and deeply personal components of context. Finally, we use these insights to suggest a way in which organizations may be able to derive more value from their investments in internal initiatives by increasing their ability to support knowing - and hence the generation of meaning - amongst their employees.\",\n",
       " \"Knowledge Management: The Benefits and Limitations of Computer Systems. Much organisational effort has been put into knowledge management initiatives in recent years, and information and communication technologies (ICTs) have been central to many of these initiatives. However, organisations have found that levering knowledge through ICTs is often hard to achieve. This paper addresses the question of why this is the case, and what we can learn of value to the future practice of knowledge management. The analysis in the paper is based on a human-centred view of knowledge, emphasising the deep tacit knowledge which underpins human thought and action, and the complex sense-reading and sense-giving processes which human beings carry out in communicating with each other and 'sharing' knowledge. The paper concludes that computer-based systems can be of benefit in knowledge-based activities, but only if we are careful in using such systems to support the development and communication of human meaning.\",\n",
       " 'Chord: a scalable peer-to-peer lookup protocol for internet applications. A fundamental problem that confronts peer-to-peer applications is the efficient location of the node that stores a desired data item. This paper presents  Chord , a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis and simulations show that Chord is scalable: Communication cost and the state maintained by each node scale logarithmically with the number of Chord nodes.',\n",
       " 'The chatty web: emergent semantics through gossiping. This paper describes a novel approach for obtaining semantic interoperability among data sources in a bottom-up, semi-automatic manner without relying on pre-existing, global semantic models. We assume that large amounts of data exist that have been organized and annotated according to local schemas. Seeing semantics as a form of agreement, our approach enables the participating data sources to incrementally develop global agreement in an evolutionary and completely decentralized process that solely relies on pair-wise, local interactions: Participants provide translations between schemas they are interested in and can learn about other translations by routing queries (gossiping). To support the participants in assessing the semantic quality of the achieved agreements we develop a formal framework that takes into account both syntactic and semantic criteria. The assessment process is incremental and the quality ratings are adjusted along with the operation of the system. Ultimately, this process results in global agreement, i.e., the semantics that all participants understand. We discuss strategies to efficiently find translations and provide results from a case study to justify our claims. Our approach applies to any system which provides a communication infrastructure (existing websites or databases, decentralized systems, P2P systems) and offers the opportunity to study semantic interoperability as a global phenomenon in a network of information sharing parties.',\n",
       " \"Evolution of transcription factors and the gene regulatory network in Escherichia coli.. The most detailed information presently available for an organism's transcriptional regulation network is that for the prokaryote Escherichia coli. In order to gain insight into the evolution of the E.coli regulatory network, we analysed information obtainable for the domains and protein families of the transcription factors and regulated genes. About three-quarters of the 271 transcription factors we identified are two-domain proteins, consisting of a DNA-binding domain along with a regulatory domain. The regulatory domains mainly bind small molecules. Many groups of transcription factors have identical domain architectures, and this implies that roughly three-quarters of the transcription factors have arisen as a consequence of gene duplication. In contrast, there is little evidence of duplication of regulatory regions together with regulated genes or of transcription factors together with regulated genes. Thirty-eight, out of the 121 transcription factors for which one or more regulated genes are known, regulate other transcription factors. This amplification effect, as well as large differences between the numbers of genes directly regulated by transcription factors, means that there are about 10 global regulators which each control many more genes than the other transcription factors.\",\n",
       " 'Natural image statistics and efficient coding. doi: 10.1088/0954-898X_7_2_014 Natural images contain characteristic statistical regularities that set them apart from purely random images. Understanding what these regularities are can enable natural images to be coded more efficiently. In this paper, we describe some of the forms of structure that are contained in natural images, and we show how these are related to the response properties of neurons at early stages of the visual system. Many of the important forms of structure require higher-order (i.e. more than linear, pairwise) statistics to characterize, which makes models based on linear Hebbian learning, or principal components analysis, inappropriate for finding efficient codes for natural images. We suggest that a good objective for an efficient coding of natural scenes is to maximize the sparseness of the representation, and we show that a network that learns sparse codes of natural scenes succeeds in developing localized, oriented, bandpass receptive fields similar to those in the mammalian striate cortex.This paper was presented at the Workshop on Information Theory and the Brain, held at the University of Stirling, UK, on 4–5 September 1995.',\n",
       " \"The `Independent Components' of Natural Scenes are Edge Filters. It has previously been suggested that neurons with line and edge selectivities found in primary visual cortex of cats and monkeys form a sparse, distributed representation of natural scenes, and it has been reasoned that such responses should emerge from an unsupervised learning algorithm that attempts to find a factorial code of independent visual features. We show here that a new unsupervised learning algorithm based on information maximization, a nonlinear “infomax” network, when applied to an ensemble of natural scenes produces sets of visual filters that are localized and oriented. Some of these filters are Gabor-like and resemble those produced by the sparseness-maximization network. In addition, the outputs of these filters are as independent as possible, since this infomax network performs Independent Components Analysis or ICA, for sparse (super-gaussian) component distributions. We compare the resulting ICA filters and their associated basis functions, with other decorrelating filters produced by Principal Components Analysis (PCA) and zero-phase whitening filters (ZCA). The ICA filters have more sparsely distributed (kurtotic) outputs on natural scenes. They also resemble the receptive fields of simple cells in visual cortex, which suggests that these neurons form a natural, information-theoretic coordinate system for natural images.\",\n",
       " 'Sparse coding with an overcomplete basis set: A strategy employed by V. The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being  localized, oriented , and  bandpass , comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete—i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.',\n",
       " 'Dynamic Typing as Staged Type Inference. Dynamic typing extends statically typed languages with a universal datatype, simplifying programs which must manipulate other programs as data, such as distributed, persistent, interpretive and generic programs. Current approaches, however, limit the use of polymorphism in dynamic values, and can be syntactically awkward. We introduce a new approach to dynamic typing, based on staged computation, which allows a single type-reconstruction algorithm to execute partly at compile time and partly at ...',\n",
       " 'Languages of the Future. This paper explores a new point in the design space of formal reasoning systems - part programming language, part logical framework. The system is built on a programming language where the user expresses equality constraints between types and the type checker then enforces these constraints. This simple extension to the type system allows the programmer to describe properties of his program in the types of  witness  objects which can be thought of as concrete evidence that the program has the property desired. These techniques and two other rich typing mechanisms, rank-N polymorphism and extensible kinds, create a powerful new programming idiom for writing programs whose types enforce semantic properties.A language with these features is  both  a practical programming language  and  a logic. This marriage between two previously separate entities increases the probability that users will apply formal methods to their programming designs. This kind of synthesis creates the foundations for the languages of the future.',\n",
       " 'THE ENGINEERING OF GENE REGULATORY NETWORKS. ▪ Abstract\\u2002 The rapid accumulation of genetic information and advancement of experimental techniques have opened a new frontier in biomedical engineering. With the availability of well-characterized components from natural gene networks, the stage has been set for the engineering of artificial gene regulatory networks with sophisticated computational and functional capabilities. In these efforts, the ability to construct, analyze, and interpret qualitative and quantitative models is becoming increasingly important. In this review, we consider the current state of gene network engineering from a combined experimental and modeling perspective. We discuss how networks with increased complexity are being constructed from simple modular components and how quantitative deterministic and stochastic modeling of these modules may provide the foundation for accurate in silico representations of gene regulatory network function in vivo.',\n",
       " \"Mathematics Is Biology's Next Microscope, Only Better; Biology Is Mathematics' Next Physics, Only Better. Although mathematics has long been intertwined with the biological sciences, an explosive synergy between biology and mathematics seems poised to enrich and extend both fi elds greatly in the coming decades (Levin 1992; Murray 1993; Jungck 1997; Hastings et al. 2003; Palmer et al. 2003; Hastings and Palmer 2003). Biology will increasingly stimulate the creation of qualitatively new realms of mathematics. Why? In biology, ensemble properties emerge at each level of organization from the interactions of heterogeneous biological units at that level and at lower and higher levels of organization (larger and smaller physical scales, faster and slower temporal scales). New mathematics will be required to cope with these ensemble properties and with the heterogeneity of the biological units that compose ensembles at each level.\",\n",
       " 'The Many Faces of Publish/Subscribe. distributed systems Subject descriptor: Distributed applications Keywords: Publish/subscribe, distribution, interaction, large-scale, decoupling Well-adapted to the loosely coupled nature of distributed interaction in large scale applications, the publish/subscribe communication paradigm has recently received an increasing attention. With systems based on the publish/subscribe interaction scheme, subscribers register their interest in an event, or a pattern of events, and are subsequently asynchronously notified of events generated by publishers. Many variants of the paradigm have recently been proposed, each variant being specifically adapted to some given application or network model. This paper factors out the common denominator underlying these variants: full decoupling of the communicating entities in time, space and synchronization. We use these three decoupling dimensions to better identify commonalities and divergences with traditional interaction paradigms. The many variations on the theme of publish/subscribe are classified and synthesized. In particular, their respective benefits and shortcomings are discussed both in terms of interfaces and implementations. 1',\n",
       " 'Filtering algorithms and implementation for very fast publish/subscribe systems. Publish/Subscribe is the paradigm in which users express long-term interests (&ldquo;subscriptions&rdquo;) and some agent &ldquo;publishes&rdquo; events (e.g., offers). The job of Publish/Subscribe software is to send events to the owners of subscriptions satisfied by those events. For example, a user subscription may consist of an interest in an airplane of a certain type, not to exceed a certain price. A published event may consist of an offer of an airplane with certain properties including price. Each subscription consists of a conjunction of (attribute, comparison operator, value) predicates. A subscription closely resembles a trigger in that it is a long-lived conditional query associated with an action (usually, informing the subscriber). However, it is less general than a trigger so novel data structures and implementations may enable the creation of more scalable, high performance publish/subscribe systems. This paper describes an attempt at the construction of such algorithms and its implementation. Using a combination of data structures, application-specific caching policies, and application-specific query processing our system can handle 600 events per second for a typical workload containing 6 million subscriptions.',\n",
       " 'Building programmable jigsaw puzzles with RNA.. One challenge in supramolecular chemistry is the design of versatile, self-assembling building blocks to attain total control of arrangement of matter at a molecular level. We have achieved reliable prediction and design of the three-dimensional structure of artificial RNA building blocks to generate molecular jigsaw puzzle units called tectosquares. They can be programmed with control over their geometry, topology, directionality, and addressability to algorithmically self-assemble into a variety of complex nanoscopic fabrics with predefined periodic and aperiodic patterns and finite dimensions. This work emphasizes the modular and hierarchical characteristics of RNA by showing that small RNA structural motifs can code the precise topology of large molecular architectures. It demonstrates that fully addressable materials based on RNA can be synthesized and provides insights into self-assembly processes involving large populations of RNA molecules.',\n",
       " 'Modeling and performance analysis of BitTorrent-like peer-to-peer networks. In this paper, we develop simple models to study the performance of BitTorrent, a second generation peer-to-peer (P2P) application. We first present a simple fluid model and study the scalability, performance and efficiency of such a file-sharing mechanism. We then consider the built-in incentive mechanism of BitTorrent and study its effect on network performance. We also provide numerical results based on both simulations and real traces obtained from the Internet.',\n",
       " 'A scalable content-addressable network. Hash tables – which map “keys ” onto “values ”  – are an essential building block in modern software systems. We believe a similar functionality would be equally valuable to large distributed systems. In this paper, we introduce the concept of a Content-Addressable Network (CAN) as a distributed infrastructure that provides hash table-like functionality on Internetlike scales. The CAN design is scalable, fault-tolerant and completely selforganizing, and we demonstrate its scalability, robustness and low-latency properties through simulation. 1',\n",
       " \"Tapestry: An Infrastructure for Fault-tolerant Wide-area Location and. In today''s chaotic network, data and services are mobile and replicated widely for availability, durability, and locality. Components within this infrastructure interact in rich and complex ways, greatly stressing traditional approaches to name service and routing. This paper explores an alternative to traditional approaches called Tapestry. Tapestry is an overlay location and routing infrastructure that provides location-independent routing of messages directly to the closest copy of an object or service using only point-to-point links and without centralized resources. The routing and directory information within this infrastructure is purely soft state and easily repaired. Tapestry is self-administering, fault-tolerant, and resilient under load. This paper presents the architecture and algorithms of Tapestry and explores their advantages through a number of experiments.\",\n",
       " 'OceanStore: an architecture for global-scale persistent storage. OceanStore is a utility infrastructure designed to span the globe and provide continuous access to persistent information. Since this infrastructure is comprised of untrusted servers, data is protected through redundancy and cryptographic techniques. To improve performance, data is allowed to be cached anywhere, anytime. Additionally, monitoring of usage patterns allows adaptation to regional outages and denial of service attacks; monitoring also enhances performance through pro-active movement of data. A prototype implementation is currently under development. 1',\n",
       " 'Consistent hashing and random trees: distributed caching protocols for relieving hot spots on the World Wide Web. We describe a family of caching protocols for distributed networks that can be used to decrease or eliminate the occurrence of hot spots in the network. Our protocols are particularly designed for use with very large networks such as the Internet, where delays caused by hot spots can be severe, and where it is not feasible for every server to have complete information about the current state of the entire network. The protocols are easy to implement using existing network protocols such as TCP/IP, and require very little overhead. The protocols work with local control, make efficient use of existing resources, and scale gracefully as the network grows. Our caching protocols are based on a special kind of hashing that we call consistent hashing. Roughly speaking, a consistent hash function is one which changes minimally as the range of the function changes. Through the development of good consistent hash functions, we are able to develop caching protocols which do not require users to ...',\n",
       " \"Peer-to-Peer Architecture Case Study: Gnutella Network. Despite recent excitement generated by the P2P paradigm and despite surprisingly fast deployment of some P2P applications, there are few quantitative evaluations of P2P system behavior. Due to its open architecture and achieved scale, Gnutella is an interesting P2P architecture case study. Gnutella, like most other P2P applications, builds at the application level a virtual network with its own routing mechanisms. The topology of this overlay network and the routing mechanisms used have a significant influence on application properties such as performance, reliability, and scalability. We built a 'crawler' to extract the topology of Gnutella's application level network, we analyze the topology graph and evaluate generated network traffic. We find that although Gnutella is not a pure power-law network, its current configuration has the benefits and drawbacks of a power-law structure. These findings lead us to propose changes to the Gnutella protocol and implementations that bring significant performance and scalability improvements\",\n",
       " 'Freenet: a distributed anonymous information storage and retrieval system. Abstract. We describe Freenet, an adaptive peer-to-peer network application that permits the publication, replication, and retrieval of data while protecting the anonymity of both authors and readers. Freenet operates as a network of identical nodes that collectively pool their storage space to store data files and cooperate to route requests to the most likely physical location of data. No broadcast search or centralized location index is employed. Files are referred to in a location-independent manner, and are dynamically replicated in locations near requestors and deleted from locations where there is no interest. It is infeasible to discover the true origin or destination of a file passing through the network, and difficult for a node operator to determine or be held responsible for the actual physical contents of her own node. 1',\n",
       " 'Placing search in context: the concept revisited. Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (\"the context\"). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web.',\n",
       " 'Statistical analysis of domains in interacting protein pairs.. ABSTRACT Motivation: Several methods have recently been developed to analyse large-scale sets of physical interactions between proteins in terms of physical contacts between the constituent domains, often with a view to predicting new pairwise interactions. Our aim is to combine genomic interaction data, in which domain--domain contacts are not explicitly reported, with the domain-level structure of individual proteins, in order to learn about the structure of interacting protein pairs. Our approach is driven by the need to assess the evidence for physical contacts between domains in a statistically rigorous way. Results: We develop a statistical approach that assigns p-values to pairs of domain superfamilies, measuring the strength of evidence within a set of protein interactions that domains from these superfamilies formcontacts. A set of p-values is calculated for SCOP superfamily pairs, based on a pooled data set of interactions from yeast. These p-values can be used to predict which domains come into contact in an interacting protein pair. This predictive scheme is tested against protein complexes in the Protein Quaternary Structure (PQS) database, and is used to predict domain--domain contacts within 705 interacting protein pairs taken from our pooled data set.',\n",
       " 'Context in Web Search. Web search engines generally treat search requests in isolation. The results for a given query are identical, independent of the user, or the context in which the user made the request. Nextgeneration search engines will make increasing use of context information, either by using explicit or implicit context information from users, or by implementing additional functionality within restricted contexts. Greater use of context in web search may help increase competition and diversity on the web. 1',\n",
       " 'Simple, fast, and practical non-blocking and blocking concurrent queue algorithms. Drawing ideas from previous authors, we present a new non-blocking concurrent queue algorithm and a new twolock queue algorithm in which one enqueue and one dequeue can proceed concurrently. Both algorithms are simple, fast, and practical; we were surprised not to find them in the literature. Experiments on a 12-node SGI Challenge multiprocessor indicate that the new non-blocking queue consistently outperforms the best known alternatives; it is the clear algorithm of choice for machines that...',\n",
       " 'Term identification in the biomedical literature. Sophisticated information technologies are needed for effective data acquisition and integration from a growing body of the biomedical literature. Successful term identification is key to getting access to the stored literature information, as it is the terms (and their relationships) that convey knowledge across scientific articles. Due to the complexities of a dynamically changing biomedical terminology, term identification has been recognized as the current bottleneck in text mining, and—as a consequence—has become an important research topic both in natural language processing and biomedical communities. This article overviews state-of-the-art approaches in term identification. The process of identifying terms is analysed through three steps: term recognition, term classification, and term mapping. For each step, main approaches and general trends, along with the major problems, are discussed. By assessing previous work in context of the overall term identification process, the review also tries to delineate needs for future work in the field.',\n",
       " \"How did alternative splicing evolve?. Alternative splicing creates transcriptome diversification, possibly leading to speciation. A large fraction of the protein-coding genes of multicellular organisms are alternatively spliced, although no regulated splicing has been detected in unicellular eukaryotes such as yeasts. A comparative analysis of unicellular and multicellular eukaryotic 5' splice sites has revealed important differences - the plasticity of the 5' splice sites of multicellular eukaryotes means that these sites can be used in both constitutive and alternative splicing, and for the regulation of the inclusion/skipping ratio in alternative splicing. So, alternative splicing might have originated as a result of relaxation of the 5' splice site recognition in organisms that originally could support only constitutive splicing.\",\n",
       " 'The Google Similarity Distance. Words and phrases acquire meaning from the way they are used in society, from their relative semantics to other words and phrases. For computers, the equivalent of \"society&#8221; is \"database,&#8221; and the equivalent of \"use&#8221; is \"a way to search the database.&#8221; We present a new theory of similarity between words and phrases based on information distance and Kolmogorov complexity. To fix thoughts, we use the World Wide Web (WWW) as the database, and Google as the search engine. The method is also applicable to other search engines and databases. This theory is then applied to construct a method to automatically extract similarity, the Google similarity distance, of words and phrases from the WWW using Google page counts. The WWW is the largest database on earth, and the context information entered by millions of independent users averages out to provide automatic semantics of useful quality. We give applications in hierarchical clustering, classification, and language translation. We give examples to distinguish between colors and numbers, cluster names of paintings by 17th century Dutch masters and names of books by English novelists, the ability to understand emergencies and primes, and we demonstrate the ability to do a simple automatic English-Spanish translation. Finally, we use the WordNet database as an objective baseline against which to judge the performance of our method. We conduct a massive randomized trial in binary classification using support vector machines to learn categories based on our Google distance, resulting in an a mean agreement of 87 percent with the expert crafted WordNet categories.',\n",
       " \"Reinforcement Learning: A Survey. This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.\",\n",
       " \"Information-seeking on the Web:  Effects of user and task variables. This study investigates how cognitive style (field dependence vs. field independence), online database search experience (novice vs. experienced searchers), and task type (known-item vs. subject search tasks) influence users' search behavior on the Web. Forty-eight undergraduate students participated in this study. The participants were divided into four groups according to cognitive style and online database search experience. Each participant searched for information on a university Web site to complete two differing search tasks. The time spent and the number of nodes visited in retrieving information were used to measure users' search performance. The choice of navigation tools was examined to determine users' search and navigational style. It was found that online search experience and cognitive style interacted and influenced search performance, as well as navigational style. Online search experience also interacted with task type to influence navigational style and the number of nodes visited. The article concludes with suggestions for improving Web interfaces and Web-user training programs.\",\n",
       " 'The LCA Problem Revisited. We present a very simple algorithm for the Least Common Ancestor problem. We thus dispel the frequently held notion that an optimal LCA computation is unwieldy and unimplementable. Interestingly, this algorithm is a sequentialization of a previously known PRAM algorithm of Berkman, Breslauer, Galil, Schieber, and Vishkin [1]. Keywords: Data Structures, Least Common Ancestor (LCA), Range Minimum Query (RMQ), Cartesian Tree. 1 Introduction One of the most fundamental algorithmic problems on trees ...',\n",
       " \"Science literacy and academic identity formulation. The purpose of this article is to report findings from an ethnographic study that focused on the co-development of science literacy and academic identity formulation within a third-grade classroom. Our theoretical framework draws from sociocultural theory and studies of scientific literacy. Through analysis of classroom discourse, we identified opportunities afforded students to learn specific scientific knowledge and practices during a series of science investigations. The results of this study suggest that the collective practice of the scientific conversations and activities that took place within this classroom enabled students to engage in the construction of communal science knowledge through multiple textual forms. By examining the ways in which students contributed to the construction of scientific understanding, and then by examining their performances within and across events, we present evidence of the co-development of students' academic identities and scientific literacy. Students' communication and participation in science during the investigations enabled them to learn the structure of the discipline by identifying and engaging in scientific activities. The intersection of academic identities with the development of scientific literacy provides a basis for considering specific ways to achieve scientific literacy for all students. \\\\&copy; 2004 Wiley Periodicals, Inc. J Res Sci Teach 41: 1111-1144, 2004\",\n",
       " \"Developing views of nature of science in an authentic context: An explicit approach to bridging the gap between nature of science and scientific inquiry. Reform efforts emphasize teaching science to promote contemporary views of the nature of science (NOS) and scientific inquiry. Within the framework of situated cognition, the assertion is that engagement in inquiry activities similar to those of scientists provides a learning context conducive to developing knowledge about the methods and activities through which science progresses, and, in turn, to developing desired views of NOS. The inclusion of a scientific inquiry context to teach about NOS has intuitive appeal. Yet, whether the learners are students, teachers, or scientists, the empirical research does <I >not</I > generally support the claim that engaging in scientific inquiry alone enhances conceptions of NOS. We studied developments in NOS conceptions during a science research internship course for preservice secondary science teachers. In addition to the research component, the course included seminars and journal assignments. Interns' NOS views were assessed in a pre/post format using the Views of Nature of Science questionnaire, [VNOS-C] and interviews. Results indicate most interns showed substantial developments in NOS knowledge. Three factors were identified as important for NOS developments during the internship: (1) reflection, (2) context, and (3) perspective. Reflective journal writing and seminars had the greatest impact on NOS views. The science research component provided a context for reflection. The interns' role perspective appeared to impact their abilities to effectively reflect. Interns who assumed a reflective stance were more successful in deepening their NOS conceptions. Those who maintained a scientist's identity were less successful in advancing their NOS views through reflection. In light of these results, we discuss the significance and challenges to teaching about NOS within inquiry contexts. \\\\&copy; 2004 Wiley Periodicals, Inc. <I >Sci Ed</I > <B >88:</B >610-645, 2004\",\n",
       " \"Accurate multiplex gene synthesis from programmable DNA microchips. Testing the many hypotheses from genomics and systems biology experiments demands accurate and cost-effective gene and genome synthesis. Here we describe a microchip-based technology for multiplex gene synthesis. Pools of thousands of 'construction' oligonucleotides and tagged complementary 'selection' oligonucleotides are synthesized on photo-programmable microfluidic chips1, released, amplified and selected by hybridization to reduce synthesis errors ninefold. A one-step polymerase assembly multiplexing reaction assembles these into multiple genes. This technology enabled us to synthesize all 21 genes that encode the proteins of the Escherichia coli 30S ribosomal subunit, and to optimize their translation efficiency in vitro through alteration of codon bias. This is a significant step towards the synthesis of ribosomes in vitro and should have utility for synthetic biology in general.\",\n",
       " \"Representing monads. We show that any monad whose unit and extension operations are expressible as purely functional terms can be embedded in a call-by-value language with composable continuations. As part of the development, we extend Meyer and Wand's characterization of the relationship between continuation-passing and direct style to one for continuation-passing vs. general monadic  style. We further show that the composablecontinuations construct can itself be represented using ordinary, non-composable first-class continuations and a single piece of state. Thus, in the presence of two specific computational effects-- storage and escapes--any expressible monadic structure (e.g., nondeterminism as represented by the list monad) can be added as a purely definitional extension, without requiring a reinterpretation of the whole language. The paper includes an implementation of the construction (in Standard ML with some New Jersey extensions) and several examples.\",\n",
       " 'An Introduction to Dependent Type Theory. Functional programming languages often feature mechanisms that involve complex computations at the level of types. These mechanisms can be analyzed uniformly in the framework of dependent types, in which types may depend on values. The purpose of this chapter is to give some background for such an analysis. We present here precise theorems, that should hopefully help the reader to understand to which extent statements like “introducing dependent types in a programming language implies that type checking is undecidable”, are justified.',\n",
       " 'Faking it---simulating dependent types in Haskell. Dependent types reflect the fact that validity of data is often a relative notion by allowing prior data to affect the types of subsequent data. Not only does this make for a precise type system, but also a highly generic one: both the type and the program for each instance of a family of operations can be computed from the data which codes for that instance.  Recent experimental extensions to the Haskell type class mechanism give us strong tools to relativize types to other types. We may...',\n",
       " 'Domain specific embedded compilers. Domain-specific embedded languages (DSELs) expressed in higher-order, typed (HOT) languages provide a composable framework for domain-specific abstractions. Such a framework is of greater utility than a collection of stand-alone domain-specific languages. Usually, embedded domain specific languages are build on top of a set of domain specific primitive functions that are ultimately implemented using some form of foreign function call. We sketch a general design pattern for embedding client-server style services into Haskell using a domain specific embedded compiler for the server&#039;s source language. In particular we apply this idea to implement Haskell/DB, a domain specific embdedded compiler that dynamically generates of SQL queries from monad comprehensions, which are then executed on an arbitrary ODBC database server.  1 Introduction  Databases are ubiquitous in computer science. For instance, a web site is usually a fancy facade in front of a conventional database, which makes the i...',\n",
       " 'Dependent Types in Practical Programming. Programming is a notoriously error-prone process, and a great deal of evidence in practice has demonstrated that the use of a type system in a programming language can effectively detect program errors at compile-time. Moreover, some recent studies have indicated that the use of types can lead to significant enhancement of program performance at run-time. For the sake of practicality of type-checking, most type systems developed for general purpose programming languages tend to be simple and coarse, and this leaves ample room for improvement. As an advocate of types, this thesis addresses the issue of designing a type system for practical programming in which a notion of dependent types is available, leading to more accurate capture of program invariants with types. In contrast to developing a type theory with dependent types and then designing upon it a functional programming language, we study practical methods for extending the type systems of existing programming languages with dependent types. We present an approach to enriching the type system of ML with a special form of dependent',\n",
       " 'The Impact of the Lambda Calculus on Logic and Computer Science. . One of the most important contributions of A. Church to logic is his invention  of the lambda calculus. We present the genesis of this theory and its two major areas of  application: the representation of computations and the resulting functional programming  languages on the one hand and the representation of reasoning and the resulting systems of  computer mathematics on the other hand.  Acknowledgment. The following persons provided help in various ways. Erik Barendsen, Jon Barwise, Johan...',\n",
       " 'Dynamic Typing in a Statically Typed Language. Statically typed programming languages allow earlier error checking, better enforcement of disciplined programming styles, and generation of more efficient object code than languages where all type consistency checks are performed at run time. However, even in statically typed languages, there is often the need to deal with data whose type cannot be determined at compile time. To handle such situations safely, we propose to add a type Dynamic whose values are pairs of a value v and a type tag T ...',\n",
       " 'Type Classes with Functional Dependencies. Type classes in Haskell allow programmers to define functions that can be used on a set of different types, with a potentially different implementation in each case. For example, type classes are used to support equality and numeric types, and for monadic programming. A commonly requested extension to support â\\x80\\x98multiple parametersâ\\x80\\x99 allows a more general interpretation of classes as relations on types, and has many potentially useful applications. Unfortunately, many of these examples do not work well in practice, leading to ambiguities and inaccuracies in inferred types and delaying the detection of type errors.  This paper illustrates the kind of problems that can occur with multiple parameter type classes, and explains how they can be resolved by allowing programmers to specify explicit dependencies between the parameters. A particular novelty of this paper is the application of ideas from the theory of relational databases to the design of type systems.',\n",
       " 'Lazy functional state threads. Some algorithms make critical internal use of updatable state, even though their external specification is purely functional. Based on earlier work on monads, we present a way of securely encapsulating such stateful computations, in the context of a non-strict, purely-functional language. There are two main new developments in this paper. First, we show how to use the type system to securely encapsulate stateful computations, including ones which manipulate multiple, named, mutable objects. Second, we give a formal semantics for our system. This paper has been submitted to Programming Languages Design and Implementation (PLDI) &#039;94. 1',\n",
       " \"What are principal typings and what are they good for?. We demonstrate the pragmatic value of the principal typing property, a property distinct from ML's principal type property, by studying a type system with principal typings. The type system is based on rank 2 intersection types and is closely related to ML. Its principal typing property provides elegant support for separate compilation, including smartest recompilation and incremental type inference, and for accurate type error messages. Moreover, it motivates a new rule for typing recursive...\",\n",
       " 'On Understanding Types, Data Abstraction, and Polymorphism. Our objective is to understand the notion of  type  in programming languages, present a model of typed, polymorphic programming languages that reflects recent research in type theory, and examine the relevance of recent research to the design of practical programming languages.   Object-oriented languages provide both a framework and a motivation for exploring the interaction among the concepts of type, data abstraction, and polymorphism, since they extend the notion of type to data abstraction and since type inheritance is an important form of polymorphism. We develop a &lgr;-calculus-based model for type systems that allows us to explore these interactions in a simple setting, unencumbered by complexities of production programming languages.   The evolution of languages from untyped universes to monomorphic and then polymorphic type systems is reviewed. Mechanisms for polymorphism such as overloading, coercion, subtyping, and parameterization are examined. A unifying framework for polymorphic type systems is developed in terms of the typed &lgr;-calculus augmented to include binding of types by quantification as well as binding of values by abstraction.   The typed &lgr;-calculus is augmented by universal quantification to model generic functions with type parameters, existential quantification and packaging (information hiding) to model abstract data types, and bounded quantification to model subtypes and type inheritance. In this way we obtain a simple and precise characterization of a powerful type system that includes abstract data types, parametric polymorphism, and multiple inheritance in a single consistent framework. The mechanisms for type checking for the augmented &lgr;-calculus are discussed.   The augmented typed &lgr;-calculus is used as a programming language for a variety of illustrative examples. We christen this language Fun because fun instead of &lgr; is the functional abstraction keyword and because it is pleasant to deal with.   Fun is mathematically simple and can serve as a basis for the design and implementation of real programming languages with type facilities that are more powerful and expressive than those of existing programming languages. In particular, it provides a basis for the design of strongly typed object-oriented languages.',\n",
       " 'Inheritance is not subtyping. In typed object-oriented languages the subtype relation is typically based on the inheritance hierarchy. This approach, however, leads either to insecure type-systems or to restrictions on inheritance that make it less flexible than untyped Smalltalk inheritance. We present a new typed model of inheritance that allows more of the flexibility of Smalltalk inheritance within a statically-typed system. Significant features of our analysis are the introduction of polymorphism into the typing of inheritance and the uniform application of inheritance to objects, classes and types. The resulting notion of  type inheritance  allows us to show that the type of an inherited object is an inherited type but not always a subtype.',\n",
       " 'The price dynamics of common trading strategies. A deterministic trading strategy can be regarded as as signal processing element that uses external information and past prices as inputs and incorporates them into future prices. This paper uses a market maker based method of price formation to study the price dynamics induced by several commonly used financial trading strategies, showing how they amplify noise, induce structure in prices, and cause phenomena such as excess and clustered volatility.',\n",
       " 'WormBase: a comprehensive data resource for Caenorhabditis biology and genomics.. WormBase (http://www.wormbase.org), the model organism database for information about Caenorhabditis elegans and related nematodes, continues to expand in breadth and depth. Over the past year, WormBase has added multiple large-scale datasets including SAGE, interactome, 3D protein structure datasets and NCBI KOGs. To accommodate this growth, the International WormBase Consortium has improved the user interface by adding new features to aid in navigation, visualization of large-scale datasets, advanced searching and data mining. Internally, we have restructured the database models to rationalize the representation of genes and to prepare the system to accept the genome sequences of three additional Caenorhabditis species over the coming year. 10.1093/nar/gki066',\n",
       " 'A repository of our own: the E-LIS e-prints archive. \\t\\t\\tThis article reviews the E-Prints in Library and Information Science (E-LIS) open access archive. E-LIS is part of the Research in Computing, Library and Information Science (RCLIS) project, an international effort to organize and disseminate scholarly papers in librarianship and related fields. E-LIS uses open source applications, and joins a growing number of OAI-compliant services dedicated to providing free access to scholarly information.',\n",
       " 'Using terminological feedback for web search refinement: a log-based study. Although interactive query reformulation has been actively studied in the laboratory, little is known about the actual behavior of web searchers who are offered terminological feedback along with their search results. We analyze log sessions for two groups of users interacting with variants of the AltaVista search engine - a baseline group given no terminological feedback and a feedback group to whom twelve refinement terms are offered along with the search results. We examine uptake, refinement effectiveness, conditions of use, and refinement type preferences. Although our measure of overall session \"success\" shows no difference between outcomes for the two groups, we find evidence that a subset of those users presented with terminological feedback do make effective use of it on a continuing basis.',\n",
       " 'A User-Centred Approach to Functions in Excel. We describe extensions to the Excel spreadsheet that integrate userdefined functions into the spreadsheet grid, rather than treating them as a “bolt-on”. Our first objective was to bring the benefits of additional programming language features to a system that is often not recognised as a programming language. Second, in a project involving the evolution of a well-established language, compatibility with previous versions is a major issue, and maintaining this compatibility was our second objective. Third and most important, the commercial success of spreadsheets is largely due to the fact that many people find them more usable than programming languages for programming-like tasks. Thus, our third objective (with resulting constraints) was to maintain this usability advantage. Simply making Excel more like a conventional programming language would not meet these objectives and constraints. We have therefore taken an approach to our design work that emphasises the cognitive requirements of the user as a primary design criterion. The analytic approach that we demonstrate in this project is based on recent developments in the study of programming usability, including the Cognitive Dimensions of Notations and the Attention Investment model of abstraction use. We believe that this approach is also applicable to the design and extension of other programming languages and environments.',\n",
       " 'Type Classes in Haskell. This article defines a set of type inference rules for resolving overloading introduced by type classes, as used in the functional programming language Haskell. Programs including type classes are transformed into ones which may be typed by standard Hindley-Milner inference rules. In contrast to other work on type classes, the rules presented here relate directly to Haskell programs. An innovative aspect of this work is the use of second-order lambda calculus to record type information in the transformed program.',\n",
       " 'How to make ad-hoc polymorphism less ad hoc. This paper presents  type classes , a new approach to  ad-hoc  polymorphism. Type classes permit overloading of arithmetic operators such as multiplication, and generalise the &ldquo;eqtype variables&rdquo; of Standard ML. Type classes extend the Hindley/Milner polymorphic type system, and provide a new approach to issues that arise in object-oriented programming, bounded type quantification, and abstract data types. This paper provides an informal introduction to type classes, and defines them formally by means of type inference rules.',\n",
       " 'Representing Control: A study of the CPS transformation. This paper investigates the transformation of v-terms into continuation-passing style (CPS). We show that by appropriate j-expansion of Fischer and Plotkin&#039;s two-pass equational specification of the CPS transform, we can obtain a static and context-free separation of the result terms into &amp;quot;essential &amp;quot; and &amp;quot;administrative &amp;quot; constructs. Interpreting the former as syntax builders and the latter as directly executable code, we obtain a simple and efficient one-pass transformation algorithm, easily extended to conditional expressions, recursive definitions, and similar constructs. This new transformation algorithm leads to a simpler proof of Plotkin&#039;s simulation and indifference results. We go on to show how CPS-based control operators similar to, but more general than, Scheme&#039;s call/cc can be naturally accommodated by the new transformation algorithm. To demonstrate the expressive power of these operators, we use them to present an equivalent but even more concise formulation of the efficient CPS transformation algorithm. Finally, we relate the fundamental ideas underlying this derivation to similar concepts from other works on program manipulation; we derive a one-pass CPS transformation of n-terms; and we outline some promising areas for future research.',\n",
       " 'Views: a way for pattern matching to cohabit with data abstraction. Pattern matching and data abstraction are important concepts in de-signing programs, but they do not fit welI together. Pattern matching depends OYI making public a free data type representation, while data abstraction depends on hiding the representation. This paper proposes the viecua mechanism as a means of rsonciling thii conflict. A view allows any type to be viewed as a free data type, thus combining the clarity of pattern matching with the efficiency of data abstraction. 1',\n",
       " 'Stochastic lambda calculus and monads of probability distributions. Probability distributions are useful for expressing the meanings of probabilistic languages, which support formal modeling of and reasoning about uncertainty. Probability distributions form a monad, and the monadic definition leads to a simple, natural semantics for a stochastic lambda calculus, as well as simple, clean implementations of common queries. But the monadic implementation of the expectation query can be much less efficient than current best practices in probabilistic modeling. We therefore present a language of measure terms, which can not only denote discrete probability distributions but can also support the best known modeling techniques. We give a translation of stochastic lambda calculus into measure terms. Whether one translates into the probability monad or into measure terms, the results of the translations denote the same probability distribution.',\n",
       " 'The Alternative Splicing Gallery (ASG): bridging the gap between genome and transcriptome. Alternative splicing essentially increases the diversity of the transcriptome and has important implications for physiology, development and the genesis of diseases. Conventionally, alternative splicing is investigated in a case-by-case fashion, but this becomes cumbersome and error prone if genes show a huge abundance of different splice variants. We use a different approach and integrate all transcripts derived from a gene into a single splicing graph. Each transcript corresponds to a path in the graph, and alternative splicing is displayed by bifurcations. This representation preserves the relationships between different splicing variants and allows us to investigate systematically all possible putative transcripts. We built a database of splicing graphs for human genes, using transcript information from various major sources (Ensembl, RefSeq, STACK, TIGR and UniGene). A Web interface allows users to display the splicing graphs, to interactively assemble transcripts and to access their sequences as well as neighboring genomic regions. We also provide for each gene an exhaustive pre-computed catalog of putative transcripts--in total more than 1.2 million sequences. We found that [~]65% of the investigated genes show evidence for alternative splicing, and in 5% of the cases, a single gene might produce over 100 transcripts. 10.1093/nar/gkh731',\n",
       " 'Splicing graphs and EST assembly problem. 10.1093/bioinformatics/18.suppl_1.S181 Motivation: The traditional approach to annotate alternative splicing is to investigate every splicing variant of the gene in a case-by-case fashion. This approach, while useful, has some serious shortcomings. Recent studies indicate that alternative splicing is more frequent than previously thought and some genes may produce tens of thousands of different transcripts. A list of alternatively spliced variants for such genes would be difficult to build and hard to analyse. Moreover, such a list does not show the relationships between different transcripts and does not show the overall structure of all transcripts. A better approach would be to represent all splicing variants for a given gene in a way that captures the relationships between different splicing variants.Results: We introduce the notion of the splicing graph that is a natural and convenient representation of all splicing variants. The key difference with the existing approaches is that we abandon the linear (sequence) representation of each transcript and replace it with a graph representation where each transcript corresponds to a path in the graph. We further design an algorithm to assemble EST reads into the splicing graph rather than assembling them into each splicing variant in a case-by-case fashion.Availability: http://www-cse.ucsd.edu/groups/bioinformatics/software.htmlContact: sheber@ucsd.eduKeywords: EST assembly; splicing graph; alternative splicing',\n",
       " \"OAI-P2P: A Peer-to-Peer Network for Open Archives. OAI is designed with a low-barrier technology approach, thus allowing institutions to provide content metadata with little effort. On the other hand, search capabilities are very limited on OAI data providers, and have to be provided by separate service providers. We propose that data providers form a peer-to-peer network which supports distributed search over all connected metadata repositories. Such an approach is already implemented for learning content metadata (project 'Edutella'). We...\",\n",
       " 'Decoherence, einselection, and the quantum origins of the classical. The manner in which states of some quantum systems become effectively classical is of great significance for the foundations of quantum physics, as well as for problems of practical interest such as quantum engineering. In the past two decades it has become increasingly clear that many (perhaps all) of the symptoms of classicality can be induced in quantum systems by their environments. Thus decoherence is caused by the interaction in which the environment in effect monitors certain observables of the system, destroying coherence between the pointer states corresponding to their eigenvalues. This leads to environment-induced superselection or einselection, a quantum process associated with selective loss of information. Einselected pointer states are stable. They can retain correlations with the rest of the universe in spite of the environment. Einselection enforces classicality by imposing an effective ban on the vast majority of the Hilbert space, eliminating especially the flagrantly nonlocal {“Schrödinger-cat} states.” The classical structure of phase space emerges from the quantum Hilbert space in the appropriate macroscopic limit. Combination of einselection with dynamics leads to the idealizations of a point and of a classical trajectory. In measurements, einselection replaces quantum entanglement between the apparatus and the measured system with the classical correlation. Only the preferred pointer observable of the apparatus can store information that has predictive power. When the measured quantum system is microscopic and isolated, this restriction on the predictive utility of its correlations with the macroscopic apparatus results in the effective “collapse of the wave packet.” The existential interpretation implied by einselection regards observers as open quantum systems, distinguished only by their ability to acquire, store, and process information. Spreading of the correlations with the effectively classical pointer states throughout the environment allows one to understand “classical reality” as a property based on the relatively objective existence of the einselected states. Effectively classical pointer states can be “found out” without being re-prepared, e.g, by intercepting the information already present in the environment. The redundancy of the records of pointer states in the environment (which can be thought of as their “fitness” in the Darwinian sense) is a measure of their classicality. A new symmetry appears in this setting. Environment-assisted invariance or envariance sheds new light on the nature of ignorance of the state of the system due to quantum correlations with the environment and leads to Born’s rules and to reduced density matrices, ultimately justifying basic principles of the program of decoherence and einselection.',\n",
       " 'LANDMARC: Indoor Location Sensing Using Active RFID. Growing convergence among mobile computing devices and embedded technology sparks the development and deployment of “context-aware” applications, where location is the most essential context. In this paper we present LANDMARC, a location sensing prototype system that uses Radio Frequency Identification (RFID) technology for locating objects inside buildings. The major advantage of LANDMARC is that it improves the overall accuracy of locating objects by utilizing the concept of reference tags. Based on experimental analysis, we demonstrate that active RFID is a viable and cost-effective candidate for indoor location sensing. Although RFID is not designed for indoor location sensing, we point out three major features that should be added to make RFID technologies competitive in this new and growing market.',\n",
       " 'Wireless broadband drivers and their social implications. Wireless local area networks now offer high-speed Internet access at numerous locations in both public and private environments. Associated with this rapid growth, numerous social implications come to the fore, especially relating to practices, such as the free use and shar- ing of bandwidth. Using case-based comparative analysis, we examine three primary strate- gies involved in providing wireless broadband access. Based on this research, we discuss the future of Wi-Fi growth, emergent competing technologies, and the broad social implications of this phenomenon.',\n",
       " 'A tutorial on support vector regression. In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for regression and function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization and capacity control from a SV point of view.',\n",
       " 'Multimodel Inference. The model selection literature has been generally poor at reflecting the deep foundations of the Akaike information criterion (AIC) and at making appropriate comparisons to the Bayesian information criterion (BIC). There is a clear philosophy, a sound criterion based in information theory, and a rigorous statistical foundation for AIC. AIC can be justified as Bayesian using a â\\x80\\x9csavvyâ\\x80\\x9d prior on models that is a function of sample size and the number of model parameters. Furthermore, BIC can be derived as a non-Bayesian result. Therefore, arguments about using AIC versus BIC for model selection cannot be from a Bayes versus frequentist perspective. The philosophical context of what is assumed about reality, approximating models, and the intent of model-based inference should determine whether AIC or BIC is used. Various facets of such multimodel inference are presented here, particularly methods of model averaging.',\n",
       " 'Greed and grievance in civil war. We investigate the causes of civil war, using a new data set of wars during 1960-99. Rebellion may be explained by atypically severe grievances, such as high inequality, a lack of political rights, or ethnic and religious divisions in society. Alternatively, it might be explained by atypical opportunities for building a rebel organization. While it is difficult to find proxies for grievances and opportunities, we find that political and social variables that are most obviously related to grievances have little explanatory power. By contrast, economic variables, which could proxy some grievances but are perhaps more obviously related to the viability of rebellion, provide considerably more explanatory power. 10.1093/oep/gpf064',\n",
       " 'The future of eLearning. In this article, the author reflects on the potential benefits of e-learning. E-learning is not a single strand but is multifaceted, covering a wide range of approaches and methods. One major but general benefit is that learning with technology can be motivating. E-learning has the potential to motivate, develop confidence and self-esteem, overcome many barriers that learners encounter, personalise the learning experience, widen access and improve the learning experience, while also helping people to develop their ICT skills. These are significant advantages that need to be made available to all learners.',\n",
       " 'Multimodal Video Indexing: A Review of the State-of-the-art. Efficient and effective handling of video documents depends on the availability of indexes. Manual indexing is unfeasible for large video collections. In this paper we survey several methods aiming at automating this time and resource consuming process. Good reviews on single modality based video indexing have appeared in literature. Effective indexing, however, requires a multimodal approach in which either the most appropriate modality is selected or the different modalities are used in collaborative fashion. Therefore, instead of separately treating the different information sources involved, and their specific algorithms, we focus on the similarities and differences between the modalities. To that end we put forward a unifying and multimodal framework, which views a video document from the perspective of its author. This framework forms the guiding principle for identifying index types, for which automatic methods are found in literature. It furthermore forms the basis for categorizing these different methods.',\n",
       " \"Active Sampling for Class Probability Estimation and Ranking. In many cost-sensitive environments class probability estimates are used by decision makers to evaluate the expected utility from a set of alternatives. Supervised learning can be used to build class probability estimates; however, it often is very costly to obtain training data with class labels. Active learning acquires data incrementally, at each phase identifying especially useful additional data for labeling, and can be used to economize on examples needed for learning. We outline the critical features of an active learner and present a sampling-based active learning method for estimating class probabilities and class-based rankings. BOOTSTRAP-LV identifies particularly informative new data for learning based on the variance in probability estimates, and uses weighted sampling to account for a potential example's informative value for the rest of the input space. We show empirically that the method reduces the number of data items that must be obtained and labeled, across a wide variety of domains. We investigate the contribution of the components of the algorithm and show that each provides valuable information to help identify informative examples. We also compare BOOTSTRAP-LV with UNCERTAINTY SAMPLING, an existing active learning method designed to maximize classification accuracy. The results show that BOOTSTRAP-LV uses fewer examples to exhibit a certain estimation accuracy and provide insights to the behavior of the algorithms. Finally, we experiment with another new active sampling algorithm drawing from both UNCERTAINTY SAMPLING and BOOTSTRAP-LV and show that it is significantly more competitive with BOOTSTRAP-LV compared to UNCERTAINTY SAMPLING. The analysis suggests more general implications for improving existing active sampling algorithms for classification.\",\n",
       " \"Learning to Decode Cognitive States from Brain Images. Over the past decade, functional Magnetic Resonance Imaging (fMRI) has emerged as a powerful new instrument to collect vast quantities of data about activity in the human brain. A typical fMRI experiment can produce a three-dimensional image related to the human subject's brain activity every half second, at a spatial resolution of a few millimeters. As in other modern empirical sciences, this new instrumentation has led to a flood of new data, and a corresponding need for new data analysis methods. We describe recent research applying machine learning methods to the problem of classifying the cognitive state of a human subject based on fRMI data observed over a single time interval. In particular, we present case studies in which we have successfully trained classifiers to distinguish cognitive states such as (1) whether the human subject is looking at a picture or a sentence, (2) whether the subject is reading an ambiguous or non-ambiguous sentence, (3) whether the word the subject is viewing is a noun or a verb, and (4) whether the noun the subject is viewing is a word describing food, people, buildings, etc. This learning problem provides an interesting case study of classifier learning from extremely high dimensional ($10^5$ features), extremely sparse (tens of training examples), noisy data. This paper summarizes the results obtained in these four case studies, as well as lessons learned about how to successfully apply machine learning methods to train classifiers in such settings.\",\n",
       " 'As We May Think. As Director of the Office of Scientific Research and Development, Dr. Vannevar Bush has coordinated the activities of some six thousand leading American scientists in the application of science to warfare. In this significant article he holds up an incentive for scientists when the fighting has ceased. He urges that men of science should then turn to the massive task of making more accessible our bewildering store of knowledge. For many years inventions have extended man’s physical powers rather than the powers of his mind. Trip hammers that multiply the fists, microscopes that sharpen the eye, and engines of destruction and detection are new results, but not the end results, of modern science. Now, says Dr. Bush, instruments are at hand which, if properly developed, will give man access to and command over the inherited knowledge of the ages. The perfection of these pacific instruments should be the first objective of our scientists as they emerge from their war work. Like Emerson’s famous address of 1837 on “The American Scholar,’’ this paper by Dr. Bush calls for a new relationship between thinking man and the sum of our knowledge. —The [Atlantic Monthly] Editor, July 1945',\n",
       " 'Not just Piaget; not just Vygotsky, and certainly not Vygotsky as alternative to Piaget. There have been many interpretations published on the relative importance of the work of both Vygotsky and Piaget: often to the detriment of the latter. This article represents an attempt to discover the meaning and intention of the former by going back to the specifics of what he said and wrote. By reference to what they said of each other it is argued that by the early 30s they had reached almost identical positions regarding child development, and that the work of each is complementary to that of the other. The implications of this position for a theory of intervention for cognitive acceleration are then discussed.',\n",
       " \"Scholarly communication in the digital environment: what do authors want?. This article reports on a large-scale international survey of authors' perception and experience of the journals system conducted by ciber in association with National Opinion Polls {(NOP).} It explores the factors that inform authors' decisions where to publish and, in particular, which groups of readers they perceive to be most important. It probes readership behaviour and the values that underlie authors' attitudes towards copyright and emerging business models, notably open access. It is concluded that many aspects of author behaviour are highly conservative and that a significant shift towards open access is, in the short to medium term, highly unlikely.\",\n",
       " 'A Model-Based Background Adjustment for Oligonucleotide Expression Arrays. High-density oligonucleotide expression arrays are widely used in many areas of biomedical research. Affymetrix GeneChip arrays are the most popular. In the Affymetrix system, a fair amount of further preprocessing and data reduction occurs after the image-processing step. Statistical procedures developed by academic groups have been successful in improving the default algorithms provided by the Affymetrix system. In this article we present a solution to one of the preprocessing steps—background adjustment—based on a formal statistical framework. Our solution greatly improves the performance of the technology in various practical applications. These arrays use short oligonucleotides to probe for genes in an RNA sample. Typically, each gene is represented by 11—20 pairs of oligonucleotide probes. The first component of these pairs is referred to as a perfect match probe and is designed to hybridize only with transcripts from the intended gene (i.e., specific hybridization). However, hybridization by other sequences (i.e., nonspecific hybridization) is unavoidable. Furthermore, hybridization strengths are measured by a scanner that introduces optical noise. Therefore, the observed intensities need to be adjusted to give accurate measurements of specific hybridization. We have found that the default ad hoc adjustment, provided as part of the Affymetrix system, can be improved through the use of estimators derived from a statistical model that uses probe sequence information. A final step in preprocessing is to summarize the probe-level data for each gene to define a measure of expression that represents the amount of the corresponding mRNA species. In this article we illustrate the practical consequences of not adjusting appropriately for the presence of nonspecific hybridization and provide a solution based on our background adjustment procedure. Software that computes our adjustment is available as part of the Bioconductor Project (http://www.bioconductor.org).',\n",
       " 'Ubiquitous computing. Emerging technologies for learning aims to help readers consider how emerging technologies may impact on education in the medium term. The publication is not intended to be a comprehensive review of educational technologies, but offers some highlights across the broad spectrum of developments and trends. It should open readers up to some of the possibilities that are developing and the potential for technology to transform our ways of working, learning and interacting over the next three to five years. This follow-up edition complements the original document published in March 2006. It offers new perspectives and challenges in the light of a rapidly changing technology landscape. However, Emerging technologies for learning is not intended to present a unified view of the future. It deliberately presents a broad range of opinions with the intention that they will stimulate debate and challenge current thinking. We have been able to expand the range of experts to offer their own particular takes on how technology developments may affect the future of education. You will find some echoes of articles in the earlier publication and some new directions. An overarching theme is that of knowledgeable users customising their tools, services, sources of information, methods of communication and networks of people to suit their personal needs. Distinctions between learning, socialising, working, playing and entertainment are beginning to blur, along with when, where and with whom these activities take place. A recent Demos study looked at the way many young people are using technology in every part of their lives. It examines some of the softer skills such as creativity, communication and collaboration they are developing through the use of technology. These skills will be increasingly important in a globally networked, knowledge economy. The current generation of young people will reinvent the workplace, and the society they live in. They will do it along the progressive lines that are built into the technology they use everyday – of networks, collaboration, co-production and participation. The change in behaviour has already happened. We have to get used to it, accept that the flow of knowledge moves both ways and do our best to make sure that no one is left behind. Their Space: Education for a digital generation (Green, H., Hannon, C., Demos, 2007)1 Some of the technologies and trends discussed in this publication are already beginning to have an impact; others are only just beginning to be explored, but show potential: The future is already here – it’s just unevenly distributed2 (William Gibson).',\n",
       " \"Speaking while monitoring addressees for understanding. (from the journal abstract) Speakers monitor their own speech and, when they discover problems, make repairs. In the proposal examined here, speakers also monitor addressees for understanding and, when necessary, alter their utterances in progress. Addressees cooperate by displaying and signaling their understanding in progress. Pairs of participants were videotaped as a director instructed a builder in assembling 10 Lego models. In one group, directors could see the builders' workspace; in a second, they could not; in a third, they gave instructions by audiotape. Two partners were much slower when directors could not see the builders' workspace, and they made many more errors when the instructions were audiotaped. When their workspace was visible, builders communicated with directors by exhibiting, poising, pointing at, placing, and orienting blocks, and by eye gaze, head nods, and head shakes, all timed with precision. Directors often responded by altering their utterances midcourse, also timed with precision. (PsycINFO Database Record (c) 2004 APA, all rights reserved) speaking; monitoring addressees; understanding\",\n",
       " 'The Network Paradigm in Organizational Research: A Review and Typology. In this paper, we review and analyze the emerging network paradigm in organizational research. We begin with a conventional review of recent research organized around recognized research streams. Next, we analyze this research, developing a set of dimensions along which network studies vary, including direction of causality, levels of analysis, explanatory goals, and explanatory mechanisms. We use the latter two dimensions to construct a 2-by-2 table cross-classifying studies of network consequences into four canonical types: structural social capital, social access to resources, contagion, and environmental shaping. We note the rise in popularity of studies with a greater sense of agency than was traditional in network research. 10.1016/S0149-2063_03_00087-4',\n",
       " 'Beyond Independent Components: Trees and Clusters. We present a generalization of independent component analysis (ICA), where instead of looking  for a linear transform that makes the data components independent, we look for a transform that  makes the data components well fit by a tree-structured graphical model. This tree-dependent  component analysis (TCA) provides a tractable and flexible approach to weakening the assumption  of independence in ICA. In particular, TCA allows the underlying graph to have multiple connected  components, and thus the method is able to find &#034;clusters&#034; of components such that components  are dependent within a cluster and independent between clusters. Finally, we make use of a notion  of graphical models for time series due to Brillinger (1996) to extend these ideas to the temporal  setting. In particular, we are able to fit models that incorporate tree-structured dependencies among  multiple time series.',\n",
       " 'Transaction cost determinants and ownership-based entry mode choice: a meta-analytical review. Entry mode choice is a critical ingredient of international entry strategies, and has been voluminously examined in the field. The findings, however, are very mixed, especially with respect to transaction-cost-related factors in determining the ownership-based entry mode choice. This study conducted a meta-analysis to quantitatively summarize the literature and empirically generalize more conclusive findings. Based on the 106 effect sizes of 38 empirical studies, the meta-analysis shows that the findings of the existing studies are moderated to varying degrees by both study-setting factors and statistical artifacts, although the combined overall effects of transaction cost-based determinants are consistent with the predictions of transaction cost economics. We extensively discuss the implications of meta-analytical results, especially moderating effects of location, country of origin, industry type, and statistical artifacts, highlight the measurement adequacy, equivalence, and multidimensionality of transaction cost determinants, and present our suggestions to improve theoretical inquiries and empirical verifications on entry mode choice.',\n",
       " \"Exploratory Data Analysis for Complex Models. ''Exploratory'' and ''confirmatory'' data analysis can both be viewed as methods for comparing observed data to what would be obtained under an implicit or explicit statistical model. For example, many of Tukey’s methods can be interpreted as checks against hypothetical linear models and Poisson distributions. In more complex situations, Bayesian methods can be useful for constructing reference distributions for various plots that are useful in exploratory data analysis. This article proposes an approach to unify exploratory data analysis with more formal statistical methods based on probability models. These ideas are developed in the context of examples from fields including psychology, medicine, and social science.\",\n",
       " 'Default-Mode Activity during a Passive Sensory Task: Uncoupled from Deactivation but Impacting Activation. Deactivation refers to increased neural activity during low-demand tasks or rest compared with high-demand tasks. Several groups have reported that a particular set of brain regions, including the posterior cingulate cortex and the medial prefrontal cortex, among others, is consistently deactivated. Taken together, these typically deactivated brain regions appear to constitute a default-mode network of brain activity that predominates in the absence of a demanding external task. Examining a passive, block-design sensory task with a standard deactivation analysis (rest epochs vs. stimulus epochs), we demonstrate that the default-mode network is undetectable in one run and only partially detectable in a second run. Using independent component analysis, however, we were able to detect the full default-mode network in both runs and to demonstrate that, in the majority of subjects, it persisted across both rest and stimulus epochs, uncoupled from the task waveform, and so mostly undetectable as deactivation. We also replicate an earlier finding that the default-mode network includes the hippocampus suggesting that episodic memory is incorporated in default-mode cognitive processing. Furthermore, we show that the more a subject\\'s default-mode activity was correlated with the rest epochs (and \"deactivated\" during stimulus epochs), the greater that subject\\'s activation to the visual and auditory stimuli. We conclude that activity in the default-mode network may persist through both experimental and rest epochs if the experiment is not sufficiently challenging. Time-series analysis of default-mode activity provides a measure of the degree to which a task engages a subject and whether it is sufficient to interrupt the processes--presumably cognitive, internally generated, and involving episodic memory--mediated by the default-mode network.',\n",
       " 'Mathematical modal logic: A view of its evolution. This is a survey of the origins of mathematical interpretations of modal logics, and their development over the last century or so. It focuses on the interconnections between  algebraic  semantics using Boolean algebras with operators and  relational  semantics using structures often called  Kripke models . It reviews the ideas of a number of people who independently contributed to the emergence of relational semantics, and compares them with the work of Kripke. It concludes with an account of several applications of modal model theory to mathematics and theoretical computer science.',\n",
       " 'The relationships of communicator style, personality-based learning style, and classroom community among online graduate students. This study examined the relationships among communicator style, personality-based learning style, and sense of classroom community among 72 graduate students enrolled in online doctoral coursework. Findings suggested that communicator style patterns were related to learning styles and to classroom community. Moreover, the results of a canonical correlation suggested that friendly and open communicator styles were significantly related to feelings of being connected and the precise communicator style was related to both feelings of connectedness and to feelings that membership in the online learning community fostered educational goal attainment. No significant relationships were found between learning styles and classroom community.',\n",
       " 'Copyrights and Copywrongs: The Rise of Intellectual Property and How it Threatens Creativity. {\"Illuminating\"<br> &#151;<i>Bookforum</i> April-June 2002 <P> \"It has taken lawyers 200-plus years to morph copyright law from the balanced compromise that our framers struck to the extraordinary system of control that it has become. In this beautifully written book, a nonlawyer has uncovered much of the damage done. <I>Copyrights and Copywrongs</I> is a rich and compelling account of the bending of American copyright law, and a promise of the balance that we could once again make the law become.\"<br> &#151;Lawrence Lessig, Stanford Law School and author of <I>Code and Other Laws of Cyberspace</I> <P> \"Siva Vaidhyanathan has done a big favor for the academic and library communities. In this book, he has spelled out in clear, understandable language what\\'s at stake in the battles over the nation\\'s intellectual property. The issues brought forward are critical to the future of scholarship and creativity. Librarians and academics are wise to purchase this book and add it to their &#145;must read\\' lists.\"<br> &#151;Nancy Kranich, President, American Library Association, 2000&#150;2001 <P> \"<I>Copyrights and Copywrongs</I> is an urgent information-age wake-up call to a public cocooned in belief that &#145;copyright\\' is a seal and safeguard for consumers and producers of culture-ware. This book guides us into the legal labyrinth of a new world of so-called intellectual property, in which &#145;fair use\\' isn\\'t fair, where rights are waived and free speech&#151;when we can get it&#151;costs a great deal of money. From print books to video games, <I>Copyrights and Copywrongs</I> shows free expression in a legalistic chokehold. Clearly written, meticulously argued, this book is a must.\"<br> &#151;Cecelia Tichi, author of<I>Embodiment of a Nation: Human Form in American Spaces</I> <P> Copyright reflects far more than economic interests. Embedded within conflicts over royalties and infringement are cultural values&#151;about race, class, access, ownership, free speech, and democracy&#151;which influence how rights are determined and enforced. Questions of legitimacy&#151;of what constitutes \"intellectual property\" or \"fair use,\" and of how to locate a precise moment of cultural creation&#151;have become enormously complicated in recent years, as advances in technology have exponentially increased the speed of cultural reproduction and dissemination. <P> In <B>Copyrights and Copywrongs</B>, Siva Vaidhyanathan tracks the history of American copyright law through the 20th century, from Mark Twain\\'s vehement exhortations for \"thick\" copyright protection, to recent lawsuits regarding sampling in rap music and the \"digital moment,\" exemplified by the rise of Napster and MP3 technology. He argues persuasively that in its current punitive, highly restrictive form, American copyright law hinders cultural production, thereby contributing to the poverty of civic culture. <P> In addition to choking cultural expression, recent copyright law, Vaidhyanathan argues, effectively sanctions biases against cultural traditions which differ from the Anglo-European model. In African-based cultures, borrowing from and building upon earlier cultural expressions is not considered a legal trespass, but a tribute. Rap and hip hop artists who practice such \"borrowing\" by sampling and mixing, however, have been sued for copyright violation and forced to pay substantial monetary damages. Similarly, the oral transmission of culture, which has a centuries-old tradition within African American culture, is complicated by current copyright laws. How, for example, can ownership of music, lyrics, or stories which have been passed down through generations be determined? Upon close examination, strict legal guidelines prove insensitive to the diverse forms of cultural expression prevalent in the United States, and reveal much about the racialized cultural values which permeate our system of laws. Ultimately, copyright is a necessary policy that should balance public and private interests but the recent rise of \"intellectual property\" as a concept have overthrown that balance. Copyright, Vaidhyanathan asserts, is policy, not property. <P> Bringing to light the republican principles behind original copyright laws as well as present-day imbalances and future possibilities for freer expression and artistic equity, this volume takes important strides towards unraveling the complex web of culture, law, race, and technology in today\\'s global marketplace.}',\n",
       " \"Collaborating with writing tools - An instrumental perspective on the problem of computer-supported collaborative activities. This paper presents an analysis of the modifications that a synchronous computer support for collaborative writing introduces into the organization of co-authors' writing. The analysis is grounded in case studies of different groups of co-authors writing a report together face to face and at a distance through a collaborative writing computer system. Drawing from these studies I suggest that the problems with using a collaborative writing computer system to provide a fully collaborative writing environment derive from underlying assumptions concerning collaboration within the co-authoring activity. I point out that a more thorough understanding of how co-authors organize their writing can provide resources to envisage more radical solutions to the problem of computer support for collaboration. I conclude by considering ways that might be adequate to reconfigure collaborative writing systems in order to provide more satisfactory support for collaboration in writing environments.\",\n",
       " 'The Concept of Information Overload: A Review of Literature from Organization Science, Accounting, Marketing, MIS, and Related Disciplines. Based on literature from the domains of organization science, marketing, accounting, and management information systems, this review article examines the theoretical basis of the information overload discourse and presents an overview of the main definitions, situations, causes, effects, and countermeasures. It analyzes the contributions from the last 30 years to consolidate the existing research in a conceptual framework and to identify future research directions.',\n",
       " 'Higher Education Management and Policy: Volume 16 Issue 2 Complete Edition. The Journal of OECD&#39;s Programme on Institutional Management in Higher Education. This issue features article on fair access, assessment of personnel, a merger, private university policy initiatives, accessibility and equity, internationalisation, and enrollment management.  - A Faustian Bargain&#63; Institutional Responses to National and International Rankings -  - &#34;Standards Will Drop&#34; - and Other Fears about the Equality Agenda in Higher Education -',\n",
       " 'Disclosure of Genetic Information Obtained Through Research. The rapid expansion of information and knowledge of genetics has implications for the question of whether, and under what circumstances, information discovered in the course of genetic research should be conveyed to research participants and/or their relatives. The aim of this paper is to propose an ethically defensible solution to a specific case example illustrating this problem. To do this we reviewed the literature to find answers to the following three questions: (1) What do current regulations, guidelines, and commentary say about the disclosure of genetic risk information obtained through research to research participants? (2) What do current regulations, guidelines, and commentary say about the disclosure of genetic risk information obtained through research to the relatives of research subjects? and (3) What do current regulations, guidelines, and commentary say about the disclosure of genetic risk information obtained through research about former research participants who are now deceased? Our conclusion is that current U.S. federal guidelines governing the use of human subjects in research, as well as much of the current literature, do not adequately address the familial dimension inherent in genetic research, are virtually silent on the issue of sharing information of relevance to family members, and do not protect the deceased. It is our belief that this omission needs to be corrected and that explicit guidance on this issue needs to be provided to institutional review boards and researchers alike.',\n",
       " \"The tragedy of the digital commons. In the paper it is argued that bridging the digital divide may cause a new ethical and social dilemma. Using Hardin's Tragedy of the Commons, we show that an improper opening and enlargement of the digital environment (Infosphere) is likely to produce a Tragedy of the Digital Commons (TDC). In the course of the analysis, we explain why Adar and Huberman's previous use of Hardin's Tragedy to interpret certain recent phenomena in the Infosphere (especially peer-to-peer communication) may not be entirely satisfactory. We then seek to provide an improved version of the TDC that avoids the possible shortcomings of their model. Next, we analyse some problems encountered by the application of classical ethics in the resolution of the TDC. In the conclusion, we outline the kind of work that will be required to develop an ethical approach that may bridge the digital divide but avoid the TDC. [PUBLICATION ABSTRACT]\",\n",
       " \"Becoming an Online Teacher: Adapting to a Changed Environment for Teaching and Learning in Higher Education. Advancements in online technologies have facilitated a convergence of distance and campus-based learning and, thus, offer new opportunities for all students through better access to resources, increased interaction between staff and students and greater flexibility in place and time. However, the transition to online teaching and learning presents new challenges as the roles and expectations of both staff and students evolve. An online teacher must create a coherent learning experience for students with whom they may not meet face-to-face and, therefore, must develop new support strategies that maintain motivation and encourage interaction. Adapting student-centred approaches to the online environment has required the development of new skills and changes to teaching practices. This paper presents an analysis of the changed environment for teachers and learners in a post-graduate coursework programme based on constructivist principles that has moved from predominately on-campus delivery to online mode. The authors examine the impact of changes to teaching and learning over the past 5 years of the programme's development and reflect on the implications of these for becoming an online teacher. <b>Devenir un professeur en ligne : Adaptation à un environnement varié d'enseignement et de formation supérieure.</b> Les progrès de technologie en ligne ont facilité une convergence de formation par correspondance et de formation au campus et ceci offre de nouvelles possibilités pour tous les étudiants par un meilleur accès aux ressources, une interaction augmentée entre le corps enseignant et les étudiants et une plus grande flexibilité temporelle et cohérente. Cependant, la transition vers l'enseignement et la formation en ligne représente de nouveaux défis étant donné que les rôles et les expectatives du corps enseignant ainsi que des étudiants altèrent. Un professeur en ligne doit créer une expérience de formation cohérente pour des étudiants qu'il ne rencontre pas personnellement et par conséquent il doit développer une nouvelle stratégie de soutien qui maintient la motivation et encourage l'interaction. L'adaption des approches concentrées sur les étudiants à un environnement en ligne exige un développement de nouveaux talents et des changements de pratiques d'enseignement. Cet exposé présente une analyse de la convergence d'environnements de formation qui a tourné d'une assistance prédominante du côté du campus à un mode en ligne et de l'effet de cette convergence sur les professeurs et les étudiants d'un programme d'études basé sur des principes constructifs. Les auteurs examinent l'impact des changements d'enseignement et de formation pendant les cinq dernières années du développement de ce programme et considèrent ses répercussions sur l'éducation d'un professeur en ligne. <b>Ausbildung zur Online-Lehrkraft: Anpassung an ein verändertes Lern- und Lehrumfeld im Bereich der Hochschulausbildung.</b> Die Fortschritte in der Online-Technologie haben eine Annäherung zwischen Fernstudium und hochschulbasiertem Lernen ermöglicht, und dies eröffnet neue Möglichkeiten für alle Studenten durch einen besseren Zugang zu Ressourcen, einer verbesserten Interaktion zwischen Lehrerkollegium und Studenten und einer größeren räumlichen und zeitlichen Flexibilität. Der Übergang zum Online-Unterricht und \\x96Lernen stellt außerdem neue Herausforderungen dar, da sich die Rolle und die Erwartungen sowohl des Lehrerkollegiums als auch der Studenten verändern. Eine Online-Lehrkraft muss ein schlüssiges Lernerlebnis für Studenten schaffen, die sie gegebenenfalls nie persönlich kennen lernt. Aus diesem Grunde muss sie neue Betreuungsstrategien entwickeln, die die Motivation aufrechterhalten und die Interaktion fördern. Durch die Anpassung der studentischen Lernansätze an ein Online-Umfeld ist die Entwicklung neuer Fähigkeiten und eine Änderung der Lehrpraktiken erforderlich geworden. Dieser Bericht stellt eine Analyse des veränderten Lernumfelds für Lehrer und Studenten in einem auf konstruktivistischen Prinzipien basierten weiterführenden Studienprogramm vor, das von einer überwiegend hochschulbasierten Wissensvermittlung zu Online-Methoden übergeht. Die Autoren untersuchen den Einfluss der Lehr- und Lernveränderungen in den vergangenen fünf Jahren der Programmentwicklung und betrachten deren Auswirkungen auf die Ausbildung einer Online-Lehrkraft.\",\n",
       " \"Why environmental scientists are becoming Bayesians. Advances in computational statistics provide a general framework for the high-dimensional models typically needed for ecological inference and prediction. Hierarchical Bayes (HB) represents a modelling structure with capacity to exploit diverse sources of information, to accommodate influences that are unknown (or unknowable), and to draw inference on large numbers of latent variables and parameters that describe complex relationships. Here I summarize the structure of HB and provide examples for common spatiotemporal problems. The flexible framework means that parameters, variables and latent variables can represent broader classes of model elements than are treated in traditional models. Inference and prediction depend on two types of stochasticity, including (1) uncertainty, which describes our knowledge of fixed quantities, it applies to all 'unobservables' (latent variables and parameters), and it declines asymptotically with sample size, and (2) variability, which applies to fluctuations that are not explained by deterministic processes and does not decline asymptotically with sample size. Examples demonstrate how different sources of stochasticity impact inference and prediction and how allowance for stochastic influences can guide research.\",\n",
       " 'Scale and Translation Invariant Collaborative Filtering Systems. Collaborative filtering systems are prediction algorithms over sparse data sets of user preferences. We modify a wide range of state-of-the-art collaborative filtering systems to make them scale and translation invariant and generally improve their accuracy without increasing their computational cost. Using the EachMovie and the Jester data sets, we show that learning-free constant time scale and translation invariant schemes outperforms other learning-free constant time schemes by at least 3% and perform as well as expensive memory-based schemes (within 4%). Over the Jester data set, we show that a scale and translation invariant Eigentaste algorithm outperforms Eigentaste 2.0 by 20%. These results suggest that scale and translation invariance is a desirable property.',\n",
       " 'Tropical Forest Fragments Enhance Pollinator Activity in Nearby Coffee Crops. :&#8194; Crop pollination by wild bees is an ecosystem service of enormous value, but it is under increasing threat from agricultural intensification. As with many ecosystem services, the mechanisms, scales, and species through which crop pollination is provided are too poorly understood to inform land-use decisions. I investigated the role of tropical forest remnants as sources of pollinators to surrounding coffee crops in Costa Rica. In 2001 and 2002 I observed bee activity and pollen deposition rates at coffee flowers along distance gradients from two fragments and one narrow riparian strip of forest. Eleven eusocial species were the most common visitors: 10 species of native meliponines and the introduced honeybee, Apis mellifera (hereafter Apis). Bee richness, overall visitation rate, and pollen deposition rate were all significantly higher in sites within approximately 100 m of forest fragments than in sites farther away (maximum distance of 1.6 km). Apis visitation rates were constant across the distance gradient, however, and Apis accounted for &#62;90% of all floral visits in distant sites. The gradient from the riparian strip showed a similar drop in bee species richness with distance, but visitation rates were uniformly low along the gradient. Throughout the study area, Apis abundances declined sharply from 2001 to 2002, reducing visitation rates by over 50% in distant sites (where Apis was almost the only pollinator). In near sites, however, overall visitation rates dropped only 9% because native species almost entirely compensated for the Apis decline. Forest fragments (more so than the riparian strip) thus provided nearby coffee with a diversity of bees that increased both the amount and stability of pollination services by reducing dependence on a single introduced pollinator. Exploring the economic links between forest preservation and coffee cultivation may help align the goals of conservation and agriculture within many regions of global conservation priority.',\n",
       " 'Analyzing student interactions and meaning construction in computer bulletin board discussions. This case study, based on social constructivist learning theory, investigated the communication patterns and the knowledge construction process of students who used a computer bulletin board system (BBS) to discuss course-related content. Collected data included the outline of BBS postings and transcripts of the BBS messages from three selected weeks during the semester in an advanced communications class. Quantitative analysis was used to examine participation and interaction rates, and qualitative procedures were used to analyze knowledge construction processes and to refine a category system of indicators and descriptors. Results showed that students engaged in a knowledge construction process that was characterized chiefly by clarification, elaboration, and interpretation, and that produced more reflective monologues than dialogical interactions. Findings were related to constructivist theories and to previous analyses of computer conferencing systems, and were used to develop a list of recommendations for practitioners interested in incorporating such systems in their courses.',\n",
       " 'A Semantic Web Primer. {The development of the Semantic Web, with machine-readable content, has the potential to revolutionize the World Wide Web and its use. <i>A Semantic Web Primer</i> provides an introduction and guide to this emerging field, describing its key ideas, languages, and technologies. Suitable for use as a textbook or for self-study by professionals, it concentrates on undergraduate-level fundamental concepts and techniques that will enable readers to proceed with building applications on their own. It includes exercises, project descriptions, and annotated references to relevant online materials. A Semantic Web Primer is the only available book on the Semantic Web to include a systematic treatment of the different languages (XML, RDF, OWL, and rules) and technologies (explicit metadata, ontologies, and logic and inference) that are central to Semantic Web development. The book also examines such crucial related topics as ontology engineering and application scenarios.<br /> <br /> After an introductory chapter, topics covered in succeeding chapters include XML and related technologies that support semantic interoperability; RDF and RDF Schema, the standard data model for machine-processable semantics; and OWL, the W3C-approved standard for a Web ontology language more extensive than RDF Schema; rules, both monotonic and nonmonotonic, in the framework of the Semantic Web; selected application domains and how the Semantic Web would benefit them; the development of ontology-based systems; and current debates on key issues and predictions for the future.}',\n",
       " 'Regulation of RNA Polymerase II Transcription by Sequence-Specific DNA Binding Factors. In eukaryotes, transcription of the diverse array of tens of thousands of protein-coding genes is carried out by RNA polymerase II. The control of this process is predominantly mediated by a network of thousands of sequence-specific DNA binding transcription factors that interpret the genetic regulatory information, such as in transcriptional enhancers and promoters, and transmit the appropriate response to the RNA polymerase II transcriptional machinery. This review will describe some early advances in the discovery and characterization of the sequence-specific DNA binding transcription factors as well as some of the properties of these regulatory proteins.',\n",
       " 'Seeing Is Believing - The Bicoid Morphogen Gradient Matures. Although Cell has a long history of publishing some of the most significant advances in developmental biology, the back to back papers by Driever and Nüsslein-Volhard on the role of the Bicoid gradient in patterning the Drosophila embryo stand out as the first molecular demonstration of two of the longest standing concepts of the field, namely localized cytoplasmic determinants and morphogen gradients. Here we discuss the impact of this ground-breaking work and review recent results on bicoid mRNA localization and the dual role of Bicoid as a transcription and translation factor.',\n",
       " 'How to Talk About the Body? the Normative Dimension of Science Studies. Science studies has often been against the normative dimension of epistemology, which made a naturalistic study of science impossible. But this is not to say that a new   type of normativity cannot be detected at work inscience studies. This is especially true in the second wave of studies dealing with the body, which has aimed at criticizing the physicalization of the body without falling into the various traps   of a phenomenology simply added to a physical substrate. This article explores the work of Isabelle Stengers and Vinciane Despret in that respect, and shows how it can   be used to rethink the articulation between the various levels that make up a body. 10.1177/1357034X04042943',\n",
       " 'Circular binary segmentation for the analysis of array-based DNA copy number data.. DNA sequence copy number is the number of copies of DNA at a region of a genome. Cancer progression often involves alterations in DNA copy number. Newly developed microarray technologies enable simultaneous measurement of copy number at thousands of sites in a genome. We have developed a modification of binary segmentation, which we call circular binary segmentation, to translate noisy intensity measurements into regions of equal copy number. The method is evaluated by simulation and is demonstrated on cell line data with known copy number alterations and on a breast cancer cell line data set.',\n",
       " 'GO::TermFinder-open source software for accessing Gene Ontology information and finding significantly enriched Gene Ontology terms associated with a list of genes. SUMMARY: GO::TermFinder comprises a set of object-oriented Perl modules for accessing Gene Ontology (GO) information and evaluating and visualizing the collective annotation of a list of genes to GO terms. It can be used to draw conclusions from microarray and other biological data, calculating the statistical significance of each annotation. GO::TermFinder can be used on any system on which Perl can be run, either as a command line application, in single or batch mode, or as a web-based CGI script. AVAILABILITY: The full source code and documentation for GO::TermFinder are freely available from http://search.cpan.org/dist/GO-TermFinder/.',\n",
       " 'MedlineR: an open source library in R for Medline literature data mining.. SUMMARY: We describe an open source library written in the R programming language for Medline literature data mining. This MedlineR library includes programs to query Medline through the NCBI PubMed database; to construct the co-occurrence matrix; and to visualize the network topology of query terms. The open source nature of this library allows users to extend it freely in the statistical programming language of R. To demonstrate its utility, we have built an application to analyze term-association by using only ten lines of code. We provide MedlineR as a library foundation for bioinformaticians and statisticians to build more sophisticated literature data mining applications. AVAILABILITY: The library is available from http://dbsr.duke.edu/pub/MedlineR.',\n",
       " 'DAGchainer: a tool for mining segmental genome duplications and synteny. Summary: Given the positions of protein-coding genes along genomic sequence and probability values for protein alignments between genes, DAGchainer identifies chains of gene pairs sharing conserved order between genomic regions, by identifying paths through a directed acyclic graph (DAG). These chains of collinear gene pairs can represent segmentally duplicated regions and genes within a single genome or syntenic regions between related genomes. Automated mining of the Arabidopsis genome for segmental duplications illustrates the use of DAGchainer. 10.1093/bioinformatics/bth397',\n",
       " 'Discovering patterns to extract protein-protein interactions from full texts.. MOTIVATION: Although there are several databases storing protein-protein interactions, most such data still exist only in the scientific literature. They are scattered in scientific literature written in natural languages, defying data mining efforts. Much time and labor have to be spent on extracting protein pathways from litera-ture. Our aim is to develop a robust and powerful methodology to mine protein-protein interactions from biomedical texts. RESULTS: We present a novel and robust approach for extracting protein-protein interactions from literature. Our method uses a dynamic programming algorithm to compute distinguishing patterns by aligning relevant sentences and key verbs that describe protein inter-actions. A matching algorithm is designed to extract the interactions between proteins. Equipped only with a dictionary of protein names, our system achieves a recall rate of 80.0% and precision rate of 80.5%. AVAILABILITY: The program is available on request from the authors.',\n",
       " 'Modeling interactome: scale-free or geometric?. MOTIVATION: Networks have been used to model many real-world phenomena to better understand the phenomena and to guide experiments in order to predict their behavior. Since incorrect models lead to incorrect predictions, it is vital to have as accurate a model as possible. As a result, new techniques and models for analyzing and modeling real-world networks have recently been introduced. RESULTS: One example of large and complex networks involves protein-protein interaction (PPI) networks. We analyze PPI networks of yeast S. cerevisiae and fruitfly D. melanogaster using a newly introduced measure of local network structure as well as the standardly used measures of global network structure. We examine the fit of four different network models, including Erd?s-R?nyi, scale-free, and geometric random network models, to these PPI networks with respect to the measures of local and global network structure. We demonstrate that the currently accepted scale-free model of PPI networks fails to fit the data in several respects and show that a random geometric model provides a much more accurate model of the PPI data. We hypothesize that only the noise in these networks is scale-free. Conclusions: We systematically evaluate how well different network models fit the PPI networks. We show that the structure of PPI networks is better modeled by a geometric random graph than by a scale-free model. CONTACT: Jurisica, I., Ontario Cancer Institute, Princess Margaret Hospital, University Health Network, Division of Cancer Informatics, 610 University Avenue, Toronto, ON, M5G 2M9, Canada. SUPPLEMENTARY INFORMATION: Supplementary information is available and submitted together with this manuscript.',\n",
       " 'Blogs and Wikis Are Valuable Software Tools for Communication Within Research Groups. :&#8194; Appropriate software tools may improve communication and ease access to knowledge for research groups. A weblog is a website which contains periodic, chronologically ordered posts on a common webpage, whereas a wiki is hypertext-based collaborative software that enables documents to be authored collectively using a web browser. Although not primarily intended for use as an intranet-based collaborative knowledge warehouse, both blogs and wikis have the potential to offer all the features of complex and expensive IT solutions. These tools enable the team members to share knowledge simply and quickly&#8212;the collective knowledge base of&#160; the group can be efficiently managed and navigated.',\n",
       " 'A review of routing protocols for mobile ad hoc networks. The 1990s have seen a rapid growth of research interests in mobile ad hoc networking. The infrastructureless and the dynamic nature of these networks demands new set of networking strategies to be implemented in order to provide efficient end-to-end communication. This, along with the diverse application of these networks in many different scenarios such as battlefield and disaster recovery, have seen MANETs being researched by many different organisations and institutes. MANETs employ the traditional TCP/IP structure to provide end-to-end communication between nodes. However, due to their mobility and the limited resource in wireless networks, each layer in the TCP/IP model require redefinition or modifications to function efficiently in MANETs. One interesting research area in MANET is routing. Routing in the MANETs is a challenging task and has received a tremendous amount of attention from researches. This has led to development of many different routing protocols for MANETs, and each author of each proposed protocol argues that the strategy proposed provides an improvement over a number of different strategies considered in the literature for a given network scenario. Therefore, it is quite difficult to determine which protocols may perform best under a number of different network scenarios, such as increasing node density and traffic. In this paper, we provide an overview of a wide range of routing protocols proposed in the literature. We also provide a performance comparison of all routing protocols and suggest which protocols may perform best in large networks.',\n",
       " 'Recursion is a computational effect. In a recent paper, Launchbury, Lewis, and Cook observe that some Haskell  applications could benefit from a combinator mfix for expressing recursion over  monadic types. We investigate three possible definitions of mfix and implement  them in Haskell.  Like traditional fixpoint operators, there are two approaches to the definition  of mfix: an unfolding one based on mathematical semantics, and an updating  one based on operational semantics. The two definitions are equivalent  in pure calculi...',\n",
       " 'A Short Survey of Noncommutative Geometry. We give a survey of selected topics in noncommutative geometry, with some emphasis on those directly related to physics, including our recent work with Dirk Kreimer on renormalization and the Riemann-Hilbert problem. We discuss at length two issues. The first is the relevance of the paradigm of geometric space, based on spectral considerations, which is central in the theory. As a simple illustration of the spectral formulation of geometry in the ordinary commutative case, we give a polynomial equation for geometries on the four dimensional sphere with fixed volume. The equation involves an idempotent e, playing the role of the instanton, and the Dirac operator D. It expresses the gamma five matrix as the pairing between the operator theoretic chern characters of e and D. It is of degree five in the idempotent and four in the Dirac operator which only appears through its commutant with the idempotent. It determines both the sphere and all its metrics with fixed volume form.   We also show using the noncommutative analogue of the Polyakov action, how to obtain the noncommutative metric (in spectral form) on the noncommutative tori from the formal naive metric. We conclude on some questions related to string theory.',\n",
       " 'An actor-network critique of community in higher education: implications for networked learning. This article provides an actor-network critique of ideas on community that are influential in higher education and draws implications for networked learning theory and practice. Networked learning is examined as an educational movement which contains alternative models of learning but which offers to create a sense of virtual community within the structures of mass higher education. Benedict Anderson’s work is drawn upon to understand the notion of community and to examine ‘the nation’ as  prototypical community. Aspects of actor-network theory are discussed and illustrated, from which a critique of the idea of community in higher education is developed.',\n",
       " 'Designs for network learning: a communities of practice perspective. This article explores the relevance for network learning of themes developed by Wenger, initially with Lave and subsequently alone. While Wenger’s fieldwork is located in the workplace, he sees his theorization on becoming a learner as applicable to any context, bit it home work or formal education. In unraveling the connectedness between learning identity and community, usefulness of Wenger’s ideas for the context of networked learning is exposed. First, the specific features of Wenger’s construct of community of practice are discussed; second, Wenger’s notions of participation and reification are explored; and, finally, his design perspective with respect to ‘facilities of engagement, imagination and alignment’ is presented. The exposition of Wenger’s *and Lave’s) ideas is interwoven with a discussion of their implications for the field of network learning.',\n",
       " 'Examining the dynamics of networked e-learning groups and communities. The organisation of students into groups (or communities) for learning purposes is an established pedagogic method in higher education. Teachers are now using group methods in networked e-learning contexts, albeit without a full understanding of the dynamics of group work in these settings. This is a new and evolving arena in higher education. In this article, the learning dynamics of three collaborative, networked e-learning groups are examined in an attempt to understand how students work in them. A detailed ethnography indicates that two of the groups worked harmoniously, and successfully produced a collective end product. The other group exhibited extreme anxiety and division, and required extra resources from its members in order to sustain itself and produce its collective end product. Anxiety became a major focus for this group, which had the effect of diverting it from effective collective production. The ethnography shows that the place of identity, control, ontological security and guilt in collaborative e-learning groups can be central to the effective work of the groups. The difference between the groups with respect to these categories is used as a point of departure in order to show how an understanding of the dynamics of networked learning groups and communities may be of benefit to teachers and students working in these new environments.',\n",
       " \"Consensus, difference and 'multiple communities' in networked learning. The article reviews the popularity in networked learning designs for values of collaboration, and in particular, of community. Examples of this are drawn from the networked learning literature, highlighting corresponding arguments for networked learning providing the basis for a more democratic ethos within higher educational programmes. The authors critique the notion of 'community', especially its association with consensus and pressures to conform. They argue for an interpretation of community which would be more likely to take account of differences, without suppressing or 'managing' them, and cite examples of network learning structures which seem to be based on principles more sympathetic to this aim.\",\n",
       " 'Subtyping Recursive Types. We investigate the interactions of subtyping and recursive types, in a simply typed &lgr;-calculus. The two fundamental questions here are whether two (recursive)types are in the subtype relation and whether a term has a type. To address the first question, we relate various definitions of type equivalence and subtyping that are induced by a model, an ordering on infinite trees, an algorithm, and a set of type rules. We show soundness and completeness among the rules, the algorithm, and the tree semantics. We also prove soundness and a restricted form of completeness for the model. To address the second question, we show that to every pair of types in the subtype relation we can associate a term whose denotation is the uniquely determined coercion map between the two types. Moreover, we derive an algorithm that, when given a term with implicit coercions, can infer its least type whenever possible.',\n",
       " \"Theorems for Free!. From the type of a polymorphic function we can derive a theorem that it satisfies. Every function of the same type satisfies the same theorem. This provides a free source of useful theorems, courtesy of Reynolds' abstraction theorem for the polymorphic lambda calculus.  1 Introduction  Write down the definition of a polymorphic function on a piece of paper. Tell me its type, but be careful not to let me see the function's definition. I will tell you a theorem that the function satisfies. The...\",\n",
       " \"Codata and Comonads in Haskell. Haskell, a wide spectrum, functional programming language, provides means to define and use an extremely rich variety of data including free, polymorphic datatypes, type classes, and data with additional computational structure abstracted by monads. Somewhat less attention has been given to supporting abstract data types, which we shall call codata types. Monomorphic, parameterless versions of codata types can be defined by modules, but Haskell's module system is not comparably powerful with the class system.\",\n",
       " \"Power laws, Pareto distributions and Zipf's law. When the probability of measuring a particular value of some quantity varies inversely as a power of that value, the quantity is said to follow a power law, also known variously as Zipf's law or the Pareto distribution. Power laws appear widely in physics, biology, earth and planetary sciences, economics and finance, computer science, demography and the social sciences. For instance, the distributions of the sizes of cities, earthquakes, forest fires, solar flares, moon craters and people's personal fortunes all appear to follow power laws. The origin of power-law behaviour has been a topic of debate in the scientific community for more than a century. Here we review some of the empirical evidence for the existence of power-law forms and the theories proposed to explain them.\",\n",
       " 'The marriage of effects and monads. Gifford and others proposed an  effect  typing discipline to delimit the scope of computational effects within a program, while Moggi and others proposed  monads  for much the same purpose. Here we marry effects to monads, uniting two previously separate lines of research. In particular, we show that the type, region, and effect system of Talpin and Jouvelot carries over directly to an analogous system for monads, including a type and effect reconstruction algorithm. The same technique should allow one to transpose any effect system into a corresponding monad system.',\n",
       " 'Adoption and focus: practical linear types for imperative programming. A type system with linearity is useful for checking software protocols and resource management at compile time. Linearity provides powerful reasoning about state changes, but at the price of restrictions on aliasing. The hard division between linear and nonlinear types forces the programmer to make a trade-off between checking a protocol on an object and aliasing the object. Most onerous is the restriction that any type with a linear component must itself be linear. Because of this, checking a protocol on an object imposes aliasing restrictions on any data structure that directly or indirectly points to the object. We propose a new type system that reduces these restrictions with the adoption and focus constructs. Adoption safely allows a programmer to alias objects on which she is checking protocols, and focus allows the reverse. A programmer can alias data structures that point to linear objects and use focus for safe access to those objects. We discuss how we implemented these ideas in the Vault programming language.',\n",
       " 'Linear types can change the world!. The linear logic of J.-Y. Girard suggests a new type system for functional  languages, one which supports operations that \"change the world\". Values belonging  to a linear type must be used exactly once: like the world, they cannot be  duplicated or destroyed. Such values require no reference counting or garbage collection, and safely admit destructive array update. Linear types extend Schmidt\\'s  notion of single threading; provide an alternative to Hudak and Bloss\\' update  analysis; and offer a practical complement to Lafont and Holmström\\'s elegant linear languages.',\n",
       " \"A Taste of Linear Logic. . This tutorial paper provides an introduction to intuitionistic logic and linear logic, and shows how they correspond to type systems for functional languages via the notion of `Propositions as Types'. The presentation of linear logic is simplified by basing it on the Logic of Unity. An application to the array update problem is briefly discussed. 1 Introduction Some of the best things in life are free; and some are not. Truth is free. Having proved a theorem, you may use this proof as many...\",\n",
       " 'Once upon a type. A number of useful optimisations are enabled if we can determine when a value is accessed at most once. We extend the Hindley-Milner type system with uses, yielding a type-inference based program analysis which determines when values are accessed at most once. Our analysis can handle higher-order functions and data structures, and admits principal types for terms. Unlike previous analyses, we prove our analysis sound with respect to call-by-need reduction. Call-by-name reduction does not...',\n",
       " 'Social presence in distributed group environments: the role of social identity. This paper argues that to achieve social presence in a distributed environment, it is not necessary to emulate face-to-face conditions of increased cues to the interpersonal. Rather, it is argued, that a sense of belongingness to the group, or perceptual immersion in the group, can be realised through the creation of a shared social identity between group members. From this perspective, social presence is a function of the cognitive representation of the group by group members and not the interpersonal bonds between group members. Furthermore, specific design features and characteristics of the distributed learning environment can be utilised to achieve and maintain this shared group identity. This approach, encapsulated by the SIDE model, is discussed and supported by two case studies of distributed students, each consisting of 10 groups, collaborating for a period of 5 weeks on group projects.',\n",
       " 'The Mathematical Development of Set Theory from Cantor to Cohen. This article is dedicated to Professor Burton Dreben on his coming of age. I owe him particular thanks for his careful reading and numerous suggestions for improvement. My thanks go also to Jose Ruiz and the referee for their helpful comments. Parts of this account were given at the 1995 summer meeting of the Association for Symbolic Logic at Haifa, in the Massachusetts Institute of Technology logic seminar, and to the Paris Logic Group. The author would like to express his thanks to the...',\n",
       " 'Autonomous Gossiping: A self-organizing epidemic algorithm for selective. We introduce autonomous gossiping (A/G), a new genre epidemic algorithm for selective dissemination of information in contrast to previous usage of epidemic algorithms which flood the whole network. A/G is a paradigm which suits well in a mobile ad-hoc networking (MANET) environment because it does not require any infrastructure or middleware like multicast tree and (un)subscription maintenance for publish/subscribe, but uses ecological and economic principles in a self-organizing manner in...',\n",
       " 'RoMEO studies 1: the impact of copyright ownership on academic author self-archiving. \\t\\t\\tThis is the first of a series of studies emanating from the UK JISC-funded RoMEO Project (Rights Metadata for Open-archiving) which investigated the IPR issues relating to academic author self-archiving of research papers. It considers the claims for copyright ownership in research papers by universities, academics, and publishers by drawing on the literature, a survey of 542 academic authors and an analysis of 80 journal publisher copyright transfer agreements. The paper concludes that self-archiving is not best supported by copyright transfer to publishers. It recommends that universities assert their interest in copyright ownership in the long term, that academics retain rights in the short term, and that publishers consider new ways of protecting the value they add through journal publishing.',\n",
       " 'Clustering data streams: Theory and practice. We study clustering under the data stream model of computation where: given a sequence of points, the objective is to maintain a consistently good clustering of the sequence observed so far, using a small amount of memory and time. The data stream model is relevant to new classes of applications involving massive data sets, such as web click stream analysis and multimedia data analysis. We give constant-factor approximation algorithms for the k-Median problem in the data stream model of computation in a single pass.  We also give negative results implying our algorithm cannot be improved in a certain sense.',\n",
       " 'Data Streams: Algorithms and Applications. Data stream algorithms as an active research agenda emerged only over the pastfew years, even though the concept of making few passes over the data forperforming computations has been around since the early days of AutomataTheory. The data stream agenda now pervades many branches of Computer Scienceincluding databases, networking, knowledge discovery and data mining, andhardware systems. Industry is in synch too, with Data Stream ManagementSystems (DSMSs) and special hardware to deal with data speeds. Even beyondComputer Science, data stream concerns are emerging in physics, atmosphericscience and statistics. Data Streams: Algorithms and Applications focuses onthe algorithmic foundations of data streaming. In the data stream scenario,input arrives very rapidly and there is limited memory to store the input.Algorithms have to work with one or few passes over the data, space less thanlinear in the input size or time significantly less than the input size. Inthe past few years, a new theory has emerged for reasoning about algorithmsthat work within these constraints on space, time and number of passes. Someof the methods rely on metric embeddings, pseudo-random computations, sparseapproximation theory and communication complexity. The applications for thisscenario include IP network traffic analysis, mining text message streams andprocessing massive data sets in general. Data Streams: Algorithms andApplications surveys the emerging area of algorithms for processing datastreams and associated applications. An extensive bibliography with over 200entries points the reader to further resources for exploration.',\n",
       " 'Continuously Adaptive Continuous Queries over Streams. We present a continuously adaptive, continuous query (CACQ) implementation based on the eddy query processing framework. We show that our design provides significant performance benefits over existing approaches to evaluating continuous queries, not only because of its adaptivity, but also because of the aggressive cross-query sharing of work and space that it enables. By breaking the abstraction of shared relational algebra expressions, our Telegraph CACQ implementation is able to share physical operators --- both selections and join state --- at a very fine grain. We augment these features with a grouped-filter index to simultaneously evaluate multiple selection predicates. We include measurements of the performance of our core system, along with a comparison to existing continuous query approaches.',\n",
       " 'Adaptive query processing: A survey.  In wide-area database systems, which may be running on unpredictable and volatile environments (such as computational grids), it is difficult to produce efficient database query plans based on information available solely at compile time. A solution to this problem is to exploit information that becomes available at query runtime and adapt the query plan to changing conditions during execution. This paper presents a survey on adaptive query processing techniques, examining the opportunities they offer to modify a plan dynamically and classifying them into categories according to the problem they focus on, their objectives, the nature of feedback they collect from the environment, the frequency at which they can adapt, their implementation environment and which component is responsible for taking the adaptation decisions.',\n",
       " 'Aurora: a new model and architecture for data stream management. Abstract.This paper describes the basic processing model and architecture of Aurora, a new system to manage data streams for monitoring applications. Monitoring applications differ substantially from conventional business data processing. The fact that a software system must process and react to continual inputs from many sources (e.g., sensors) rather than from human operators requires one to rethink the fundamental architecture of a DBMS for this application area. In this paper, we present Aurora, a new DBMS currently under construction at Brandeis University, Brown University, and M.I.T. We first provide an overview of the basic Aurora model and architecture and then describe in detail a stream-oriented set of operators.',\n",
       " 'Query Languages and Data Models for Database. We study the fundamental limitations of relational  algebra (RA) and SQL in supporting  sequence and stream queries, and present effective  query language and data model enrichments  to deal with them. We begin by observing  the well-known limitations of SQL in  application domains which are important for  data streams, such as sequence queries and  data mining. Then we present a formal proof  that, for continuous queries on data streams,  SQL su#ers from additional expressive power  problems....',\n",
       " 'Queryfree news search. Many daily activities present information in the form of a stream of text, and often people can benefit from additional information on the topic discussed. TV broadcast news can be treated as one such stream of text; in this paper we discuss finding news articles on the web that are relevant to news currently being broadcast.We evaluated a variety of algorithms for this problem, looking at the impact of inverse document frequency, stemming, compounds, history, and query length on the relevance and coverage of news articles returned in real time during a broadcast. We also evaluated several postprocessing techniques for improving the precision, including reranking using additional terms, reranking by document similarity, and filtering on document similarity. For the best algorithm, 84%-91% of the articles found were relevant, with at least 64% of the articles being on the exact topic of the broadcast. In addition, a relevant article was found for at least 70% of the topics.',\n",
       " \"Random early detection gateways for congestion avoidance. This paper presents Random Early Detection (RED) gateways for congestion avoidance in packet-switched networks. The gateway detects incipient congestion by computing the average queue size. The gateway could notify connections of congestion either by dropping packets arriving at the gateway or by setting a bit in packet headers. When the average queue size exceeds a preset threshold, the gateway drops or marks each arriving packet with a certain probability, where the exact probability is a function of the average queue size. RED gateways keep the average queue size low while allowing occasional bursts of packets in the queue. During congestion, the probability that the gateway notifies a particular connection to reduce its window is roughly proportional to that connection's share of the bandwidth through the gateway. RED gateways are designed to accompany a transport-layer congestion control protocol such as TCP. The RED gateway has no bias against bursty traffic and avoids the global synchronization of many connections decreasing their window at the same time. Simulations of a TCP/IP network are used to illustrate the performance of RED gateways.\",\n",
       " 'A measurement study of the bittorrent peer-to-peer file-sharing system. P2P systems for sharing content have become very popular over the last few  years. However, despite the increasing attention of both the research community  and large numbers of users, the actual behavior of these systems over prolonged periods  of time is still poorly understood. This paper presents a detailed measurement  study over a period of eight months of BitTorrent/Suprnova, a P2P file-sharing  system that is quickly gaining in popularity. In particular, we show measurement  results of...',\n",
       " \"Analyzing peer-to-peer traffic across large networks. The use of peer-to-peer (P2P) applications is growing dramatically, particularly for sharing large video/audio files and software. In this paper, we analyze P2P traffic by measuring flow-level information collected at multiple border routers across a large ISP network, and report our investigation of three popular P2P systems-FastTrack, Gnutella, and Direct-Connect. We characterize the P2P traffic observed at a single ISP and its impact on the underlying network. We observe very skewed distribution in the traffic across the network at different levels of spatial aggregation (IP, prefix, AS). All three P2P systems exhibit significant dynamics at short time scale and particularly at the IP address level. Still, the fraction of P2P traffic contributed by each prefix is more stable than the corresponding distribution of either Web traffic or overall traffic. The high volume and good stability properties of P2P traffic suggests that the P2P workload is a good candidate for being managed via application-specific layer-3 traffic engineering in an ISP's network.\",\n",
       " 'The Small-World Phenomenon: An Algorithmic Perspective. Long a matter of folklore, the &amp;quot;small-world phenomenon &amp;quot;--- the principle that we are all linked by short chains of acquaintances--- was inaugurated as an area of experimental study in the social sciences through the pioneering work of Stanley Milgram in the 1960&#039;s. This work was among the first to make the phenomenon quantitative, allowing people to speak of the &amp;quot;six degrees of separation &amp;quot; between any two people in the United States. Since then, a number of network models have been proposed as frameworks in which to study the problem analytically. One of the most refined of these models was formulated in recent work of Watts and Strogatz; their framework provided compelling evidence that the small-world phenomenon is pervasive in a range of networks arising in nature and technology, and a fundamental ingredient in the evolution of the World Wide Web. But existing models are insu#cient to explain the striking algorithmic component of Milgram&#039;s original findings: that individuals using local information are collectively very e#ective at actually constructing short paths between two points in a social network. Although recently proposed network models are rich in short paths, we prove that no decentralized algorithm, operating with local information only, can construct short paths in these networks with non-negligible probability. We then define an infinite family of network models that naturally generalizes the Watts-Strogatz model, and show that for one of these models, there is a decentralized algorithm capable of finding short paths with high probability. More generally, we provide a strong characterization of this family of network models, showing that there is in fact a unique model within the family for which decentralized algorithms are e#ective.',\n",
       " 'Purely functional data structures. Most books on data structures assume an imperative language such as C or C++. However, data structures for these languages do not always translate well to functional languages such as Standard ML, Haskell, or Scheme. This book describes data structures from the point of view of functional languages, with examples, and presents design techniques that allow programmers to develop their own functional data structures. The author includes both classical data structures, such as red-black trees and binomial queues, and a host of new data structures developed exclusively for functional languages. All source code is given in Standard ML and Haskell, and most of the programs are easily adaptable to other functional languages. This handy reference for professional programmers working with functional languages can also be used as a tutorial or for self-study.',\n",
       " 'Modular Domain Specific Languages and Tools. A domain specific language (DSL) allows one to develop software for a particular application domain quickly and effectively, yielding programs that are easy to understand, reason about, and maintain. On the other hand, there may be a significant overhead in creating the infrastructure needed to support a DSL. To solve this problem, a methodology is described for building domain specific embedded languages (DSELs), in which a DSL is designed within an existing, higher-order and typed, programming language such as Haskell or ML. In addition, techniques are described for building modular interpreters and tools for DSELs. The resulting methodology facilitates reuse of syntax, semantics, implementation code, software tools, as well as look-and-feel.',\n",
       " 'Domain-Specific Languages: An Annotated Bibliography. Domain-Specific Languages are used in software engineering in order to enhance quality, flexibility, and timely delivery of software systems, by taking advantage of specific properties of a particular application domain. This survey covers terminology, risks and benefits, examples, design methodologies, and implementation techniques of domain-specific languages as used for the construction and maintenance of software systems. Moreover, it covers an annotated selection of 75 key publications in...',\n",
       " \"Compiling Embedded Languages. Functional languages are particularly well-suited to the implementation of interpreters for domain-specific embedded languages (DSELs). We describe an implemented technique for producing optimizing compilers for DSELs, based on Kamin's idea of DSELs for program generation. The technique uses a data type of syntax for basic types, a set of smart constructors that perform rewriting over those types, some code motion transformations, and a back-end code generator. Domain-specific optimization...\",\n",
       " 'Notable design patterns for domain-specific languages. The realisation of domain-specific languages ( s) differs in fundamental ways from that of traditional programming languages. We describe eight recurring patterns that we have identified as being used for design and implementation. Existing languages can be extended, restricted, partially used, or become hosts for s. Simple s can be implemented by lexical processing. In addition, s can be used to create front-ends to existing systems or to express complicated data structures. Finally, s can be combined using process pipelines. The patterns described form a pattern language that can be used as a building block for a systematic view of the software development process involving s.',\n",
       " 'Monadic Parser Combinators. In functional programming, a popular approach to building recursive descent parsers is to model parsers as functions, and to define higher-order functions (or combinators) that implement grammar constructions such as sequencing, choice, and repetition. Such parsers form an instance of a monad , an algebraic structure from mathematics that has proved useful for addressing a number of computational problems. The purpose of this article is to provide a step-by-step tutorial on the monadic approach to building functional parsers, and to explain some of the benefits that result from exploiting monads. No prior knowledge of parser combinators or of monads is assumed. Indeed, this article can also be viewed as a first introduction to the use of monads in programming.  2 Graham Hutton and Erik Meijer  Contents  1 Introduction 3 2 Combinator parsers 4 2.1 The type of parsers 4 2.2 Primitive parsers 4 2.3 Parser combinators 5 3 Parsers and monads 8 3.1 The parser monad 8 3.2 Monad comprehension ...',\n",
       " 'Organizational Learning. This paper reviews the literature on organizational learning. Organizational learning is viewed as routine-based, history-dependent, and target-oriented. Organizations are seen as learning by encoding inferences from history into routines that guide behavior. Within this perspective on organizational learning, topics covered include how organizations learn from direct experience, how organizations learn from the experience of others, and how organizations develop conceptual frameworks or paradigms for interpreting that experience. The section on organizational memory discusses how organizations encode, store, and retrieve the lessons of history despite the turnover of personnel and the passage of time. Organizational learning is further complicated by the ecological structure of the simultaneously adapting behavior of other organizations, and by an endogenously changing environment. The final section discusses the limitations as well as the possibilities of organizational learning as a form of intelligence.',\n",
       " \"Predicate Dispatching: A Unified Theory of Dispatch. Predicate dispatching generalizes previous method dispatch  mechanisms by permitting arbitrary predicates to control method applicability  and by using logical implication between predicates as the  overriding relationship. The method selected to handle a message send  can depend not just on the classes of the arguments, as in ordinary  object-oriented dispatch, but also on the classes of subcomponents, on  an argument's state, and on relationships between objects. This simple  mechanism...\",\n",
       " 'Concurrent Haskell. Some applications are most easily expressed in a programming language that supports concurrency, notably interactive and distributed systems. We propose extensions to the purely-functional language Haskell that allows it to express explicitly concurrent applications; we call the resulting language Concurrent Haskell. The resulting system appears to be both expressive and efficient, and we give a number of examples of useful abstractions that can be built from our primitives. We have developed a freely-available implementation of Concurrent Haskell, and are now using it as a substrate for a graphical user interface toolkit. 1',\n",
       " \"Open Access Is Only Part of the Story. Open Access is increasingly considered the most logical and hopeful solution to ease the burden on library budgets, as well as the best way to unshackle research findings--its primary role. But is Open Access the panacea for all the ills that currently beset those of us dedicated to the dissemination and preservation of good quality academic research? In this article, Richard Gedye (Sales Director, Oxford Journals) explores the background to one of Oxford Journals' current Open Access initiatives and explains some elements of the model adopted. He also offers his personal opinions on the significant role librarians could play in the Open Access story.\",\n",
       " \"The Shifting Sands of Open Access Publishing, a Publisher's View. This paper sets Open Access (OA) publishing in the context of today's scientific, technical, and medical (STM) publishing trends. Four areas are covered: (a) a brief overview of STM publishing and its value today; (b) OA's place in the industry; (c) the underlying economics of OA, particularly its author-pays model; and (d) directions in moving towards “universal access” to STM information, where both researchers and the public have access to the scientific information they need.\",\n",
       " \"A Not-for-Profit Publisher's Perspective on Open Access. Recent legislative activity in the US House of Representatives and the UK House of Commons has added fuel to a debate over electronic access to the Scientific, Technical and Medical (STM) literature that was initiated in 1999 with the introduction of E-Biomed. Ongoing efforts to change the landscape of STM publishing involve moving it away from a subscription basis to an author-pays model. This article chronicles the swift evolution of electronic access to the scientific literature and asks whether the scholarly community will really be better off with government-mandated Open Access (OA) publishing.\",\n",
       " 'Delivery, Management and Access Model for E_prints and Open Access Journals. A study conducted for the (United Kingdom) Joint Information Systems Committee reviewed possible models for implementing Open Access to research reports in institutional archives and Open Access journals. The conclusion was that a \"harvesting model,\" in which full texts reside on the original servers but metadata are harvested, held, and enhanced by a central service, was preferable to either a centralized national service or a completely decentralized service for the UK. The study included issues of populating institutional archives (IAs) and some form of mandatory archiving for publicly funded research results to obtain a critical mass of Open Access material in such a system.',\n",
       " 'Open Access: Science Publishing as Science Publishing Should Be. Full and unimpeded access (Open Access) to science literature is needed. It is not provided by the traditional subscription-based publishing model. Instead of criticizing Open Access and attacking its proponents, traditional publishers should make imaginative and innovative efforts to build their businesses around the needs of their customers rather than around their desire to continue a model that may be lucrative, but that is no longer satisfactory to science or society.',\n",
       " 'The Access/Impact Problem and the Green and Gold Roads to Open Access. The research access/impact problem arises because journal articles are not accessible to all of their would-be users; hence, they are losing potential research impact. The solution is to make all articles Open Access {(OA;} i.e., accessible online, free for all). {OA} articles have significantly higher citation impact than {non-OA} articles. There are two roads to {OA:} the \"golden\" road (publish your article in an {OA} journal) and the \"green\" road (publish your article in a {non-OA} journal but also self-archive it in an {OA} archive). Only 5% of journals are gold, but over 90% are already green (i.e., they have given their authors the green light to self-archive); yet only about 10-20% of articles have been self-archived. To reach 100% {OA,} self-archiving needs to be mandated by researchers\\' employers and funders, as the United Kingdom and the United States have recently recommended, and universities need to implement that mandate.',\n",
       " 'Hourly analysis of a very large topically categorized web query log. We review a query log of hundreds of millions of queries that constitute the total query traffic for an entire week of a general-purpose commercial web search service. Previously, query logs have been studied from a single, cumulative view. In contrast, our analysis shows changes in popularity and uniqueness of topically categorized queries across the hours of the day. We examine query traffic on an hourly basis by matching it against lists of queries that have been topically pre-categorized by human editors. This represents 13% of the query traffic. We show that query traffic from particular topical categories differs both from the query stream as a whole and from other categories. This analysis provides valuable insight for improving retrieval effectiveness and efficiency. It is also relevant to the development of enhanced query disambiguation, routing, and caching algorithms.',\n",
       " 'Probabilistic query expansion using query logs. Query expansion has long been suggested as an effective way to resolve the short query and word mismatching problems. A number of query expansion methods have been proposed in traditional information retrieval. However, these previous methods do not take into account the specific characteristics of web searching; in particular, of the availability of large amount of user interaction information recorded in the web query logs. In this study, we propose a new method for query expansion based on query logs. The central idea is to extract probabilistic correlations between query terms and document terms by analyzing query logs. These correlations are then used to select high-quality expansion terms for new queries. The experimental results show that our log-based probabilistic query expansion method can greatly improve the search performance and has several advantages over other existing methods.',\n",
       " 'Assessing computational tools for the discovery of transcription factor binding sites. The prediction of regulatory elements is a problem where computational methods offer great hope. Over the past few years, numerous tools have become available for this task. The purpose of the current assessment is twofold: to provide some guidance to users regarding the accuracy of currently available tools in various settings, and to provide a benchmark of data sets for assessing future tools.',\n",
       " 'Document clustering by concept factorization. In this paper, we propose a new data clustering method called concept factorization that models each concept as a linear combination of the data points, and each data point as a linear combination of the concepts. With this model, the data clustering task is accomplished by computing the two sets of linear coefficients, and this linear coefficients computation is carried out by finding the non-negative solution that minimizes the reconstruction error of the data points. The cluster label of each data point can be easily derived from the obtained linear coefficients. This method differs from the method of clustering based on non-negative matrix factorization (NMF) in that it can be applied to data containing negative values and the method can be implemented in the kernel space. Our experimental results show that the proposed data clustering method and its variations performs best among 11 algorithms and their variations that we have evaluated on both TDT2 and Reuters-21578 corpus. In addition to its good performance, the new method also has the merit in its easy and reliable derivation of the clustering results',\n",
       " \"Modular epistasis in yeast metabolism.. Epistatic interactions, manifested in the effects of mutations on the phenotypes caused by other mutations, may help uncover the functional organization of complex biological networks. Here, we studied system-level epistatic interactions by computing growth phenotypes of all single and double knockouts of 890 metabolic genes in Saccharomyces cerevisiae, using the framework of flux balance analysis. A new scale for epistasis identified a distinctive trimodal distribution of these epistatic effects, allowing gene pairs to be classified as buffering, aggravating or noninteracting. We found that the ensuing epistatic interaction network could be organized hierarchically into function-enriched modules that interact with each other 'monochromatically' (i.e., with purely aggravating or purely buffering epistatic links). This property extends the concept of epistasis from single genes to functional units and provides a new definition of biological modularity, which emphasizes interactions between, rather than within, functional modules. Our approach can be used to infer functional gene modules from purely phenotypic epistasis measurements.\",\n",
       " \"The 'evolvability' of promiscuous protein functions.. How proteins with new functions (e.g., drug or antibiotic resistance or degradation of man-made chemicals) evolve in a matter of months or years is still unclear. This ability is dependent on the induction of new phenotypic traits by a small number of mutations (plasticity). But mutations often have deleterious effects on functions that are essential for survival. How are these seemingly conflicting demands met at the single-protein level? Results from directed laboratory evolution experiments indicate that the evolution of a new function is driven by mutations that have little effect on the native function but large effects on the promiscuous functions that serve as starting point. Thus, an evolving protein can initially acquire increased fitness for a new function without losing its original function. Gene duplication and the divergence of a completely new protein may then follow.\",\n",
       " 'Delivering Semantic Web Services. The growing infrastructure for Web Services assumes a programmer in the loop that hardcodes the connections between Web Services and directly programs Web Service composition. Emerging technology based on DAML-S and the Semantic Web allows Web Services to connect and transact automatically with minimal or no intervention from programmers. In this paper we discuss the problems related with autonomous Web Services, and how DAMLS provides the information to solve them. Furthermore, we describe...',\n",
       " \"Constraining Theories of Embodied Cognition. Influences of perceptual and motor activity on evaluation have led to theories of embodied cognition suggesting that putatively complex judgments can be carried out using only perceptual and motor representations. We present an experiment that revisited a movement-compatibility effect in which people are faster to respond to positive words by pulling a lever than by pushing a lever and are faster to respond to negative words by pushing than by pulling. We demonstrate that the compatibility effect depends on people's representation of their selves in space rather than on their physical location. These data suggest that accounting for embodied phenomena requires understanding the complex interplay between perceptual and motor representations and people's representations of their selves in space.\",\n",
       " 'Biological networks.. Recent advances in high-throughput methods have provided us with a first glimpse of the overall structure of molecular interaction networks in biological systems. Ultimately, we expect that such information will change how we think about biological systems in a fundamental way. Instead of viewing the genetic parts list of an organism as a loose collection of biochemical activities, in the best case, we anticipate discrete networks of function to bridge the gap between genotype and phenotype, and to do so in a more profound way than the current qualitative classification of linked reactions into familiar pathways, such as glycolysis and the MAPK signal transduction cascades. At the present time, however, we are still far from a complete answer to the most basic question: what can we learn about biology by studying networks? Promising steps in this direction have come from such diverse approaches as mathematical analysis of global network structure, partitioning networks into functionally related modules and motifs, and even  de novo  design of networks. A complete picture will probably require integrating the data obtained from all of these approaches with modeling efforts at many different levels of detail.',\n",
       " 'Exploring the diversity of complex metabolic networks.. MOTIVATION: Metabolism, the network of chemical reactions that make life possible, is one of the most complex processes in nature. We describe here the development of a computational approach for the identification of every possible biochemical reaction from a given set of enzyme reaction rules that allows the de novo synthesis of metabolic pathways composed of these reactions, and the evaluation of these novel pathways with respect to their thermodynamic properties. RESULTS: We applied this framework to the analysis of the aromatic amino acid pathways and discovered almost 75 000 novel biochemical routes from chorismate to phenylalanine, more than 350 000 from chorismate to tyrosine, but only 13 from chorismate to tryptophan. Thermodynamic analysis of these pathways suggests that the native pathways are thermodynamically more favorable than the alternative possible pathways. The pathways generated involve compounds that exist in biological databases, as well as compounds that exist in chemical databases and novel compounds, suggesting novel biochemical routes for these compounds and the existence of biochemical compounds that remain to be discovered or synthesized through enzyme and pathway engineering. AVAILABILITY: Framework will be available via web interface at http://systemsbiology.northwestern.edu/BNICE (site under construction). CONTACT: vassily@northwestern.edu or broadbelt@northwestern.edu SUPPLEMENTARY INFORMATION: http://systemsbiology.northwestern.edu/BNICE/publications.',\n",
       " \"Affective computing. {As a scientist who works in computer development, Rosalind Picard is accustomed to working with what is rational and logical. But in her research on how to enable computers to better perceive the world, she discovered something surprising: In the human brain, a critical part of our ability to see and perceive is not logical, but emotional. Therefore, for computers to have some of the advanced abilities we desire, it may be necessary that they comprehend and, in some cases, feel emotions. <i>Affective Computing</i> isn't about making PCs that get grumpy when you enter repeated errors or that may react out of fear like <i>2001</i>'s Hal or <i>The Terminator</i>'s SkyNet; it's about incorporating emotional competencies that allow computers to better perform their jobs. On the simplest level, this may mean installing sensors and programming that simply allow a computerized system to determine the emotional state of its user and respond accordingly. The book also mentions options such as the ability to include emotional content in computer-moderated communications that work far better than today's emoticons.<p> The first part of Picard's book introduces the theoretical foundations and principles of affective computing in a thoroughly nontechnical manner. She explores why feelings may soon become part of computing technology and discusses the advantages and the concerns of such a development. Picard raises a number of ethical issues, including the potential for misleading users into thinking they're communicating with another human and the need to incorporate responsible behavior into affective computer programming, along the lines of Isaac Asimov's famous three laws of robotics. In part 2, the book becomes more technical, although it is still within the comprehension of most laypeople. This section discusses how computers might be designed, constructed, and programmed to allow them to recognize, express, and even <I>have</I> emotions. This book is a solid scientific introduction to a subject that seems like a doorway into science fiction.} {The latest scientific findings indicate that emotions play an essential role in decision making, perception, learning, and more--that is, they influence the very mechanisms of rational thinking. Not only too much, but too little emotion can impair decision making. According to Rosalind Picard, if we want computers to be genuinely intelligent and to interact naturally with us, we must give computers the ability to recognize, understand, even to have and express emotions.<br /> <br /> Part 1 of this book provides the intellectual framework for affective computing. It includes background on human emotions, requirements for emotionally intelligent computers, applications of affective computing, and moral and social questions raised by the technology. Part 2 discusses the design and construction of affective computers. Although this material is more technical than that in Part 1, the author has kept it less technical than typical scientific publications in order to make it accessible to newcomers. Topics in Part 2 include signal-based representations of emotions, human affect recognition as a pattern recognition and learning problem, recent and ongoing efforts to build models of emotion for synthesizing emotions in computers, and the new application area of affective wearable computers.}\",\n",
       " \"Complexity, Self-Organization and Selection. \\t\\t\\tRecent work on self organization promises an explanation of complex order which is independent of adaptation. Self-organizing systems are complex systems of simple units, projecting order as a consequence of localized and generally nonlinear interactions between these units. Stuart Kauffman offers one variation on the theme of self-organization, offering what he calls a ``statistical mechanics'' for complex systems. This paper explores the explanatory strategies deployed in this ``statistical mechanics,'' initially focusing on the autonomy of statistical explanation as it applies in evolutionary settings and then turning to Kauffman's analysis. Two primary morals emerge as a consequence of this examination: first, the view that adaptation and self-organization should be seen as competing theories or models is misleading and simplistic; and second, while we need a synthesis treating self-organization and adaptation as geared toward different problems, at different levels of organization, and deploying different methods, we do not yet have such a synthesis.\",\n",
       " 'HADDOCK: a protein-protein docking approach based on biochemical or biophysical information.. The structure determination of protein-protein complexes is a rather tedious and lengthy process, by both NMR and X-ray crystallography. Several methods based on docking to study protein complexes have also been well developed over the past few years. Most of these approaches are not driven by experimental data but are based on a combination of energetics and shape complementarity. Here, we present an approach called HADDOCK (High Ambiguity Driven protein-protein Docking) that makes use of biochemical and/or biophysical interaction data such as chemical shift perturbation data resulting from NMR titration experiments or mutagenesis data. This information is introduced as Ambiguous Interaction Restraints (AIRs) to drive the docking process. An AIR is defined as an ambiguous distance between all residues shown to be involved in the interaction. The accuracy of our approach is demonstrated with three molecular complexes. For two of these complexes, for which both the complex and the free protein structures have been solved, NMR titration data were available. Mutagenesis data were used in the last example. In all cases, the best structures generated by HADDOCK, that is, the structures with the lowest intermolecular energies, were the closest to the published structure of the respective complexes (within 2.0 A backbone RMSD).',\n",
       " 'Guided Docking Approaches to Structure-Based Design and Screening. \\t\\t\\tWith the number of protein-ligand complexes available in the Protein Data Bank constantly growing, structure-based approaches to drug design and screening have become increasingly important. Alongside this explosion of structural information, a number of molecular docking methods have been developed over the last years with the aim of maximally exploiting all available structural and chemical information that can be derived from proteins, from ligands, and from protein-ligand complexes. In this respect, the term &#039;guided docking&#039; is introduced to refer to docking approaches that incorporate some degree of chemical information to actively guide the orientation of the ligand into the binding site. To reflect the focus on the use of chemical information, a classification scheme for guided docking approaches is proposed. In general terms, guided docking approaches can be divided into indirect and direct approaches. Indirect approaches incorporate chemical information implicitly, having an effect on scoring but not on orienting the ligand during sampling. In contrast, direct approaches incorporate chemical information explicitly, thus actively guiding the orientation of the ligand during sampling. Direct approaches can be further divided into protein-based, mapping-based, and ligandbased approaches to reflect the source used to derive the features capturing the chemical information inside the protein cavity. Within each category, a representative list of docking approaches is discussed. In view of the limitations of current scoring functions, it was generally found that making optimal use of chemical information represents an efficient knowledge-based strategy for improving binding affinity estimations, ligand binding-mode predictions, and virtual screening enrichments obtained from protein-ligand docking.',\n",
       " 'Kademlia: A peer-to-peer information system based on the XOR metric. We describe a peer-to-peer distributed hash table with provable consistency and performance in a fault-prone environment. Our system routes queries and locates nodes using a novel XOR-based metric topology that simplifies the algorithm and facilitates our proof. The topology has the property that every message exchanged conveys or reinforces useful contact information. The system exploits this information to send parallel, asynchronous query messages that tolerate node failures without imposing timeout delays on users.',\n",
       " 'Topology of biological networks and reliability of information processing. 10.1073/pnas.0509132102 Survival of living cells and organisms is largely based on highly reliable function of their regulatory networks. However, the elements of biological networks, e.g., regulatory genes in genetic networks or neurons in the nervous system, are far from being reliable dynamical elements. How can networks of unreliable elements perform reliably? We here address this question in networks of autonomous noisy elements with fluctuating timing and study the conditions for an overall system behavior being reproducible in the presence of such noise. We find a clear distinction between reliable and unreliable dynamical attractors. In the reliable case, synchrony is sustained in the network, whereas in the unreliable scenario, fluctuating timing of single elements can gradually desynchronize the system, leading to nonreproducible behavior. The likelihood of reliable dynamical attractors strongly depends on the underlying topology of a network. Comparing with the observed architectures of gene regulation networks, we find that those 3-node subgraphs that allow for reliable dynamics are also those that are more abundant in nature, suggesting that specific topologies of regulatory networks may provide a selective advantage in evolution through their resistance against noise.',\n",
       " 'The familiar stranger: anxiety, comfort, and play in public places. As humans we live and interact across a wildly diverse set of physical spaces. We each formulate our own personal meaning of place using a myriad of observable cues such as public-private, large-small, daytime-nighttime, loud-quiet, and crowded-empty. Not surprisingly, it is the people with which we share such spaces that dominate our perception of place. Sometimes these people are friends, family and colleagues. More often, and particularly in public urban spaces we inhabit, the individuals who affect us are ones that we repeatedly observe and yet do not directly interact with - our  Familiar Strangers . This paper explores our often ignored yet real relationships with Familiar Strangers. We describe several experiments and studies that led to designs for both a personal, body-worn, wireless device and a mobile phone based application that extend the Familiar Stranger relationship while respecting the delicate, yet important, constraints of our feelings and affinities with strangers in pubic places.',\n",
       " 'Tap: a semantic web platform. Activities such as Web Services and the Semantic Web are working to create a distributed web of machine understandable data. We address three important problems that need to be solved to realize this vision. We discuss the problem of scalable and deployable query systems and present a simple, but general query interface called GetData. We address the issue of creating global agreements on vocabularies and introduce the concept of Semantic Negotiation, a process by which two programs can...',\n",
       " 'The Semantic Web: The Roles of XML and RDF. The World Wide Web is possible because a set of widely established standards guarantees interoperability at various levels. Until now, the Web has been designed for direct human processing, but the next-generation Web, which Tim Berners-Lee and others call the ?Semantic Web,? aims at machine-processible information. The Semantic Web will enable intelligent services?such as information brokers, search agents, and information filters?which offer greater functionality and interoperability than current stand-alone services. The Semantic Web will only be possible once further levels of interoperability have been established. Standards must be defined not only for the syntactic form of documents, but also for their semantic content. Notable among recent W3C standardization efforts are XML/XML schema and RDF/RDF schema, which facilitate semantic interoperability. In this article, we explain the role of ontologies in the architecture of the Semantic Web. We then briefly summarize key elements of XML and RDF, showing why using XML as a tool for semantic interoperability will be ineffective in the long run. We argue that a further representation and inference layer is needed on top of the Web?s current layers, and to establish such a layer, we propose a general method for encoding ontology representation languages into RDF/RDF schema. We illustrate the extension method by applying it to Ontology Interchange Language (OIL), an ontology representation and inference language.',\n",
       " 'On the notion of inheritance. One of the most intriguing&mdash;and at the same time most problematic&mdash;notions in object-oriented programing is  inheritance . Inheritance is commonly regarded as the feature that distinguishes object-oriented  programming from other modern programming paradigms, but researchers rarely agree on its meaning and usage. Yet inheritance of often hailed as a solution to many problems hampering software development, and many of the alleged benefits of object-oriented programming, such as improved conceptual modeling and reusability, are largely credited to it. This article aims at a comprehensive understanding of inheritance, examining its usage, surveying its varieties, and presenting a simple taxonomy of mechanisms that can be seen as underlying different inheritance models.',\n",
       " 'A Maximum Entropy Approach to Natural Language Processing. The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.',\n",
       " 'Breaking and fixing the Needham-Schroeder public-key protocol using FDR. In this paper we analyse the well known Needham-Schroeder Public-Key Protocol using FDR, a refinement checker for CSP. We use FDR to discover an attack upon the protocol, which allows an intruder to impersonate another agent. We adapt the protocol, and then use FDR to show that the new protocol is secure, at least for a small system. Finally we prove a result which tells us that if this small system is secure, then so is a system of arbitrary size. 1 Introduction  In a distributed computer...',\n",
       " \"An extensive empirical study of feature selection metrics for text classification. Machine learning for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is essential to make the learning task efficient and more accurate. This paper presents an empirical comparison of twelve feature selection methods (e.g. Information Gain) evaluated on a benchmark of 229 text classification problem instances that were gathered from Reuters, TREC, OHSUMED, etc. The results are analyzed from multiple goal perspectives-accuracy, F-measure, precision, and recall-since each is appropriate in different situations. The results reveal that a new feature selection metric we call 'Bi-Normal Separation' (BNS), outperformed the others by a substantial margin in most situations. This margin widened in tasks with high class skew, which is rampant in text classification problems and is particularly challenging for induction algorithms. A new evaluation methodology is offered that focuses on the needs of the data mining practitioner faced with a single dataset who seeks to choose one (or a pair of) metrics that are most likely to yield the best performance. From this perspective, BNS was the top single choice for all goals except precision, for which Information Gain yielded the best result most often. This analysis also revealed, for example, that Information Gain and Chi-Squared have correlated failures, and so they work poorly together. When choosing optimal pairs of metrics for each of the four performance goals, BNS is consistently a member of the pair---e.g., for greatest recall, the pair BNS + F1-measure yielded the best performance on the greatest number of tasks by a considerable margin.\",\n",
       " 'Probabilistic Boolean networks: a rule-based uncertainty model  for gene regulatory networks. Motivation: Our goal is to construct a model for genetic regulatory networks such that the model class: (i) incorporates rule-based dependencies between genes; (ii) allows the systematic study of global network dynamics; (iii) is able to cope with uncertainty, both in the data and the model selection; and (iv) permits the quantification of the relative influence and sensitivity   of genes in their interactions with other genes.  Results: We introduce Probabilistic Boolean Networks (PBN) that   share the appealing rule-based properties of Boolean networks, but are robust in the face of uncertainty. We show how the dynamics of these networks can be studied in the probabilistic context of Markov chains, with standard Boolean networks being special cases. Then, we discuss the relationship between PBNs and Bayesian networks--a family of graphical models that explicitly represent probabilistic relationships between variables. We show how probabilistic dependencies between a gene and its parent genes, constituting the basic building blocks of Bayesian networks, can be obtained from PBNs. Finally, we present methods for quantifying the influence of genes on other genes, within the context of PBNs. Examples illustrating the above concepts are presented throughout the paper.  Contact: is@ieee.org 10.1093/bioinformatics/18.2.261',\n",
       " 'Neuroethics: the practical and the philosophical.  In comparison with the ethical issues surrounding molecular genetics, there has been little public awareness of the ethical implications of neuroscience. Yet recent progress in cognitive neuroscience raises a host of ethical issues of at least comparable importance. Some are of a practical nature, concerning the applications of neurotechnology and their likely implications for individuals and society. Others are more philosophical, concerning the way we think about ourselves as persons, moral agents and spiritual beings. This article reviews key examples of each type of issue, including the relevant advances in science and technology and their accompanying social and philosophical problems.',\n",
       " 'Simultaneous determination of protein structure and dynamics. We present a protocol for the experimental determination of ensembles of protein conformations that represent simultaneously the native structure and its associated dynamics. The procedure combines the strengths of nuclear magnetic resonance spectroscopy—for obtaining experimental information at the atomic level about the structural and dynamical features of proteins—with the ability of molecular dynamics simulations to explore a wide range of protein conformations. We illustrate the method for human ubiquitin in solution and find that there is considerable conformational heterogeneity throughout the protein structure. The interior atoms of the protein are tightly packed in each individual conformation that contributes to the ensemble but their overall behaviour can be described as having a significant degree of liquid-like character. The protocol is completely general and should lead to significant advances in our ability to understand and utilize the structures of native proteins.',\n",
       " 'Studying cooperation and conflict between authors with <i>history flow</i> visualizations. The Internet has fostered an unconventional and powerful style of collaboration: \"wiki\" web sites, where every visitor has the power to become an editor. In this paper we investigate the dynamics of Wikipedia, a prominent, thriving wiki. We make three contributions. First, we introduce a new exploratory data analysis tool, the  history flow  visualization, which is effective in revealing patterns within the wiki context and which we believe will be useful in other collaborative situations as well. Second, we discuss several collaboration patterns highlighted by this visualization tool and corroborate them with statistical analysis. Third, we discuss the implications of these patterns for the design and governance of online collaborative social spaces. We focus on the relevance of authorship, the value of community surveillance in ameliorating antisocial behavior, and how authors with competing perspectives negotiate their differences.',\n",
       " 'Self-assembling hypertexts, weblogs, and wikis. Although most theory and research in the hypertext community has been directed toward systems and implementations with fairly conventional patterns of authorship, hypertext as it has evolved on the Internet contains a number of stranger species: Web logs (or \"blogs\") that consist largely of citations or pointers to other Web content; reader-writeable text spaces sometimes called \"Wikis\"; and in spaces outside the Web, shared writing environments like MUDs and MOOs. This panel brings together several writer/designers who have experience in one or more of these areas. The panelists will consider how open-form and self-assembling texts fit and stretch the hypertext paradigm, and what contribution these writing practices might make to the future of writing on the Net.',\n",
       " 'Asynchronous collaborative writing through annotations. Annotation is central to iterative reviewing and revising activities in asynchronous collaborative writing. Currently most digital annotation models and systems assume static context information and provide far less functionality than physical annotations. We extend prior annotation research by Marshall and Cadiz and design an activity-oriented annotation model to mimic the rich functionality of physical annotations for an enhanced collaborative writing process. In this model, we define an annotation life cycle and support annotation version control. We implement a collaborative writing system that supports improved in-situ communication and cross-role feedback based on our annotation model.',\n",
       " 'Principles of docking: An overview of search algorithms and a guide to scoring functions.. The docking field has come of age. The time is ripe to present the principles of docking, reviewing the current state of the field. Two reasons are largely responsible for the maturity of the computational docking area. First, the early optimism that the very presence of the \"correct\" native conformation within the list of predicted docked conformations signals a near solution to the docking problem, has been replaced by the stark realization of the extreme difficulty of the next scoring/ranking step. Second, in the last couple of years more realistic approaches to handling molecular flexibility in docking schemes have emerged. As in folding, these derive from concepts abstracted from statistical mechanics, namely, populations. Docking and folding are interrelated. From the purely physical standpoint, binding and folding are analogous processes, with similar underlying principles. Computationally, the tools developed for docking will be tremendously useful for folding. For large, multidomain proteins, domain docking is probably the only rational way, mimicking the hierarchical nature of protein folding. The complexity of the problem is huge. Here we divide the computational docking problem into its two separate components. As in folding, solving the docking problem involves efficient search (and matching) algorithms, which cover the relevant conformational space, and selective scoring functions, which are both efficient and effectively discriminate between native and non-native solutions. It is universally recognized that docking of drugs is immensely important. However, protein-protein docking is equally so, relating to recognition, cellular pathways, and macromolecular assemblies. Proteins function when they are bound to other molecules. Consequently, we present the review from both the computational and the biological points of view. Although large, it covers only partially the extensive body of literature, relating to small (drug) and to large protein-protein molecule docking, to rigid and to flexible. Unfortunately, when reviewing these, a major difficulty in assessing the results is the non-uniformity in the formats in which they are presented in the literature. Consequently, we further propose a way to rectify it here.',\n",
       " 'Automated docking using a Lamarckian genetic algorithm and an empirical binding free energy function. A novel and robust automated docking method that predicts the bound conformations of flexible ligands to macromolecular targets has been developed and tested, in combination with a new scoring function that estimates the free energy change upon binding. Interestingly, this method applies a Lamarckian model of genetics, in which environmental adaptations of an individual\\'s phenotype are reverse transcribed into its genotype and become heritable traits (<I >sic</I >). We consider three search methods, Monte Carlo simulated annealing, a traditional genetic algorithm, and the Lamarckian genetic algorithm, and compare their performance in dockings of seven protein-ligand test systems having known three-dimensional structure. We show that both the traditional and Lamarckian genetic algorithms can handle ligands with more degrees of freedom than the simulated annealing method used in earlier versions of A<FONT SIZE=\"-1\" >UTO</FONT >D<FONT SIZE=\"-1\" >OCK</FONT >, and that the Lamarckian genetic algorithm is the most efficient, reliable, and successful of the three. The empirical free energy function was calibrated using a set of 30 structurally known protein-ligand complexes with experimentally determined binding constants. Linear regression analysis of the observed binding constants in terms of a wide variety of structure-derived molecular properties was performed. The final model had a residual standard error of 9.11 kJ mol<SUP >-1</SUP > (2.177 kcal mol<SUP >-1</SUP >) and was chosen as the new energy function. The new search methods and empirical free energy function are available in A<FONT SIZE=\"-1\" >UTO</FONT >D<FONT SIZE=\"-1\" >OCK</FONT >, version 3.0.&nbsp;&nbsp;&nbsp;&copy; 1998 John Wiley &amp; Sons, Inc.&nbsp;&nbsp;&nbsp;J Comput Chem 19: 1639-1662, 1998',\n",
       " 'Blogging as social activity, or, would you let 900 million people read your diary?. \"Blogging\" is a Web-based form of communication that is rapidly becoming mainstream. In this paper, we report the results of an ethnographic study of blogging, focusing on blogs written by individuals or small groups, with limited audiences. We discuss motivations for blogging, the quality of social interactivity that characterized the blogs we studied, and relationships to the blogger’s audience. We consider the way bloggers related to the known audience of their personal social networks as well as the wider “blogosphere” of unknown readers. We then make design recommendations for blogging software based on these findings.',\n",
       " 'Case-based reasoning: foundational issues, methodological variations, and system approaches. Case-based reasoning is a recent approach to problem solving and learning that has got a lot of attention over the last few years. Originating in the US, the basic idea and underlying theories have spread to other continents, and we are now within a period of highly active research in case-based reasoning in Europe, as well. This paper gives an overview of the foundational issues related to case-based reasoning, describes some of the leading methodological approaches within the field, and exemplifies the current state through pointers to some systems. Initially, a general framework is defined, to which the subsequent descriptions and discussions will refer. The framework is influenced by recent methodologies for knowledge level descriptions of intelligent systems. The methods for case retrieval, reuse, solution testing, and learning are summarized, and their actual realization is discussed in the light of a few example systems that represent different CBR approaches. We also discuss the role of case-based methods as one type of reasoning and learning method within an integrated system architecture. 1.',\n",
       " 'The location stack: A layered model for location in ubiquitous computing. Based on five design principles extracted from a survey of location systems, we present the Location Stack, a layered software engineering model for location in ubiquitous computing. Our model is similar in spirit to the seven-layer Open System Interconnect (OSI) model for computer networks. We map two existing ubiquitous computing systems to the model to illustrate the leverage the Location Stack provides. By encouraging system designers to think of their applications in this way, we hope to drive location-based computing toward a common vocabulary and standard infrastructure, permitting members of the ubiquitous computing community to easily evaluate and build on each other’s work.',\n",
       " 'Location Systems for Ubiquitous Computing. To serve us well, emerging mobile computing applications will need to know the physical location of things so that they can record them and report them to us: What lab bench was I standing by when I prepared these tissue samples? How should our search-and-rescue team move to quickly locate all the avalanche victims? Can I automatically display this stock devaluation chart on the large screen I am standing next to? Researchers are working to meet these and similar needs by developing systems and technologies that automatically locate people, equipment, and other tangibles. Indeed, many systems over the years have addressed the problem of automatic location sensing. Because each approach solves a slightly different problem or supports different applications, they vary in many parameters, such as the physical phenomena used for location determination, the form factor of the sensing apparatus, power requirements, infrastructure versus portable elements, and resolution in time and space. To make sense of this domain, we have developed ataxonomy to help developers of location-aware applications better evaluate their options when choosing a location-sensing system. The taxonomy may also aid researchers in identifying opportunities for new location-sensing techniques.',\n",
       " 'The challenges of user-centered design and evaluation for infrastructure. Infrastructure software comprises code libraries or runtime processes that support the development or operation of application software. A particular infrastructure system may support certain styles of application, and may even determine the features of applications built using it. This poses a challenge: although we have good techniques for designing and evaluating interactive applications, our techniques for designing and evaluating infrastructure intended to support these applications are much less well formed. In this paper, we reflect on case studies of two infrastructure systems for interactive applications. We look at how traditional user-centered techniques, while appropriate for application design and evaluation, fail to properly support infrastructure design and evaluation. We present a set of lessons from our experience, and conclude with suggestions for better user-centered design and evaluation of infrastructure software.',\n",
       " 'WebBase: a repository of Web pages. In this paper, we study the problem of constructing and maintaining a large shared repository of web pages. We discuss the unique characteristics of such a repository, propose an architecture, and identify its functional modules. We focus on the storage manager module, and illustrate how traditional techniques for storage and indexing can be tailored to meet the requirements of a web repository. To evaluate design alternatives, we also present experimental results from a prototype repository...',\n",
       " 'Indexing by Latent Semantic Analysis. A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (&#034;semantic structure&#034;) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. Deerwester - 1 - 1. Introduction We describe here a new approach to automatic indexing and retrieval. It is designed to overcome a fundamental problem that plagu...',\n",
       " 'Haploview: analysis and visualization of LD and haplotype maps. Summary: Research over the last few years has revealed significant haplotype structure in the human genome. The characterization of these patterns, particularly in the context of medical genetic association studies, is becoming a routine research activity. Haploview is a software package that provides computation of linkage disequilibrium statistics and population haplotype patterns from primary genotype data in a visually appealing and interactive interface.               Availability:                http://www.broad.mit.edu/mpg/haploview/                           Contact:                jcbarret@broad.mit.edu',\n",
       " 'Gene name ambiguity of eukaryotic nomenclatures. Motivation: With more and more scientific literature published online, the effective management and reuse of this knowledge has become problematic. Natural language processing (NLP) may be a potential solution by extracting, structuring and organizing biomedical information in online literature in a timely manner. One essential task is to recognize and identify genomic entities in text. â\\x80\\x98Recognitionâ\\x80\\x99 can be accomplished using pattern matching and machine learning. But for â\\x80\\x98identificationâ\\x80\\x99 these techniques are not adequate. In order to identify genomic entities, NLP needs a comprehensive resource that specifies and classifies genomic entities as they occur in text and that associates them with normalized terms and also unique identifiers so that the extracted entities are well defined. Online organism databases are an excellent resource to create such a lexical resource. However, gene name ambiguity is a serious problem because it affects the appropriate identification of gene entities. In this paper, we explore the extent of the problem and suggest ways to address it.               Results: We obtained gene information from 21 organisms and quantified naming ambiguities within species, across species, with English words and with medical terms. When the case (of letters) was retained, official symbols displayed negligible intra-species ambiguity (0.02%) and modest ambiguities with general English words (0.57%) and medical terms (1.01%). In contrast, the across-species ambiguity was high (14.20%). The inclusion of gene synonyms increased intra-species ambiguity substantially and full names contributed greatly to gene-medical-term ambiguity. A comprehensive lexical resource that covers gene information for the 21 organisms was then created and used to identify gene names by using a straightforward string matching program to process 45â\\x80\\x89000 abstracts associated with the mouse model organism while ignoring case and gene names that were also English words. We found that 85.1% of correctly retrieved mouse genes were ambiguous with other gene names. When gene names that were also English words were included, 233% additional â\\x80\\x98geneâ\\x80\\x99 instances were retrieved, most of which were false positives. We also found that authors prefer to use synonyms (74.7%) to official symbols (17.7%) or full names (7.6%) in their publications.               Contact:                lifeng.chen@dbmi.columbia.edu',\n",
       " \"Outcome signature genes in breast cancer: is there a unique set?. Motivation: Predicting the metastatic potential of primary malignant tissues has direct bearing on the choice of therapy. Several microarray studies yielded gene sets whose expression profiles successfully predicted survival. Nevertheless, the overlap between these gene sets is almost zero. Such small overlaps were observed also in other complex diseases, and the variables that could account for the differences had evoked a wide interest. One of the main open questions in this context is whether the disparity can be attributed only to trivial reasons such as different technologies, different patients and different types of analyses.               Results: To answer this question, we concentrated on a single breast cancer dataset, and analyzed it by a single method, the one which was used by van't Veer et al. to produce a set of outcome-predictive genes. We showed that, in fact, the resulting set of genes is not unique; it is strongly influenced by the subset of patients used for gene selection. Many equally predictive lists could have been produced from the same analysis. Three main properties of the data explain this sensitivity: (1) many genes are correlated with survival; (2) the differences between these correlations are small; (3) the correlations fluctuate strongly when measured over different subsets of patients. A possible biological explanation for these properties is discussed.               Contact:                eytan.domany@weizmann.ac.il                           Supplementary information:                http://www.weizmann.ac.il/physics/complex/compphys/downloads/liate/\",\n",
       " 'Designing mobile technologies to support co-present collaboration. Mobile technologies offer new opportunities for children&#8217;s educational activities in that they can be used across different locations and times. Naturally, some instances of mobile technology use will necessitate, or be enhanced by, the sharing of information. Social interaction is important for sharing ideas, constructing and shaping understanding and fundamental for educational development. However the physical size of mobile technologies presents interesting challenges when designing for collaborative activities. When designing mobile technologies the importance of collaborative tasks has often been overlooked. The replacement of low-tech artefacts with digital devices, for supporting multiple users, can inhibit the shareability of information. We present three projects where mobile technologies have been used as part of a larger mixed reality experience. Novel technologies were used to support children&#8217;s collaborative activities in storytelling, an adventure game and during an outdoor field trip. Interaction with mobile devices within each project is reviewed and the authors highlight important considerations for their design and use across multiple contexts.',\n",
       " 'Conservation Biogeography: assessment and prospect. There is general agreement among scientists that biodiversity is under assault on a global basis and that species are being lost at a greatly enhanced rate. This article examines the role played by biogeographical science in the emergence of conservation guidance and makes the case for the recognition of Conservation Biogeography as a key subfield of conservation biology delimited as: the application of biogeographical principles, theories, and analyses, being those concerned with the distributional dynamics of taxa individually and collectively, to problems concerning the conservation of biodiversity. Conservation biogeography thus encompasses both a substantial body of theory and analysis, and some of the most prominent planning frameworks used in conservation. Considerable advances in conservation guidelines have been made over the last few decades by applying biogeographical methods and principles. Herein we provide a critical review focussed on the sensitivity to assumptions inherent in the applications we examine. In particular, we focus on four inter-related factors: (i) scale dependency (both spatial and temporal); (ii) inadequacies in taxonomic and distributional data (the so-called Linnean and Wallacean shortfalls); (iii) effects of model structure and parameterisation; and (iv) inadequacies of theory. These generic problems are illustrated by reference to studies ranging from the application of historical biogeography, through island biogeography, and complementarity analyses to bioclimatic envelope modelling. There is a great deal of uncertainty inherent in predictive analyses in conservation biogeography and this area in particular presents considerable challenges. ÊÊProtected area planning frameworks and their resulting map outputs are amongst the most powerful and influential applications within conservation biogeography, and at the global scale are characterised by the production, by a small number of prominent NGOs, of bespoke schemes, which serve both to mobilise funds and channel efforts in a highly targeted fashion. We provide a simple typology of protected area planning frameworks, with particular reference to the global scale, and provide a brief critique of some of their strengths and weaknesses. Finally, we discuss the importance, especially at regional scales, of developing more responsive analyses and models that integrate pattern (the compositionalist approach) and processes (the functionalist approach) such as range collapse and climate change, again noting the sensitivity of outcomes to starting assumptions. We make the case for the greater engagement of the biogeographical community in a programme of evaluation and refinement of all such schemes to test their robustness and their sensitivity to alternative conservation priorities and goals.',\n",
       " 'Neural Topography and Content of Movement Representations. We have used implicit motor imagery to investigate the neural correlates of motor planning independently from actual movements. Subjects were presented with drawings of left or right hands and asked to judge the hand laterality, regardless of the stimulus rotation from its upright orientation. We paired this task with a visual imagery control task, in which subjects were presented with typographical characters and asked to report whether they saw a canonical letter or its mirror image, regardless of its rotation. We measured neurovascular activity with fast event-related fMRI, distinguishing responses parametrically related to motor imagery from responses evoked by visual imagery and other task-related phenomena. By quantifying behavioral and neurovascular correlates of imagery on a trial-by-trial basis, we could discriminate between stimulus-related, mental rotation-related, and response-related neural activity. We found that specific portions of the posterior parietal and precentral cortex increased their activity as a function of mental rotation only during the motor imagery task. Within these regions, the parietal cortex was visually responsive, whereas the dorsal precentral cortex was not. Response- but not rotation-related activity was found around the left central sulcus (putative primary motor cortex) during both imagery tasks. Our study provides novel evidence on the topography and content of movement representations in the human brain. During intended action, the posterior parietal cortex combines somatosensory and visuomotor information, whereas the dorsal premotor cortex generates the actual motor plan, and the primary motor cortex deals with movement execution. We discuss the relevance of these results in the context of current models of action planning.',\n",
       " 'Towards Flexible Teamwork. Many AI researchers are today striving to build agent teams for complex, dynamic multi-agent domains, with intended applications in arenas such as education, training, entertainment, information integration, and collective robotics. Unfortunately, uncertainties in these complex, dynamic domains obstruct coherent teamwork. In particular, team members often encounter differing, incomplete, and possibly inconsistent views of their environment. Furthermore, team members can unexpectedly fail in...',\n",
       " 'An Eulerian path approach to DNA fragment assembly. For the last 20 years, fragment assembly in DNA sequencing followed the â\\x80\\x9coverlapâ\\x80\\x93layoutâ\\x80\\x93consensusâ\\x80\\x9d paradigm that is used in all currently available assembly tools. Although this approach proved useful in assembling clones, it faces difficulties in genomic shotgun assembly. We abandon the classical â\\x80\\x9coverlapâ\\x80\\x93layoutâ\\x80\\x93consensusâ\\x80\\x9d approach in favor of a new euler algorithm that, for the first time, resolves the 20-year-old â\\x80\\x9crepeat problemâ\\x80\\x9d in fragment assembly. Our main result is the reduction of the fragment assembly to a variation of the classical Eulerian path problem that allows one to generate accurate solutions of large-scale sequencing problems. euler, in contrast to the celera assembler, does not mask such repeats but uses them instead as a powerful fragment assembly tool.',\n",
       " 'Spectral learning. We present a simple, easily implemented spectral learning algorithm that applies equally whether we have no supervisory information, pairwise link constraints, or labeled examples. In the unsupervised case, it performs consistently with other spectral clustering algorithms. In the supervised case, our approach achieves high accuracy on the categorization of thousands of documents given only a few dozen labeled training documents for the 20 Newsgroups data set. Furthermore, its classification accuracy increases with the addition of unlabeled documents, demonstrating effective use of unlabeled data.',\n",
       " 'Human Category Learning . Abstract Much recent evidence suggests some dramatic differences in the way people learn perceptual categories, depending on exactly how the categories were constructed. Four different kinds of category-learning tasks are currently popular--rule-based tasks, information-integration tasks, prototype distortion tasks, and the weather prediction task. The cognitive, neuropsychological, and neuroimaging results obtained using these four tasks are qualitatively different. Success in rule-based (explicit reasoning) tasks depends on frontal-striatal circuits and requires working memory and executive attention. Success in information-integration tasks requires a form of procedural learning and is sensitive to the nature and timing of feedback. Prototype distortion tasks induce perceptual (visual cortical) learning. A variety of different strategies can lead to success in the weather prediction task. Collectively, results from these four tasks provide strong evidence that human category learning is mediated by multiple, qualitatively distinct systems.',\n",
       " 'Brain Organization for Music Processing. Research on how the brain processes music is emerging as a rich and stimulating area of investigation of perception, memory, emotion, and performance. Results emanating from both lesion studies and neuroimaging techniques are reviewed and integrated for each of these musical functions. We focus our attention on the common core of musical abilities shared by musicians and nonmusicians alike. Hence, the effect of musical training on brain plasticity is examined in a separate section, after a review of the available data regarding music playing and reading skills that are typically cultivated by musicians. Finally, we address a currently debated issue regarding the putative existence of music-specific neural networks. Unfortunately, due to scarcity of research on the macrostructure of music organization and on cultural differences, the musical material under focus is at the level of the musical phrase, as typically used in Western popular music.',\n",
       " 'Integrating ethics and science in the International HapMap Project.. Genomics resources that use samples from identified populations raise scientific, social and ethical issues that are, in many ways, inextricably linked. Scientific decisions about which populations to sample to produce the HapMap, an international genetic variation resource, have raised questions about the relationships between the social identities used to recruit participants and the biological findings of studies that will use the HapMap. The sometimes problematic implications of those complex relationships have led to questions about how to conduct genetic variation research that uses identified populations in an ethical way, including how to involve members of a population in evaluating the risks and benefits posed for everyone who shares that identity. The ways in which these issues are linked is increasingly drawing the scientific and ethical spheres of genomics research closer together.',\n",
       " 'De novo repeat classification and fragment assembly.. Repetitive sequences make up a significant fraction of almost any genome, and an important and still open question in bioinformatics is how to represent all repeats in DNA sequences. We propose a new approach to repeat classification that represents all repeats in a genome as a mosaic of sub-repeats. Our key algorithmic idea also leads to new approaches to multiple alignment and fragment assembly. In particular, we show that our FragmentGluer assembler improves on Phrap and ARACHNE in assembly of BACs and bacterial genomes.',\n",
       " 'Interspersed repeats and other mementos of transposable elements in mammalian genomes.. The bulk of the human genome is ultimately derived from transposable elements. Observations in the past year lead to some new and surprising ideas on functions and consequences of these elements and their remnants in our genome. The many new examples of human genes derived from single transposon insertions highlight the large contribution of selfish DNA to genomic evolution.',\n",
       " 'A measure of betweenness centrality based on random walks. Betweenness is a measure of the centrality of a node in a network, and is normally calculated as the fraction of shortest paths between node pairs that pass through the node of interest. Betweenness is, in some sense, a measure of the influence a node has over the spread of information through the network. By counting only shortest paths, however, the conventional definition implicitly assumes that information spreads only along those shortest paths. Here, we propose a betweenness measure that relaxes this assumption, including contributions from essentially all paths between nodes, not just the shortest, although it still gives more weight to short paths. The measure is based on random walks, counting how often a node is traversed by a random walk between two other nodes. We show how our measure can be calculated using matrix methods, and give some examples of its application to particular networks. (C) 2004 Elsevier B.V. All rights reserved.',\n",
       " 'Mixing patterns in networks. We study assortative mixing in networks, the tendency for vertices in networks to be connected to other vertices that are like ͑or unlike͒ them in some way. We consider mixing according to discrete characteristics such as language or race in social networks and scalar characteristics such as age. As a special example of the latter we consider mixing according to vertex degree, i.e., according to the number of connections vertices have to other vertices: do gregarious people tend to associate with other gregarious people? We propose a number of measures of assortative mixing appropriate to the various mixing types, and apply them to a variety of real-world networks, showing that assortative mixing is a pervasive phenomenon found in many networks. We also propose several models of assortatively mixed networks, both analytic ones based on generating function methods, and numerical ones based on Monte Carlo graph generation techniques. We use these models to probe the properties of networks as their level of assortativity is varied. In the particular case of mixing by degree, we ﬁnd strong variation with assortativity in the connectivity of the network and in the resilience of the network to the removal of vertices.',\n",
       " 'Spectral biclustering of microarray data: coclustering genes and conditions.. Global analyses of RNA expression levels are useful for classifying genes and overall phenotypes. Often these classification problems are linked, and one wants to find \"marker genes\" that are differentially expressed in particular sets of \"conditions.\" We have developed a method that simultaneously clusters genes and conditions, finding distinctive \"checkerboard\" patterns in matrices of gene expression data, if they exist. In a cancer context, these checkerboards correspond to genes that are markedly up- or downregulated in patients with particular types of tumors. Our method, spectral biclustering, is based on the observation that checkerboard structures in matrices of expression data can be found in eigenvectors corresponding to characteristic expression patterns across genes or conditions. In addition, these eigenvectors can be readily identified by commonly used linear algebra approaches, in particular the singular value decomposition (SVD), coupled with closely integrated normalization steps. We present a number of variants of the approach, depending on whether the normalization over genes and conditions is done independently or in a coupled fashion. We then apply spectral biclustering to a selection of publicly available cancer expression data sets, and examine the degree to which the approach is able to identify checkerboard structures. Furthermore, we compare the performance of our biclustering methods against a number of reasonable benchmarks (e.g., direct application of SVD or normalized cuts to raw data).',\n",
       " 'Comparative genome sequencing of Drosophila pseudoobscura: chromosomal, gene, and cis-element evolution.. We have sequenced the genome of a second Drosophila species, Drosophila pseudoobscura, and compared this to the genome sequence of Drosophila melanogaster, a primary model organism. Throughout evolution the vast majority of Drosophila genes have remained on the same chromosome arm, but within each arm gene order has been extensively reshuffled, leading to a minimum of 921 syntenic blocks shared between the species. A repetitive sequence is found in the D. pseudoobscura genome at many junctions between adjacent syntenic blocks. Analysis of this novel repetitive element family suggests that recombination between offset elements may have given rise to many paracentric inversions, thereby contributing to the shuffling of gene order in the D. pseudoobscura lineage. Based on sequence similarity and synteny, 10,516 putative orthologs have been identified as a core gene set conserved over 25-55 million years (Myr) since the pseudoobscura/melanogaster divergence. Genes expressed in the testes had higher amino acid sequence divergence than the genome-wide average, consistent with the rapid evolution of sex-specific proteins. Cis-regulatory sequences are more conserved than random and nearby sequences between the species--but the difference is slight, suggesting that the evolution of cis-regulatory elements is flexible. Overall, a pattern of repeat-mediated chromosomal rearrangement, and high coadaptation of both male genes and cis-regulatory sequences emerges as important themes of genome divergence between these species of Drosophila.',\n",
       " 'The modular architecture of proteinâ\\x80\\x93protein binding interfaces. Proteinâ\\x80\\x93protein interactions are essential for life. Yet, our understanding of the general principles governing binding is not complete. In the present study, we show that the interface between proteins is built in a modular fashion; each module is comprised of a number of closely interacting residues, with few interactions between the modules. The boundaries between modules are defined by clustering the contact map of the interface. We show that mutations in one module do not affect residues located in a neighboring module. As a result, the structural and energetic consequences of the deletion of entire modules are surprisingly small. To the contrary, within their module, mutations cause complex energetic and structural consequences. Experimentally, this phenomenon is shown on the interaction between TEM1-Î²lactamase and Î²-lactamase inhibitor protein (BLIP) by using multiple-mutant analysis and x-ray crystallography. Replacing an entire module of five interface residues with Ala created a large cavity in the interface, with no effect on the detailed structure of the remaining interface. The modular architecture of binding sites, which resembles human engineering design, greatly simplifies the design of new protein interactions and provides a feasible view of how these interactions evolved.',\n",
       " 'Concept Decompositions for Large Sparse Text Data Using Clustering. Unlabeled document collections are becoming increasingly common and available; mining such data sets represents a major contemporary challenge. Using words as features, text documents are often represented as high-dimensional and sparse vectors–a few thousand dimensions and a sparsity of 95 to 99% is typical. In this paper, we study a certain spherical k-means algorithm for clustering such document vectors. The algorithm outputs k disjoint clusters each with a concept vector that is the centroid of the cluster normalized to have unit Euclidean norm. As our first contribution, we empirically demonstrate that, owing to the high-dimensionality and sparsity of the text data, the clusters produced by the algorithm have a certain “fractal-like” and “self-similar” behavior. As our second contribution, we introduce concept decompositions to approximate the matrix of document vectors; these decompositions are obtained by taking the least-squares approximation onto the linear subspace spanned by all the concept vectors. We empirically establish that the approximation errors of the concept decompositions are close to the best possible, namely, to truncated singular value decompositions. As our third contribution, we show that the concept vectors are localized in the word space, are sparse, and tend towards orthonormality. In contrast, the singular vectors are global in the word space and are dense. Nonetheless, we observe the surprising fact that the linear subspaces spanned by the concept vectors and the leading singular vectors are quite close in the sense of small principal angles between them. In conclusion, the concept vectors produced by the spherical k-means algorithm constitute a powerful sparse and localized “basis” for text data sets.',\n",
       " 'Wizard of Oz prototyping of computer vision based action games for children. This paper describes the use of the Wizard of Oz (WOz) method in the design of computer vision based action games controlled with body movements. A WOz study was carried out with 34 children of ages 7 to 9 in order to find out the most intuitive movements for game controls and to evaluate the relationship between avatar and player actions. Our study extends the previous Wizard of Oz studies by showing that WOz prototyping of perceptive action games is feasible despite the delay caused by the wizard. The results also show that distinctive movement categories and gesture patterns can be found by observing the children playing games controlled by a human wizard. The approach minimizes the need for fully functional prototypes in the early stages of the design and provides video material for testing and developing computer vision algorithms, as well as guidelines for animating the game character.',\n",
       " 'Functional imaging with cellular resolution reveals precise micro-architecture in visual cortex.. Neurons in the cerebral cortex are organized into anatomical columns, with ensembles of cells arranged from the surface to the white matter. Within a column, neurons often share functional properties, such as selectivity for stimulus orientation; columns with distinct properties, such as different preferred orientations, tile the cortical surface in orderly patterns. This functional architecture was discovered with the relatively sparse sampling of microelectrode recordings. Optical imaging of membrane voltage or metabolic activity elucidated the overall geometry of functional maps, but is averaged over many cells (resolution >100 microm). Consequently, the purity of functional domains and the precision of the borders between them could not be resolved. Here, we labelled thousands of neurons of the visual cortex with a calcium-sensitive indicator in vivo. We then imaged the activity of neuronal populations at single-cell resolution with two-photon microscopy up to a depth of 400 microm. In rat primary visual cortex, neurons had robust orientation selectivity but there was no discernible local structure; neighbouring neurons often responded to different orientations. In area 18 of cat visual cortex, functional maps were organized at a fine scale. Neurons with opposite preferences for stimulus direction were segregated with extraordinary spatial precision in three dimensions, with columnar borders one to two cells wide. These results indicate that cortical maps can be built with single-cell precision.',\n",
       " 'Evolutionary dynamics on graphs. Evolutionary dynamics have been traditionally studied in the context of homogeneous or spatially extended populations1, 2, 3, 4. Here we generalize population structure by arranging individuals on a graph. Each vertex represents an individual. The weighted edges denote reproductive rates which govern how often individuals place offspring into adjacent vertices. The homogeneous population, described by the Moran process3, is the special case of a fully connected graph with evenly weighted edges. Spatial structures are described by graphs where vertices are connected with their nearest neighbours. We also explore evolution on random and scale-free networks5, 6, 7. We determine the fixation probability of mutants, and characterize those graphs for which fixation behaviour is identical to that of a homogeneous population7. Furthermore, some graphs act as suppressors and others as amplifiers of selection. It is even possible to find graphs that guarantee the fixation of any advantageous mutant. We also study frequency-dependent selection and show that the outcome of evolutionary games can depend entirely on the structure of the underlying graph. Evolutionary graph theory has many fascinating applications ranging from ecology to multi-cellular organization and economics.',\n",
       " \"Emergence of scaling in random networks. Recently retired as head of the Global Alliance for Vaccines and Immunization (GAVI) secretariat and as a health advisor to leading global entities, Tore Godal is now a Special Advisor to the Norwegian Prime Minister. He is nevertheless continuing to fight for better global health, cogently articulating the needs of the world's poor and disadvantaged. He is a leading leprosy expert, ex-director of the world's premier agency for research and training in tropical diseases, instigator and prime mover of some global innovative public-private health sector partnerships, adept fund mobilizer, and advocate of the `let's get it done' school of leadership. Few individuals are, therefore, more experienced or better suited for such a crucial and much-needed role\",\n",
       " 'Test input generation with java PathFinder. We show how model checking and symbolic execution can be used to generate test inputs to achieve structural coverage of code that manipulates complex data structures. We focus on obtaining branch-coverage during unit testing of some of the core methods of the red-black tree implementation in the Java  TreeMap  library, using the Java PathFinder model checker. Three different test generation techniques will be introduced and compared, namely, straight model checking of the code, model checking used in a black-box fashion to generate all inputs up to a fixed size, and lastly, model checking used during white-box test input generation. The main contribution of this work is to show how efficient white-box test input generation can be done for code manipulating complex data, taking into account complex method preconditions.',\n",
       " 'Symbolic execution and program testing. This paper describes the symbolic execution of programs. Instead of supplying the normal inputs to a program (e.g. numbers) one supplies symbols representing arbitrary values. The execution proceeds as in a normal execution except that values may be symbolic formulas over the input symbols. The difficult, yet interesting issues arise during the symbolic execution of conditional branch type statements. A particular system called EFFIGY which provides symbolic execution for program testing and debugging is also described. It interpretively executes programs written in a simple PL/I style programming language. It includes many standard debugging features, the ability to manage and to prove things about symbolic expressions, a simple program testing manager, and a program verifier. A brief discussion of the relationship between symbolic execution and program proving is also included.',\n",
       " 'Generalized symbolic execution for model checking and testing. Modern software systems, which often are concurrent and manipulate complex data structures must be extremely reliable. We present a novel framework based on symbolic execution, for automated checking of such systems. We provide a two-fold generalization of traditional symbolic execution based approaches. First, we define a source to source translation to instrument a program, which enables standard model checkers to perform symbolic execution of the program. Second, we give a novel symbolic execution algorithm that handles dynamically allocated structures (e.g., lists and trees), method preconditions (e.g., acyclicity), data (e.g., integers and strings) and concurrency. The program instrumentation enables a model checker to automatically explore different program heap configurations and manipulate logical formulae on program data (using a decision procedure). We illustrate two applications of our framework: checking correctness of multi-threaded programs that take inputs from unbounded domains with complex structure and generation of non-isomorphic test inputs that satisfy a testing criterion. Our implementation for Java uses the Java PathFinder model checker.',\n",
       " 'Random Graph Models of Social Networks. A social network is a set of people or groups of people, &#034;actors&#034; in the jargon of the field, with some pattern of interactions or &#034;ties&#034; between them [1, 2]. Friendships among a group of individuals, business relationships between companies, and intermarriages between families are all examples of networks which have been studied in the past. Network analysis has a long history in sociology, the literature on the topic stretching back at least half a century to the pioneering work of Rapaport, Harary, and others in the 1940s and 1950s. Typically network studies in sociology have been data-oriented, involving empirical investigation of real-world networks followed, usually, by graph theoretical analysis often aimed at determining the centrality or influence of the various actors. More recently, following a surge in interest in network structure among mathematicians and physicists, partly as a result of research on the Internet and the World Wide Web, another body o...',\n",
       " 'Estimating mutual information.. We present two classes of improved estimators for mutual information $M(X,Y)$, from samples of random points distributed according to some joint probability density $\\\\mu(x,y)$. In contrast to conventional estimators based on binnings, they are based on entropy estimates from $k$-nearest neighbour distances. This means that they are data efficient (with $k=1$ we resolve structures down to the smallest possible scales), adaptive (the resolution is higher where data are more numerous), and have minimal bias. Indeed, the bias of the underlying entropy estimates is mainly due to non-uniformity of the density at the smallest resolved scale, giving typically systematic errors which scale as functions of $k/N$ for $N$ points. Numerically, we find that both families become {\\\\it exact} for independent distributions, i.e. the estimator $\\\\hat M(X,Y)$ vanishes (up to statistical fluctuations) if $\\\\mu(x,y) = \\\\mu(x) \\\\mu(y)$. This holds for all tested marginal distributions and for all dimensions of $x$ and $y$. In addition, we give estimators for redundancies between more than 2 random variables. We compare our algorithms in detail with existing algorithms. Finally, we demonstrate the usefulness of our estimators for assessing the actual independence of components obtained from independent component analysis (ICA), for improving ICA, and for estimating the reliability of blind source separation.',\n",
       " 'Why social networks are different from other types of networks.. We argue that social networks differ from most other types of networks, including technological and biological networks, in two important ways. First, they have non-trivial clustering or network transitivity, and second, they show positive correlations, also called assortative mixing, between the degrees of adjacent vertices. Social networks are often divided into groups or communities, and it has recently been suggested that this division could account for the observed clustering. We demonstrate that group structure in networks can also account for degree correlations. We show using a simple model that we should expect assortative mixing in such networks whenever there is variation in the sizes of the groups and that the predicted level of assortative mixing compares well with that observed in real-world networks.',\n",
       " 'Assortative mixing in networks.. A network is said to show assortative mixing if the nodes in the network that have many connections tend to be connected to other nodes with many connections. We define a measure of assortative mixing for networks and use it to show that social networks are often assortatively mixed, but that technological and biological networks tend to be disassortative. We propose a model of an assortative network, which we study both analytically and numerically. Within the framework of this model we find that assortative networks tend to percolate more easily than their disassortative counterparts and that they are also more robust to vertex removal.',\n",
       " 'Community structure in social and biological networks. 10.1073/pnas.122653799 A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well known—a collaboration network and a food web—and find that it detects significant and informative community divisions in both cases.',\n",
       " 'Scaling and percolation in the small-world network model.. In this paper we study the small-world network model of Watts and Strogatz, which mimics some aspects of the structure of networks of social interactions. We argue that there is one non-trivial length-scale in the model, analogous to the correlation length in other systems, which is well-defined in the limit of infinite system size and which diverges continuously as the randomness in the network tends to zero, giving a normal critical point in this limit. This length-scale governs the cross-over from large- to small-world behavior in the model, as well as the number of vertices in a neighborhood of given radius on the network. We derive the value of the single critical exponent controlling behavior in the critical region and the finite size scaling form for the average vertex-vertex distance on the network, and, using series expansion and Pade approximants, find an approximate analytic form for the scaling function. We calculate the effective dimension of small-world graphs and show that this dimension varies as a function of the length-scale on which it is measured, in a manner reminiscent of multifractals. We also study the problem of site percolation on small-world networks as a simple model of disease propagation, and derive an approximate expression for the percolation probability at which a giant component of connected vertices first forms (in epidemiological terms, the point at which an epidemic occurs). The typical cluster radius satisfies the expected finite size scaling form with a cluster size exponent close to that for a random graph. All our analytic results are confirmed by extensive numerical simulations of the model.',\n",
       " 'Are randomly grown graphs really random?. Recent work on the structure of social networks and the internet has focused attention on graphs with distributions of vertex degree that are significantly different from the Poisson degree distributions that have been widely studied in the past. In this paper we develop in detail the theory of random graphs with arbitrary degree distributions. In addition to simple undirected, unipartite graphs, we examine the properties of directed and bipartite graphs. Among other results, we derive exact expressions for the position of the phase transition at which a giant component first forms, the mean component size, the size of the giant component if there is one, the mean number of vertices a certain distance away from a randomly chosen vertex, and the average vertex-vertex distance within a graph. We apply our theory to some real-world graphs, including the world-wide web and collaboration graphs of scientists and Fortune 1000 company directors. We demonstrate that in some cases random graphs with appropriate distributions of vertex degree predict with surprising accuracy the behavior of the real world, while in others there is a measurable discrepancy between theory and reality, perhaps indicating the presence of additional social structure in the network that is not captured by the random graph.',\n",
       " 'Scientific collaboration networks. I. Network construction and fundamental results.. Using computer databases of scientific papers in physics, biomedical research, and computer science, we have constructed networks of collaboration between scientists in each of these disciplines. In these networks two scientists are considered connected if they have coauthored one or more papers together. We study a variety of statistical properties of our networks, including numbers of papers written by authors, numbers of authors per paper, numbers of collaborators that scientists have, existence and size of a giant component of connected scientists, and degree of clustering in the networks. We also highlight some apparent differences in collaboration patterns between the subjects studied. In the following paper, we study a number of measures of centrality and connectedness in the same networks.',\n",
       " 'Scientific collaboration networks. II. Shortest paths, weighted networks, and centrality.. Using computer databases of scientific papers in physics, biomedical research, and computer science, we have constructed networks of collaboration between scientists in each of these disciplines. In these networks two scientists are considered connected if they have coauthored one or more papers together. Here we study a variety of nonlocal statistics for these networks, such as typical distances between scientists through the network, and measures of centrality such as closeness and betweenness. We further argue that simple networks such as these cannot capture variation in the strength of collaborative ties and propose a measure of collaboration strength based on the number of papers coauthored by pairs of scientists, and the number of other scientists with whom they coauthored those papers.',\n",
       " 'Textpresso: An Ontology-Based Information Retrieval and Extraction System for Biological Literature. We have developed Textpresso, a new text-mining system for scientific literature whose capabilities go far beyond those of a simple keyword search engine. Textpresso&#39;s two major elements are a collection of the full text of scientific articles split into individual sentences, and the implementation of categories of terms for which a database of articles and individual sentences can be searched. The categories are classes of biological concepts (e.g., gene, allele, cell or cell group, phenotype, etc.) and classes that relate two objects (e.g., association, regulation, etc.) or describe one (e.g., biological process, etc.). Together they form a catalog of types of objects and concepts called an ontology. After this ontology is populated with terms, the whole corpus of articles and abstracts is marked up to identify terms of these categories. The current ontology comprises 33 categories of terms. A search engine enables the user to search for one or a combination of these tags and/or keywords within a sentence or document, and as the ontology allows word meaning to be queried, it is possible to formulate semantic queries. Full text access increases recall of biological data types from 45&#37; to 95&#37;. Extraction of particular biological facts, such as gene-gene interactions, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences; in searches for two uniquely named genes and an interaction term, the ontology confers a 3-fold increase of search efficiency. Textpresso currently focuses on Caenorhabditis elegans literature, with 3,800 full text articles and 16,000 abstracts. The lexicon of the ontology contains 14,500 entries, each of which includes all versions of a specific word or phrase, and it includes all categories of the Gene Ontology database. Textpresso is a useful curation tool, as well as search engine for researchers, and can readily be extended to other organism-specific corpora of text. Textpresso can be accessed at http://www.textpresso.org or via WormBase at http://www.wormbase.org.',\n",
       " ' Case-based medical informatics. BACKGROUND: The \"applied\" nature distinguishes applied sciences from theoretical sciences. To emphasize this distinction, we begin with a general, meta-level overview of the scientific endeavor. We introduce the knowledge spectrum and four interconnected modalities of knowledge. In addition to the traditional differentiation between implicit and explicit knowledge we outline the concepts of general and individual knowledge. We connect general knowledge with the \"frame problem,\" a fundamental issue of artificial intelligence, and individual knowledge with another important paradigm of artificial intelligence, case-based reasoning, a method of individual knowledge processing that aims at solving new problems based on the solutions to similar past problems.We outline the fundamental differences between Medical Informatics and theoretical sciences and propose that Medical Informatics research should advance individual knowledge processing (case-based reasoning) and that natural language processing research is an important step towards this goal that may have ethical implications for patient-centered health medicine. DISCUSSION: We focus on fundamental aspects of decision-making, which connect human expertise with individual knowledge processing. We continue with a knowledge spectrum perspective on biomedical knowledge and conclude that case-based reasoning is the paradigm that can advance towards personalized healthcare and that can enable the education of patients and providers.We center the discussion on formal methods of knowledge representation around the frame problem. We propose a context-dependent view on the notion of \"meaning\" and advocate the need for case-based reasoning research and natural language processing. In the context of memory based knowledge processing, pattern recognition, comparison and analogy-making, we conclude that while humans seem to naturally support the case-based reasoning paradigm (memory of past experiences of problem-solving and powerful case matching mechanisms), technical solutions are challenging.Finally, we discuss the major challenges for a technical solution: case record comprehensiveness, organization of information on similarity principles, development of pattern recognition and solving ethical issues. SUMMARY: Medical Informatics is an applied science that should be committed to advancing patient-centered medicine through individual knowledge processing. Case-based reasoning is the technical solution that enables a continuous individual knowledge processing and could be applied providing that challenges and ethical issues arising are addressed appropriately.',\n",
       " 'ArrayExpress--a public repository for microarray gene expression data at the EBI. ArrayExpress is a new public database of microarray gene expression data at the EBI, which is a generic gene expression database designed to hold data from all microarray platforms. ArrayExpress uses the annotation standard Minimum Information About a Microarray Experiment (MIAME) and the associated XML data exchange format Microarray Gene Expression Markup Language (MAGE-ML) and it is designed to store well annotated data in a structured way. The ArrayExpress infrastructure consists of the database itself, data submissions in MAGE-ML format or via an online submission tool MIAMExpress, online database query interface, and the Expression Profiler online analysis tool. ArrayExpress accepts three types of submission, arrays, experiments and protocols, each of these is assigned an accession number. Help on data submission and annotation is provided by the curation team. The database can be queried on parameters such as author, laboratory, organism, experiment or array types. With an increasing number of organisations adopting MAGE-ML standard, the volume of submissions to ArrayExpress is increasing rapidly. The database can be accessed at http://www.ebi.ac.uk/arrayexpress.',\n",
       " \"Entrez Gene: gene-centered information at NCBI. Entrez Gene (www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=gene) is NCBI's database for gene-specific information. It does not include all known or predicted genes; instead Entrez Gene focuses on the genomes that have been completely sequenced, that have an active research community to contribute gene-specific information, or that are scheduled for intense sequence analysis. The content of Entrez Gene represents the result of curation and automated integration of data from NCBI's Reference Sequence project (RefSeq), from collaborating model organism databases, and from many other databases available from NCBI. Records are assigned unique, stable and tracked integers as identifiers. The content (nomenclature, map location, gene products and their attributes, markers, phenotypes, and links to citations, sequences, variation details, maps, expression, homologs, protein domains and external databases) is updated as new information becomes available. Entrez Gene is a step forward from NCBI's LocusLink, with both a major increase in taxonomic scope and improved access through the many tools associated with NCBI Entrez. 10.1093/nar/gki031\",\n",
       " 'Online Mendelian Inheritance in Man (OMIM), a knowledgebase of human genes and genetic disorders.. Online Mendelian Inheritance in Man (OMIM) is a comprehensive, authoritative and timely knowledgebase of human genes and genetic disorders compiled to support research and education in human genomics and the practice of clinical genetics. Started by Dr Victor A. McKusick as the definitive reference Mendelian Inheritance in Man, OMIM (www.ncbi.nlm.nih.gov/omim) is now distributed electronically by the National Center for Biotechnology Information (NCBI), where it is integrated with the Entrez suite of databases. Derived from the biomedical literature, OMIM is written and edited at Johns Hopkins University with input from scientists and physicians around the world. Each OMIM entry has a full-text summary of a genetically determined phenotype and/or gene and has numerous links to other genetic databases such as DNA and protein sequence, PubMed references, general and locus-specific mutation databases, approved gene nomenclature, and the highly detailed mapviewer, as well as patient support groups and many others. OMIM is an easy and straightforward portal to the burgeoning information in human genetics.',\n",
       " 'The Molecular Biology Database Collection: 2005 update. The Nucleic Acids Research Molecular Biology Database Collection is a public online resource that lists the databases described in this and previous issues of Nucleic Acids Research together with other databases of value to the biologist and available throughout the world. All databases included in this Collection are freely available to the public. The 2005 update includes 719 databases, 171 more than the 2004 one. The databases are organized in a hierarchical classification that simplifies the process of finding the right database for any given task. The growing number of databases related to immunology, plant and organelle research have been accommodated by separating them into three new categories. The database summaries provide brief descriptions of the databases, contact details, appropriate references and acknowledgements. The online summaries also serve as a venue for the maintainers of each database to introduce database updates and other improvements in the scope and tools. These updates are particularly important for those databases that have not been described in print in the recent past. The database list and summaries are available online at the Nucleic Acids Research web site, http://nar.oupjournals.org/. 10.1093/nar/gki139',\n",
       " \"STRING: known and predicted protein-protein associations, integrated and transferred across organisms. A full description of a protein's function requires knowledge of all partner proteins with which it specifically associates. From a functional perspective,  association' can mean direct physical binding, but can also mean indirect interaction such as participation in the same metabolic pathway or cellular process. Currently, information about protein association is scattered over a wide variety of resources and model organisms. STRING aims to simplify access to this information by providing a comprehensive, yet quality-controlled collection of protein-protein associations for a large number of organisms. The associations are derived from high-throughput experimental data, from the mining of databases and literature, and from predictions based on genomic context analysis. STRING integrates and ranks these associations by benchmarking them against a common reference set, and presents evidence in a consistent and intuitive web interface. Importantly, the associations are extended beyond the organism in which they were originally described, by automatic transfer to orthologous protein pairs in other organisms, where applicable. STRING currently holds 730 000 proteins in 180 fully sequenced organisms, and is available at http://string.embl.de/. 10.1093/nar/gki005\",\n",
       " 'Reactome: a knowledgebase of biological pathways.. Reactome, located at http://www.reactome.org is a curated, peer-reviewed resource of human biological processes. Given the genetic makeup of an organism, the complete set of possible reactions constitutes its reactome. The basic unit of the Reactome database is a reaction; reactions are then grouped into causal chains to form pathways. The Reactome data model allows us to represent many diverse processes in the human system, including the pathways of intermediary metabolism, regulatory pathways, and signal transduction, and high-level processes, such as the cell cycle. Reactome provides a qualitative framework, on which quantitative data can be superimposed. Tools have been developed to facilitate custom data entry and annotation by expert biologists, and to allow visualization and exploration of the finished dataset as an interactive process map. Although our primary curational domain is pathways from Homo sapiens, we regularly create electronic projections of human pathways onto other organisms via putative orthologs, thus making Reactome relevant to model organism research communities. The database is publicly available under open source terms, which allows both its content and its software infrastructure to be freely used and redistributed.',\n",
       " 'The Biomolecular Interaction Network Database and related tools 2005 update. The Biomolecular Interaction Network Database (BIND) (http://bind.ca) archives biomolecular interaction, reaction, complex and pathway information. Our aim is to curate the details about molecular interactions that arise from published experimental research and to provide this information, as well as tools to enable data analysis, freely to researchers worldwide. BIND data are curated into a comprehensive machine-readable archive of computable information and provides users with methods to discover interactions and molecular mechanisms. BIND has worked to develop new methods for visualization that amplify the underlying annotation of genes and proteins to facilitate the study of molecular interaction networks. BIND has maintained an open database policy since its inception in 1999. Data growth has proceeded at a tremendous rate, approaching over 100 000 records. New services provided include a new BIND Query and Submission interface, a Standard Object Access Protocol service and the Small Molecule Interaction Database (http://smid.blueprint.org) that allows users to determine probable small molecule binding sites of new sequences and examine conserved binding residues. 10.1093/nar/gki051',\n",
       " 'The PANTHER database of protein families, subfamilies, functions and pathways.. PANTHER is a large collection of protein families that have been subdivided into functionally related subfamilies, using human expertise. These subfamilies model the divergence of specific functions within protein families, allowing more accurate association with function (ontology terms and pathways), as well as inference of amino acids important for functional specificity. Hidden Markov models (HMMs) are built for each family and subfamily for classifying additional protein sequences. The latest version, 5.0, contains 6683 protein families, divided into 31,705 subfamilies, covering approximately 90% of mammalian protein-coding genes. PANTHER 5.0 includes a number of significant improvements over previous versions, most notably (i) representation of pathways (primarily signaling pathways) and association with subfamilies and individual protein sequences; (ii) an improved methodology for defining the PANTHER families and subfamilies, and for building the HMMs; (iii) resources for scoring sequences against PANTHER HMMs both over the web and locally; and (iv) a number of new web resources to facilitate analysis of large gene lists, including data generated from high-throughput expression experiments. Efforts are underway to add PANTHER to the InterPro suite of databases, and to make PANTHER consistent with the PIRSF database. PANTHER is now publicly available without restriction at http://panther.appliedbiosystems.com.',\n",
       " \"E-MSD: an integrated data resource for bioinformatics. The Macromolecular Structure Database (MSD) group (http://www.ebi.ac.uk/msd/) continues to enhance the quality and consistency of macromolecular structure data in the worldwide Protein Data Bank (wwPDB) and to work towards the integration of various bioinformatics data resources. One of the major obstacles to the improved integration of structural databases such as MSD and sequence databases like UniProt is the absence of up to date and well-maintained mapping between corresponding entries. We have worked closely with the UniProt group at the EBI to clean up the taxonomy and sequence cross-reference information in the MSD and UniProt databases. This information is vital for the reliable integration of the sequence family databases such as Pfam and Interpro with the structure-oriented databases of SCOP and CATH. This information has been made available to the eFamily group (http://www.efamily.org.uk/) and now forms the basis of the regular interchange of information between the member databases (MSD, UniProt, Pfam, Interpro, SCOP and CATH). This exchange of annotation information has enriched the structural information in the MSD database with annotation from wider sequence-oriented resources. This work was carried out under the  Structure Integration with Function, Taxonomy and Sequences (SIFTS)' initiative (http://www.ebi.ac.uk/msd-srv/docs/sifts) in the MSD group. 10.1093/nar/gki058\",\n",
       " 'The Universal Protein Resource (UniProt).. The Universal Protein Resource (UniProt) provides the scientific community with a single, centralized, authoritative resource for protein sequences and functional information. Formed by uniting the Swiss-Prot, TrEMBL and PIR protein database activities, the UniProt consortium produces three layers of protein sequence databases: the UniProt Archive (UniParc), the UniProt Knowledgebase (UniProt) and the UniProt Reference (UniRef) databases. The UniProt Knowledgebase is a comprehensive, fully classified, richly and accurately annotated protein sequence knowledgebase with extensive cross-references. This centrepiece consists of two sections: UniProt/Swiss-Prot, with fully, manually curated entries; and UniProt/TrEMBL, enriched with automated classification and annotation. During 2004, tens of thousands of Knowledgebase records got manually annotated or updated; we introduced a new comment line topic: TOXIC DOSE to store information on the acute toxicity of a toxin; the UniProt keyword list got augmented by additional keywords; we improved the documentation of the keywords and are continuously overhauling and standardizing the annotation of post-translational modifications. Furthermore, we introduced a new documentation file of the strains and their synonyms. Many new database cross-references were introduced and we started to make use of Digital Object Identifiers. We also achieved in collaboration with the Macromolecular Structure Database group at EBI an improved integration with structural databases by residue level mapping of sequences from the Protein Data Bank entries onto corresponding UniProt entries. For convenient sequence searches we provide the UniRef non-redundant sequence databases. The comprehensive UniParc database stores the complete body of publicly available protein sequence data. The UniProt databases can be accessed online (http://www.uniprot.org) or downloaded in several formats (ftp://ftp.uniprot.org/pub). New releases are published every two weeks.',\n",
       " 'Rfam: annotating non-coding RNAs in complete genomes.. Rfam is a comprehensive collection of non-coding RNA (ncRNA) families, represented by multiple sequence alignments and profile stochastic context-free grammars. Rfam aims to facilitate the identification and classification of new members of known sequence families, and distributes annotation of ncRNAs in over 200 complete genome sequences. The data provide the first glimpses of conservation of multiple ncRNA families across a wide taxonomic range. A small number of large families are essential in all three kingdoms of life, with large numbers of smaller families specific to certain taxa. Recent improvements in the database are discussed, together with challenges for the future. Rfam is available on the Web at http://www.sanger.ac.uk/Software/Rfam/ and http://rfam.wustl.edu/.',\n",
       " \"Monad transformers and modular interpreters. We show how a set of building blocks can be used to construct programming language interpreters, and present implementations of such building blocks capable of supporting many commonly known features, including simple expressions, three different function call mechanisms (call-by-name, callby- value and lazy evaluation), references and assignment, nondeterminism, first-class continuations, and program tracing. The underlying mechanism of our system is monad transformers, a simple form of abstraction for introducing a wide range of computational behaviors, such as state, I/O, continuations, and exceptions. Our work is significant in the following respects. First, we have succeeded in designing a fully modular interpreter based on monad transformers that includes features missing from Steele's, Espinosa's, and Wadler's earlier efforts. Second, we have found new ways to lift monad operations through monad transformers, in particular difficult cases not achieved in Moggi's original work. Third, we have demonstrated that interactions between features are reflected in liftings and that semantics can be changed by reordering monad transformers. Finally, we have implemented our interpreter in Gofer, whose constructor classes provide just the added power over Haskell's type classes to allow precise and convenient expression of our ideas. This implementation includes a method for constructing extensible unions and a form of subtyping that is interesting in its own right.\",\n",
       " \"Comprehending Monads. Category theorists invented \\\\\\\\emph{monads} in the 1960's to concisely express certain aspects of universal algebra. Functional programmers invented \\\\\\\\emph{list comprehensions} in the 1970's to concisely express certain programs involving lists. This paper shows how list comprehensions may be generalised to an arbitrary monad, and how the resulting programming feature can concisely express in a pure functional language some programs that manipulate state, handle exceptions, parse text, or invoke continuations. A new solution to the old problem of destructive array update is also presented. No knowledge of category theory is assumed.\",\n",
       " 'Monads for functional programming. . The use of monads to structure functional programs is described. Monads provide a convenient framework for simulating effects found in other languages, such as global state, exception handling, output, or non-determinism. Three case studies are looked at in detail: how monads ease the modification of a simple evaluator; how monads act as the basis of a datatype of arrays subject to in-place update; and how monads can be used to build parsers. 1 Introduction Shall I be pure or impure? The...',\n",
       " 'The Open Agent Architecture: a framework for building distributed software systems. The Open Agent Architecture (OAA), developed and used for several years at SRI International, makes it possible for software services to be provided through the cooperative efforts of distributed collections of autonomous agents. Communication and cooperation between agents are brokered by one or more facilitators, which are responsible for matching requests, from users and agents, with descriptions of the capabilities of other agents. Thus it is not generally required that a user or agent know the identities, locations, or number of other agents involved in satisfying a request. OAA is structured so as to minimize the effort involved in creating new agents and \"wrapping\" legacy applications, written in various languages and operating on various platforms; to encourage the reuse of existing agents; and to allow for dynamism and flexibility in the makeup of agent communities. Distinguishing features of OAA as compared with related work include extreme flexibility in using facilitator-based delegation of complex goals, triggers, and data management requests; agent-based provision of multimodal user interfaces; and built-in support for including the user as a privileged member of the agent community. This article explains the structure and elements of agent-based systems constructed using OAA. The characteristics and use of each major component of OAA infrastructure are described, including the agent library, the Interagent Communication Language, capabilities declarations, service requests, facilitation, management of data repositories, and autonomous monitoring using triggers. To provide technical context, we describe the motivations for OAA\\'s design, and situate its features within the realm of alternative software paradigms. A summary is given of OAA-based systems built to date, and brief descriptions are given of several of these. Copyright © 1999 Taylor & Francis.',\n",
       " 'Adapting Golog for composition of semantic Web services. Motivated by the problem of automatically composing network accessible services, such as those on the World Wide Web, this paper proposes an approach to building agent technology based on the notion of generic procedures and customizing user constraint. We argue that an augmented version of the logic programming language Golog provides a natural formalism for automatically composing services on the Semantic Web. To this end, we adapt and extend the Golog language to enable programs that are...',\n",
       " 'A correspondence between continuation passing style and static single assignment form. We define syntactic transformations that convert continuation passing style (CPS) programs into static single assignment form (SSA) and vice versa. Some CPS programs cannot be converted to SSA, but these are not produced by the usual CPS transformation. The CPS&rarr;SSA transformation is especially helpful for compiling functional programs. Many optimizations that normally require flow analysis can be performed directly on functional CPS programs by viewing them as SSA programs. We also present a simple program transformation that merges CPS procedures together and by doing so greatly increases the scope of the SSA flow information. This transformation is useful for analyzing loops expressed as recursive procedures.',\n",
       " 'The essence of compiling with continuations. In order to simplify the compilation process, many compilers for higher-order languages use the continuation-passing style (CPS) transformation in a first phase to generate an intermediate representation of the source program. The salient aspect of this intermediate form is that all procedures take an argument that represents the rest of the computation (the &ldquo;continuation&rdquo;). Since the nai&uml;ve CPS transformation considerably increases the size of programs, CPS compilers perform reductions to produce a more compact intermediate representation. Although often implemented as a part of the CPS transformation, this step is conceptually a second phase. Finally, code generators for typical CPS compilers treat continuations specially in order to optimize the interpretation of continuation parameters.   A thorough analysis of the abstract machine for CPS terms show that the actions of the code generator  invert  the nai&uml;ve CPS translation step. Put differently, the combined effect of the three phases is equivalent to a source-to-source transformation that simulates the compaction phase. Thus, fully developed CPS compilers do not need to employ the CPS transformation but can achieve the same results with a simple source-level transformation.',\n",
       " 'Rabbit: A Compiler for Scheme. We have developed a compiler for the lexically-scoped dialect of LISP known as SCHEME. The compiler knows relatively little about specific data manipulation primitives such as arithmetic operators, but concentrates on general issues of environment and control. Rather than having specialized knowledge about a large variety of control and environment constructs, the compiler handles only a small basis set which reflects the semantics of lambda-calculus. All of the traditional imperative constructs, such as sequencing, assignment, looping, GO TO, as well as many standard LISP constructs such as AND, OR and COND, are expressed as macros in terms of the applicative basis set. A small number of optimization techniques, coupled with the treatment of function calls as GO TO statements, serves to produce code as good as that produced by more traditional compilers.',\n",
       " 'Effective Web Crawling. The key factors for the success of the World Wide Web are its large size and the lack of a centralized control over its contents. Both issues are also the most important source of problems for locating information. The Web is a context in which traditional information Retrieval methods are challenged, and given the volume of the Web and its speed of change, the coverage of modern search engines is relatively small. Moreover, the distribution of quality is very skewed, and interesting pages are scarce in comparison with the rest of the content. Web crawling is the process used by search engines to collect pages from the Web. This thesis studies Web crawling at several different levels, ranging from the long-term goal of crawling important pages first, to the short-term goal of using the network connectivity efficiently, including implementation issues that are essential for crawling in practice. We start by designing a new model and architecture for a Web crawler that tightly integrates the crawler with the rest of the search engine, providing access to the metadata and links of the documents that can be used to guide the crawling process effectively. We implement this design in the WIRE project as an efficient Web crawler that provides an experimental framework for this research. In fact, we have used our crawler to characterize the Chilean Web, using the results as feedback to improve the crawler design. We argue that the number of pages on the Web can be considered infinite, and given that a Web crawler cannot download all the pages, it is important to capture the most important ones as early as possible during the crawling process. We propose, study, and implement algorithms for achieving this goal, showing that we can crawl 50% of a large Web collection and capture 80% of its total Pagerank value in both simulated and real Web environments. We also model and study user browsing behavior inWeb sites, concluding that it is not necessary to go deeper than five levels from the home page to capture most of the pages actually visited by people, and support this conclusion with log analysis of several Web sites. We also propose several mechanisms for server cooperation to reduce network traffic and improve the representation of a Web page in a search engine with the help of Web site managers.',\n",
       " 'Having a BLAST with bioinformatics (and avoiding BLASTphemy).. Searching for similarities between biological sequences is the principal means by which bioinformatics contributes to our understanding of biology. Of the various informatics tools developed to accomplish this task, the most widely used is BLAST, the basic local alignment search tool. This article discusses the principles, workings, applications and potential pitfalls of BLAST, focusing on the implementation developed at the National Center for Biotechnology Information.',\n",
       " \"Deleting species from model food webs. In natural biological communities the disappearance of one species can have knock-on effects causing extinction of further species from the food web. To investigate these effects we used an evolutionary model to assemble many independent simulated food webs, and studied their dynamical behaviour when one species was deleted. On average, only 2.1% of the remaining species went extinct as a result of the deletion. However, the probability of extinction of predators and indirect predators (more than one link up the chain) of the deleted species was several times larger than for an average species. The model allows predators to adapt their choice of prey in response to changing frequencies of the prey. It was found that the larger the proportion of the deleted species in the predator's diet, the greater its probability of extinction. The probability of extinction of prey of the deleted species was also significantly higher than for an average species. This is due to increased competition between prey species after removal of their predator. The effect was largest for prey species that formed an intermediate fraction of the diet of the deleted species. The number of further extinctions increased significantly with the number of links in the food web to the deleted species prior to deletion, and was also correlated with the bottom-up and top-down keystone species indices. We also considered which properties of the web as a whole influenced its robustness to species deletion. This revealed a significant correlation between ecosystem redundancy and deletion stability, but no clear relationship with complexity\",\n",
       " 'Topological structure and interaction strengths in model food webs. We report the results of carrying out a large number of simulations on a coevolutionary model of multispecies communities. A wide range of parameter values were investigated which allowed a rather complete picture of the change in behaviour of the model as these parameters were varied to be built up. Our main interest was in the nature of the community food webs constructed via the simulations. We identify the range of parameter values which give rise to realistic food webs and give arguments which allow some of the structure which is found to be understood in an intuitive way. Since the webs are evolved according to the rules of the model, the strengths of the predator-prey links are not determined a priori, and emerge from the process of constructing the web. We measure the distribution of these link strengths, and find that there are a large number of weak links, in agreement with recent suggestions. We also review some of the data on food webs available in the literature, and make some tentative comparisons with our results. The difficulties of making such comparisons and the possible future developments of the model are also briefly discussed.',\n",
       " 'A Sense of Self for Unix Processes. A method for anomaly detection is introduced in which &#034;normal&#034; is defined by short-range correlations in a process &#039; system calls. Initial experiments suggest that the definition is stable during normal behavior for standard UNIX programs. Further, it is able to detect several common intrusions involving sendmail and lpr. This work is part of a research program aimed at building computer security systems that incorporate the mechanisms and algorithms used by natural immune systems. 1 Introduction  We are interested in developing computer security methods that are based on the way natural immune systems distinguish self from other. Such &#034;artificial immune systems&#034; would have richer notions of identity and protection than those afforded by current operating systems, and they could provide a layer of general-purpose protection to augment current computer security systems. An important prerequisite of such a system is an appropriate definition of self, which is the subject of this paper. W...',\n",
       " 'Data mining approaches for intrusion detection. In this paper we discuss our research in developing general and systematic methods for intrusion detection. The key ideas are to use data mining techniques to discover consistent and useful patterns of system features that describe program and user behavior, and use the set of relevant system features to compute (inductively learned) classifiers that can recognize anomalies and known intrusions. Using experiments on the sendmail system call data and the network tcpdump data, we demonstrate that ...',\n",
       " \"Tracking Down Software Bugs Using Automatic Anomaly Detection. This paper introduces DIDUCE, a practical and effective tool that aids programmers in detecting complex program errors and identifying their root causes. By instrumenting a program and observing its behavior as it runs, DIDUCE dynamically formulates hypotheses of invariants obeyed by the program. DIDUCE hypothesizes the strictest invariants at the beginning, and gradually relaxes the hypothesis as violations are detected to allow for new behavior. The violations reported help users to catch software bugs as soon as they occur. They also give programmers new visibility into the behavior of the programs such as identifying rare corner cases in the program logic or even locating hidden errors that corrupt the program's results.We implemented the DIDUCE system for Java programs and applied it to four programs of significant size and complexity. DIDUCE succeeded in identifying the root causes of programming errors in each of the programs quickly and automatically. In particular, DIDUCE is effective in isolating a timing-dependent bug in a released JSSE (Java Secure Socket Extension) library, which would have taken an experienced programmer days to find. Our experience suggests that detecting and checking program invariants dynamically is a simple and effective methodology for debugging many different kinds of program errors across a wide variety of application domains.\",\n",
       " \"Insertion, Evasion, and Denial of Service: Eluding Network Intrusion Detection. All currently available network intrusion detection (ID) systems rely  upon a mechanism of data collection---passive protocol analysis---which  is fundamentally flawed. In passive protocol analysis, the intrusion detection  system (IDS) unobtrusively watches all traffic on the network, and  scrutinizes it for patterns of suspicious activity. We outline in this paper  two basic problems with the reliability of passive protocol analysis:  (1) there isn't enough information on the wire on which to ...\",\n",
       " 'Computer immunology. Summary:\\u2002 This review describes a body of work on computational immune systems that behave analogously to the natural immune system. These artificial immune systems (AIS) simulate the behavior of the natural immune system and in some cases have been used to solve practical engineering problems such as computer security. AIS have several strengths that can complement wet lab immunology. It is easier to conduct simulation experiments and to vary experimental conditions, for example, to rule out hypotheses; it is easier to isolate a single mechanism to test hypotheses about how it functions; agent-based models of the immune system can integrate data from several different experiments into a single in silico experimental system.',\n",
       " 'Learning the parts of objects by non-negative matrix factorization.. Is perception of the whole based on perception of its parts? There is psychological1 and physiological2, 3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4, 5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.',\n",
       " 'Machine learning in automated text categorization. The automated categorisation (or classiﬁcation) of texts into topical categories has a long history, dating back at least to the early ’60s. Until the late ’80s, the most eﬀective approach to the problem seemed to be that of manually building automatic classiﬁers by means of knowledgeengineering techniques, i.e. manually deﬁning a set of rules encoding expert knowledge on how to classify documents under a given set of categories. In the ’90s, with the booming production and availability of on-line documents, automated text categorisation has witnessed an increased and renewed interest, prompted by which the machine learning paradigm to automatic classiﬁer construction has emerged and deﬁnitely superseded the knowledge-engineering approach. Within the machine learning paradigm, a general inductive process (called the learner) automatically builds a classiﬁer (also called the rule, or the hypothesis) by “learning”, from a set of previously classiﬁed documents, the characteristics of one or more categories. The advantages of this approach are a very good eﬀectiveness, a considerable savings in terms of expert manpower, and domain independence. In this survey we look at the main approaches that have been taken towards automatic text categorisation within the general machine learning paradigm. Issues pertaining to document indexing, classiﬁer construction, and classiﬁer evaluation, will be discussed in detail. A ﬁnal section will be devoted to the techniques that have speciﬁcally been devised for an emerging application such as the automatic classiﬁcation of Web pages into “Yahoo!-like” hierarchically structured sets of categories.',\n",
       " 'External memory algorithms and data structures: dealing with massive data. Data sets in large applications are often too massive to fit completely inside the computers internal memory. The resulting input/output communication (or I/O) between fast internal memory and slower external memory (such as disks) can be a major performance bottleneck. In this article we survey the state of the art in the design and analysis of external memory (or EM) algorithms and data structures, where the goal is to exploit locality in order to reduce the I/O costs. We consider a variety of EM paradigms for solving batched and online problems efficiently in external memory. For the batched problem of sorting and related problems such as permuting and fast Fourier transform, the key paradigms include distribution and merging. The paradigm of disk striping offers an elegant way to use multiple disks in parallel. For sorting, however,  disk striping can be nonoptimal with respect to I/O, so to gain further improvements we discuss distribution and merging techniques for using the disks independently. We also consider useful techniques for batched EM problems involving matrices (such as matrix multiplication and transposition), geometric data (such as finding intersections and constructing convex hulls), and graphs (such as list ranking, connected components, topological sorting, and shortest paths). In the online domain, canonical EM applications include dictionary lookup and range searching. The two important classes of indexed data structures are based upon extendible hashing and B-trees. The paradigms of filtering and bootstrapping provide a convenient means in online data structures to make effective use of the data accessed from disk. We also reexamine some of the above EM problems in slightly different settings, such as when the data items are moving, when the data items are variable-length (e.g., text strings), or when the allocated amount of internal memory can change dynamically. Programming tools and environments are available for simplifying the EM programming task. During the course of the survey, we report on some experiments in the domain of spatial databases using the TPIE system (transparent parallel I/O programming environment). The newly developed EM algorithms and data structures that incorporate the paradigms we discuss are significantly faster than methods currently used in practice.',\n",
       " 'Searching in metric spaces. The problem of searching the elements of a set that are close to a given query element under some similarity criterion has a vast number of applications in many branches of computer science, from pattern recognition to textual and multimedia information retrieval. We are interested in the rather general case where the similarity criterion defines a metric space, instead of the more restricted case of a vector space. Many solutions have been proposed in different areas, in many cases without cross-knowledge. Because of this, the same ideas have been reconceived several times, and very different presentations have been given for the same approaches. We present some basic results that explain the intrinsic difficulty of the search problem. This includes a quantitative definition of the elusive concept of \"intrinsic dimensionality.\" We also present a unified view of all the known proposals to organize metric spaces, so as to be able to understand them under a common framework. Most approaches turn out to be variations on a few different concepts. We organize those works in a taxonomy that allows us to devise new algorithms from combinations of concepts not noticed before because of the lack of communication between different communities. We present experiments validating our results and comparing the existing approaches. We finish with recommendations for practitioners and open questions for future development.',\n",
       " 'Searching in high-dimensional spaces: Index structures for improving the performance of multimedia databases. During the last decade, multimedia databases have become increasingly important in many application areas such as medicine, CAD, geography, and molecular biology. An important research issue in the field of multimedia databases is the content-based retrieval of similar multimedia objects such as images, text, and videos. However, in contrast to searching data in a relational database, a content-based retrieval requires the search of similar objects as a basic functionality of the database system. Most of the approaches addressing similarity search use a so-called feature transformation that transforms important properties of the multimedia objects into high-dimensional points (feature vectors). Thus, the similarity search is transformed into a search of points in the feature space that are close to a given query point in the high-dimensional feature space. Query processing in high-dimensional spaces has therefore been a very active research area over the last few years. A number of new index structures and algorithms have been proposed. It has been shown that the new index structures considerably improve the performance in querying large multimedia databases. Based on recent tutorials [Berchtold and Keim 1998], in this survey we provide an overview of the current state of the art in querying multimedia databases, describing the index structures and algorithms for an efficient query processing in high-dimensional spaces. We identify the problems of processing queries in high-dimensional space, and we provide an overview of the proposed approaches to overcome these problems.',\n",
       " 'Trust in recommender systems. Recommender systems have proven to be an important response to the information overload problem, by providing users with more proactive and personalized information services. And collaborative filtering techniques have proven to be an vital component of many such recommender systems as they facilitate the generation of high-quality recom-mendations by leveraging the preferences of communities of similar users. In this paper we suggest that the traditional emphasis on user similarity may be overstated. We argue that additional factors have an important role to play in guiding recommendation. Specifically we propose that the trustworthiness of users must be an important consideration. We present two computational models of trust and show how they can be readily incorporated into standard collaborative filtering frameworks in a variety of ways. We also show how these trust models can lead to improved predictive accuracy during recommendation.',\n",
       " \"Collaboration Analysis in Recommender Systems using Social Networks. Ce papier présente une analyse de la collaboration dans les systèmes de recommendations. L'analyse se base sur des mesures des social networks : * Locality : La distance max considérée. Si =1 : chaque pair ne voit que ces voisins direct. Si =2 il peut y avoir transitivité entre 3 agents : soit A,B,C avec A->B et B->C alors A->C * Size : La taille du réseau en nombre de personnes * Density : is the proportion of all ties that could be present that actually are * Degree Centrality : is a measure which counts the number of ties an actor has * Network Centrality : degree centrality appliqué au réseau en entier * Clique membership : le nombre de cliques auxquelles l'acteur appartient * Factions : Partitions dans le graphe, moins stricte qu'une clique Appliquent ces mesures pour tester un système de recommandation de restos. Mesures testées avec UCINet et implémentées avec JADE\",\n",
       " \"The impact of web-logs (blogs) on student perceptions of isolation and alienation in a web-based distance-learning environment. In the rush to promote the use of computer-mediated technologies for both traditional and distance learning, relatively little research has been conducted about learner feelings of isolation, alienation and frustration. More recent technologies such as web-logs (blogs) may provide a wider range of tools for bridging learners' feelings of isolation. The purpose of this research is to investigate the impact of using blogs in a web-based learning environment. This qualitative investigation presents an interpretive case study of student perceptions of using blogs in a web-based technology integration course for K-12 pre-service teacher education students. Findings indicate that the use of blogs helped prevent feelings of isolation and alienation for distance learners\",\n",
       " 'Trust-Aware Collaborative Filtering for Recommender Systems. Recommender Systems allow people to find the resources they need by making use of the experiences and opinions of their nearest neighbours. Costly annotations by experts are replaced by a distributed process where the users take the initiative. While the collaborative approach enables the collection of a vast amount of data, a new issue arises: the quality assessment. The elicitation of trust values among users, termed ldquoweb of trustrdquo, allows a twofold enhancement of Recommender Systems. Firstly, the filtering process can be informed by the reputation of users which can be computed by propagating trust. Secondly, the trust metrics can help to solve a problem associated with the usual method of similarity assessment, its reduced computability. An empirical evaluation on Epinions.com dataset shows that trust propagation can increase the coverage of Recommender Systems while preserving the quality of predictions. The greatest improvements are achieved for users who provided few ratings.',\n",
       " \"Uncertainty in predictions of the climate response to rising levels of greenhouse gases. The range of possibilities for future climate evolution1, 2, 3 needs to be taken into account when planning climate change mitigation and adaptation strategies. This requires ensembles of multi-decadal simulations to assess both chaotic climate variability and model response uncertainty4, 5, 6, 7, 8, 9. Statistical estimates of model response uncertainty, based on observations of recent climate change10, 11, 12, 13, admit climate sensitivities—defined as the equilibrium response of global mean temperature to doubling levels of atmospheric carbon dioxide—substantially greater than 5 K. But such strong responses are not used in ranges for future climate change14 because they have not been seen in general circulation models. Here we present results from the 'climateprediction.net' experiment, the first multi-thousand-member grand ensemble of simulations using a general circulation model and thereby explicitly resolving regional details15, 16, 17, 18, 19, 20, 21. We find model versions as realistic as other state-of-the-art climate models but with climate sensitivities ranging from less than 2 K to more than 11 K. Models with such extreme sensitivities are critical for the study of the full range of possible responses of the climate system to rising greenhouse gas levels, and for assessing the risks associated with specific targets for stabilizing these levels.\",\n",
       " 'Self-similarity of complex networks. Complex networks have been studied extensively due to their relevance to many real systems as diverse as the World-Wide-Web (WWW), the Internet, energy landscapes, biological and social networks \\\\cite{ab-review,mendes,vespignani,newman,amaral}. A large number of real networks are called “scale-free” because they show a power-law distribution of the number of links per node \\\\cite{ab-review,barabasi1999,faloutsos}. However, it is widely believed that complex networks are not {\\\\it length-scale} invariant or self-similar. This conclusion originates from the “small-world” property of these networks, which implies that the number of nodes increases exponentially with the “diameter” of the network \\\\cite{erdos,bollobas,milgram,watts}, rather than the power-law relation expected for a self-similar structure. Nevertheless, here we present a novel approach to the analysis of such networks, revealing that their structure is indeed self-similar. This result is achieved by the application of a renormalization procedure which coarse-grains the system into boxes containing nodes within a given \"size\". Concurrently, we identify a power-law relation between the number of boxes needed to cover the network and the size of the box defining a finite self-similar exponent. These fundamental properties, which are shown for the WWW, social, cellular and protein-protein interaction networks, help to understand the emergence of the scale-free property in complex networks. They suggest a common self-organization dynamics of diverse networks at different scales into a critical state and in turn bring together previously unrelated fields: the statistical physics of complex networks with renormalization group, fractals and critical phenomena.',\n",
       " 'A measure of similarity between graph vertices. We introduce a concept of similarity between vertices of directed graphs. Let G_A and G_B be two directed graphs. We define a similarity matrix whose (i, j)-th real entry expresses how similar vertex j (in G_A) is to vertex i (in G_B. The similarity matrix can be obtained as the limit of the normalized even iterates of a linear transformation. In the special case where G_A=G_B=G, the matrix is square and the (i, j)-th entry is the similarity score between the vertices i and j of G. We point out that Kleinberg\\'s \"hub and authority\" method to identify web-pages relevant to a given query can be viewed as a special case of our definition in the case where one of the graphs has two vertices and a unique directed edge between them. In analogy to Kleinberg, we show that our similarity scores are given by the components of a dominant eigenvector of a non-negative matrix. Potential applications of our similarity concept are numerous. We illustrate an application for the automatic extraction of synonyms in a monolingual dictionary.',\n",
       " 'The economics of ideas and intellectual property.. Innovation and the adoption of new ideas is fundamental to economic progress. Here we examine the underlying economics of the market for ideas. From a positive perspective, we examine how such markets function with and without government intervention. From a normative perspective, we examine the pitfalls of existing institutions, and how they might be improved. We highlight recent research by us and others challenging the notion that government awards of monopoly through patents and copyright are \"the way\" to provide appropriate incentives for innovation.',\n",
       " 'Visual speech speeds up the neural processing of auditory speech.. Synchronous presentation of stimuli to the auditory and visual systems can modify the formation of a percept in either modality. For example, perception of auditory speech is improved when the speaker\\'s facial articulatory movements are visible. Neural convergence onto multisensory sites exhibiting supra-additivity has been proposed as the principal mechanism for integration. Recent findings, however, have suggested that putative sensory-specific cortices are responsive to inputs presented through a different modality. Consequently, when and where audiovisual representations emerge remain unsettled. In combined psychophysical and electroencephalography experiments we show that visual speech speeds up the cortical processing of auditory signals early (within 100 ms of signal onset). The auditory-visual interaction is reflected as an articulator-specific temporal facilitation (as well as a nonspecific amplitude reduction). The latency facilitation systematically depends on the degree to which the visual signal predicts possible auditory targets. The observed auditory-visual data support the view that there exist abstract internal representations that constrain the analysis of subsequent speech inputs. This is evidence for the existence of an \"analysis-by-synthesis\" mechanism in auditory-visual speech perception.',\n",
       " 'The Byzantine Generals Problem. Reliable computer systems must handle malfunctioning components that give conflicting information to different parts of the system. This situation can be expressed abstractly in terms of a group of generals of the Byzantine army camped with their troops around an enemy city. Communicating only by messenger, the generals must agree upon a common battle plan. However, one or more of them may be traitors who will try to confuse the others. The problem is to find an algorithm to ensure that the loyal generals will reach agreement. It is shown that, using only oral messages, this problem is solvable if and only if more than two-thirds of the generals are loyal; so a single traitor can confound two loyal generals. With unforgeable written messages, the problem is solvable for any number of generals and possible traitors. Applications of the solutions to reliable computer systems are then discussed.',\n",
       " 'Texture classification: Are filter banks necessary. We question the role that large scale filter banks have traditionally played in texture classification. It is demonstrated that textures can be classified using the joint distribution of intensity values over extremely compact neighborhoods (starting from as small as 3 &times; 3 pixels square), and that this outperforms classification using filter banks with large support. We develop a novel texton based representation, which is suited to modeling this joint neighborhood distribution for MRFs. The representation is learnt from training images, and then used to classify novel images (with unknown viewpoint and lighting) into texture classes. The power of the method is demonstrated by classifying over 2800 images of all 61 textures present in the Columbia-Utrecht database. The classification performance surpasses that of recent state-of-the-art filter bank based classifiers such as Leung & Malik, Cula & Dana, and Varma & Zisserman.',\n",
       " 'Human-Computer Interaction: Psychological Aspects of the Human Use of Computing. Updates and expands the last review of human-computer interaction in the Annual Review of Psychology (J. M. Carroll, 1997). It is maintained that there has been steady growth in the field since then, and this article reviews some of the most significant changes. Explicit attention is given to the emergence of research at the group and organizational level. The following topics are discussed: (1) the science of human computer interaction (cognitive modeling, distributed cognition, scope of the theories, human computer interaction work on information retrieval); (2) user interface developments (mobile devices, immersive environments, ubiquitous computing, Internet, World Wide Web, extended populations of users); (3) usability methods; (4) the workplace; (5) computer supported cooperative work (E-mail, meeting support, conferencing tools: voice and video, Instant messaging, Chat, MUDs, awareness, group calendars, repositories of shared knowledge, social filtering, trust of people via the technology, and collaboratories); and (6) the larger social context.',\n",
       " 'Human-Computer Interaction: Psychology as a Science of Design. Human-computer interaction (HCI) is the area of intersection between psychology and the social sciences, on the one hand, and computer science and technology, on the other. HCI researchers analyse and design-specific user-interface technologies (e.g. three-dimensional pointing devices, interactive video). They study and improve the processes of technology development (e.g. usability evaluation, design rationale). They develop and evaluate new applications of technology (e.g. computer conferencing, software design environments). Through the past two decades, HCI has progressively integrated its scientific concerns with the engineering goal of improving the usability of computer systems and applications, thus establishing a body of technical knowledge and methodology. HCI continues to provide a challenging test domain for applying and developing psychology and social science in the context of technology development and use.',\n",
       " 'iPfam: visualization of protein-protein interactions in PDB at domain and amino acid resolutions.. SUMMARY: There are many resources that contain information about binary interactions between proteins. However, protein interactions are defined by only a subset of residues in any protein. We have implemented a web resource that allows the investigation of protein interactions in the Protein Data Bank structures at the level of Pfam domains and amino acid residues. This detailed knowledge relies on the fact that there are a large number of multidomain proteins and protein complexes being deposited in the structure databases. The resource called iPfam is hosted within the Pfam UK website. Most resources focus on the interactions between proteins; iPfam includes these as well as interactions between domains in a single protein. AVAILABILITY: iPfam is available on the Web for browsing at http://www.sanger.ac.uk/Software/Pfam/iPfam/; the source-data for iPfam is freely available in relational tables via the ftp site ftp://ftp.sanger.ac.uk/pub/databases/Pfam/database_files/ CONTACT: rdf@sanger.ac.uk.',\n",
       " \"Simulation tools for biochemical networks: evaluation of performance and usability.. MOTIVATION: Simulation of dynamic biochemical systems is receiving considerable attention due to increasing availability of experimental data of complex cellular functions. Numerous simulation tools have been developed for numerical simulation of the behavior of a system described in mathematical form. However, there exist only a few evaluation studies of these tools. Knowledge of the properties and capabilities of the simulation tools would help bioscientists in building models based on experimental data. RESULTS: We examine selected simulation tools that are intended for the simulation of biochemical systems. We choose four of them for more detailed study and perform time series simulations using a specific pathway describing the concentration of the active form of protein kinase C. We conclude that the simulation results are convergent between the chosen simulation tools. However, the tools differ in their usability, support for data transfer to other programs and support for automatic parameter estimation. From the experimentalists' point of view, all these are properties that need to be emphasized in the future. CONTACT: antti.pettinen@tut.fi.\",\n",
       " \"A Bayesian approach to reconstructing genetic regulatory networks with hidden factors. Motivation: We have used state-space models (SSMs) to reverse engineer transcriptional networks from highly replicated gene expression profiling time series data obtained from a well-established model of T cell activation. SSMs are a class of dynamic Bayesian networks in which the observed measurements depend on some hidden state variables that evolve according to Markovian dynamics. These hidden variables can capture effects that cannot be directly measured in a gene expression profiling experiment, for example: genes that have not been included in the microarray, levels of regulatory proteins, the effects of mRNA and protein degradation, etc.  Results: We have approached the problem of inferring the model structure of these state-space models using both classical and Bayesian methods. In our previous work, a bootstrap procedure was used to derive classical confidence intervals for parameters representing  gene-gene' interactions over time. In this article, variational approximations are used to perform the analogous model selection task in the Bayesian context. Certain interactions are present in both the classical and the Bayesian analyses of these regulatory networks. The resulting models place JunB and JunD at the centre of the mechanisms that control apoptosis and proliferation. These mechanisms are key for clonal expansion and for controlling the long term behavior (e.g. programmed cell death) of these cells.  Availability: Supplementary data is available at http://public.kgi.edu/wild/index.htm and Matlab source code for variational Bayesian learning of SSMs is available at http://www.cse.buffalo.edu/faculty/mbeal/software.html  Contact: David_Wild@kgi.edu 10.1093/bioinformatics/bti014\",\n",
       " 'Software Agents. Technological advance, allied with a consistent reduction in hardware costs, is accelerating the automatization of society as a whole. Hand in hand with this process, we are faced with the problem of coping with the rapidly growing complexity of the processes of integrating, managing, and operating computer-based systems (Sterritt and Hinchey 2005). Today’s distributed systems demand software applications that do more than simply coping with service demands; they must also be able to anticipate, predict, and adapt themselves to respond to user needs. Indeed, researchers from both industry and academia are investigating the development of autonomous software agents that may take over tasks once conducted by humans. The development of autonomic (i.e., self-managing) systems is only part of this effort (Rouff 2006). According to the Software Agents Group of the MIT Media Laboratory, software agents are very different from conventional software because they are semi-autonomous, pro-active, adaptive, and long-lived.',\n",
       " 'Cognitive Principles of Multimedia Learning: The Role of Modality and Contiguity. Students viewed a computer animation depicting the process of lightning. In Experiment 1, they concurrently viewed on-screen text presented near the animation or far from the animation, or concurrently listened to a narration. In Experiment 2, they concurrently viewed on-screen text or listened to a narration, viewed on-screen text following or preceding the animation, or listened to a narration following or preceding the animation. Learning was measured by retention, transfer, and matching tests. Experiment 1 revealed a spatial-contiguity effect in which students learned better when visual and verbal materials were physically close. Both experiments revealed a modality effect in which students learned better when verbal input was presented auditorily as speech rather than visually as text. The results support 2 cognitive principles of multimedia learning.',\n",
       " 'Aids to computer-based multimedia learning. Computer-based multimedia learning environments - consisting of pictures (such as animation) and words (such as narration) - offer a potentially powerful venue for improving student understanding. How can we use words and pictures to help people understand how scientific systems work, such as how a lightning storm develops, how the human respiratory system operates, or how a bicycle the pump works? This paper presents a cognitive theory of multimedia learning which draws on dual coding theory, cognitive load theory, and constructivist learning theory. Based on the theory, principles of instructional design for fostering multimedia learning are derived and tested. The multiple representation principle states that it is better to present an explanation in words and pictures than solely in words. The contiguity principle is that it is better to present corresponding words and pictures simultaneously rather than separately when giving a multimedia explanation. The coherence principle is that multimedia explanations are better understood when they include few rather than many extraneous words and sounds. The modality principle is that it is better to present words as auditory narration than as visual on-screen text. The redundancy principle is that it is better to present animation and narration than to present animation, narration, and on-screen text. By beginning with a cognitive theory of how learners process multimedia information, we have been able to conduct focused research that yields some preliminary principles of instructional design for multimedia messages. (C) 2001 Elsevier Science Ltd. All rights reserved.',\n",
       " 'The promise of multimedia learning: using the same instructional design methods across different media. Multimedia learning occurs when students build mental representations from words and pictures that are presented to them (e.g., printed text and illustrations or narration and animation). The promise of multimedia learning is that students can learn more deeply from well-designed multimedia messages consisting of words and pictures than from more traditional modes of communication involving words alone. This article explores a program of research aimed at determining (a) research-based principles for the design of multimedia explanations--which can be called methods, and (b) the extent to which methods are effective across different learning environments--which can be called media. A review of research on the design of multimedia explanations conducted in our lab at Santa Barbara shows (a) a multimedia effect--in which students learn more deeply from words and pictures than from words alone--in both book-based and computer-based environments, (b) a coherence effect--in which students learn more deeply when extraneous material is excluded rather than included--in both book-based and computer-based environments, (c) a spatial contiguity effect--in which students learn more deeply when printed words are placed near rather than far from corresponding pictures--in both book-based and computer-based environments, and (d) a personalization effect--in which students learn more deeply when words are presented in conversational rather than formal style--both in computer-based environments containing spoken words and those using printed words. Overall, our results provide four examples in which the same instructional design methods are effective across different media.',\n",
       " 'Federated database systems for managing distributed, heterogeneous, and autonomous databases. A federated database system (FDBS) is a collection of cooperating database systems that are autonomous and possibly heterogeneous. In this paper, we define a reference architecture for distributed database management systems from system and schema viewpoints and show how various FDBS architectures can be developed. We then define a methodology for developing one of the popular architectures of an FDBS. Finally, we discuss critical issues related to developing and operating an FDBS.',\n",
       " 'Intelligent Agents: Theory and Practice. The concept of an \\\\\\\\emph{agent} has become important in both Artificial Intelligence (AI) and mainstream computer science. Our aim in this paper is to point the reader at what we perceive to be the most important theoretical and practical issues associated with the design and construction of intelligent agents. For convenience, we divide these issues into three areas (though as the reader will see, the divisions are at times somewhat arbitrary). \\\\\\\\emph{Agent theory} is concerned with the question of what an agent is, and the use of mathematical formalisms for representing and reasoning about the properties of agents. \\\\\\\\emph{Agent architectures} can be thought of as software engineering models of agents; researchers in this area are primarily concerned with the problem of designing software or hardware systems that will satisfy the properties specified by agent theorists. Finally, \\\\\\\\emph{agent languages} are software systems for programming and experimenting with agents; these languages may embody principles proposed by theorists. The paper is \\\\\\\\emph{not} intended to serve as a tutorial introduction to all the issues mentioned; we hope instead simply to identify the most important issues, and point to work that elaborates on them. The article includes a short review of current and potential applications of agent technology.',\n",
       " 'Protein flexibility in ligand docking and virtual screening to protein kinases.. The main complicating factor in structure-based drug design is receptor rearrangement upon ligand binding (induced fit). It is the induced fit that complicates cross-docking of ligands from different ligand-receptor complexes. Previous studies have shown the necessity to include protein flexibility in ligand docking and virtual screening. Very few docking methods have been developed to predict the induced fit reliably and, at the same time, to improve on discriminating between binders and non-binders in the virtual screening process. We present an algorithm called the ICM-flexible receptor docking algorithm (IFREDA) to account for protein flexibility in virtual screening. By docking flexible ligands to a flexible receptor, IFREDA generates a discrete set of receptor conformations, which are then used to perform flexible ligand-rigid receptor docking and scoring. This is followed by a merging and shrinking step, where the results of the multiple virtual screenings are condensed to improve the enrichment factor. In the IFREDA approach, both side-chain rearrangements and essential backbone movements are taken into consideration, thus sampling adequately the conformational space of the receptor, even in cases of large loop movements. As a preliminary step, to show the importance of incorporating protein flexibility in ligand docking and virtual screening, and to validate the merging and shrinking procedure, we compiled an extensive small-scale virtual screening benchmark of 33 crystal structures of four different protein kinases sub-families (cAPK, CDK-2, P38 and LCK), where we obtained an enrichment factor fold-increase of 1.85+/-0.65 using two or three multiple experimental conformations. IFREDA was used in eight protein kinase complexes and was able to find the correct ligand conformation and discriminate the correct conformations from the \"misdocked\" conformations solely on the basis of energy calculation. Five of the generated structures were used in the small-scale virtual screening stage and, by merging and shrinking the results with those of the original structure, we show an enrichment factor fold increase of 1.89+/-0.60, comparable to that obtained using multiple experimental conformations. Our cross-docking tests on the protein kinase benchmark underscore the necessity of incorporating protein flexibility in both ligand docking and virtual screening. The methodology presented here will be extremely useful in cases where few or no experimental structures of complexes are available, while some binders are known.',\n",
       " 'Glide:\\u2009 A New Approach for Rapid, Accurate Docking and Scoring. 1. Method and Assessment of Docking Accuracy. PMID: 15027865 Unlike other methods for docking ligands to the rigid 3D structure of a known protein receptor, Glide approximates a complete systematic search of the conformational, orientational, and positional space of the docked ligand. In this search, an initial rough positioning and scoring phase that dramatically narrows the search space is followed by torsionally flexible energy optimization on an OPLS-AA nonbonded potential grid for a few hundred surviving candidate poses. The very best candidates are further refined via a Monte Carlo sampling of pose conformation; in some cases, this is crucial to obtaining an accurate docked pose. Selection of the best docked pose uses a model energy function that combines empirical and force-field-based terms. Docking accuracy is assessed by redocking ligands from 282 cocrystallized PDB complexes starting from conformationally optimized ligand geometries that bear no memory of the correctly docked pose. Errors in geometry for the top-ranked pose are less than 1 Å in nearly half of the cases and are greater than 2 Å in only about one-third of them. Comparisons to published data on rms deviations show that Glide is nearly twice as accurate as GOLD and more than twice as accurate as FlexX for ligands having up to 20 rotatable bonds. Glide is also found to be more accurate than the recently described Surflex method.',\n",
       " 'Semantic integration: a survey of ontology-based approaches. Semantic integration is an active area of research in several disciplines, such as databases, information-integration, and ontologies. This paper provides a brief survey of the approaches to semantic integration developed by researchers in the ontology community. We focus on the approaches that differentiate the ontology research from other related areas. The goal of the paper is to provide a reader who may not be very familiar with ontology research with introduction to major themes in this research and with pointers to different research projects. We discuss techniques for finding correspondences between ontologies, declarative ways of representing these correspondences, and use of these correspondences in various semantic-integration tasks 1. ONTOLOGIES AND SEMANTIC INTE-',\n",
       " 'Power-Law Distribution of the World Wide Web. of links is hardly surprising; all sites are not created equal. An exciting site that appears in 1999 will soon have more links than a bland site created in 1993. The rate of acquisition of new links is probably proportional to the number of links the site already has, because the more links a site has, the more visible it becomes and the more new links it will get. (There should, however, be an additional proportionality factor, or growth rate, that varies from site to site.)  Our recently...',\n",
       " 'Emergent semantics. The article discusses ways to let semantics emerge from simple observations from the bottom-up, rather than imposing concepts on the observations top-down, to provide precise query, retrieval, communication or translation for a wide variety of applications. The following areas are examined: image retrieval and databases; media information spaces including the Semantic Web and MPEG frameworks; language games for emergent semantics; and emergent semantics for ontologies',\n",
       " \"Automatic Meaning Discovery Using Google. We propose a new method to extract semantic knowledge from the world-wide-web for both supervised and unsupervised learning using the Google search engine in an unconventional manner. The approach is novel in its unrestricted problem domain, simplicity of implementation, and manifestly ontological underpinnings. We give evidence of elementary learning of the semantics of concepts, in contrast to most prior approaches. The method works as follows: The world-wide-web is the largest database on earth, and it induces a probability mass function, the Google distribution, via page counts for combinations of search queries. This distribution allows us to tap the latent semantic knowledge on the web. Shannon's coding theorem is used to establish a code-length associated with each search query. Viewing this mapping as a data compressor, we connect to earlier work on Normalized Compression Distance. We give applications in (i) unsupervised hierarchical clustering, demonstrating the ability to distinguish between colors and numbers, and to distinguish between 17th century Dutch painters; (ii) supervised concept-learning by example, using Support Vector Machines, demonstrating the ability to understand electrical terms, religious terms, emergency incidents, and by conducting a massive experiment in understanding WordNet categories; and (iii) matching of meaning, in an example of automatic English-Spanish translation.\",\n",
       " 'The open archives initiative: building a low-barrier interoperability framework. The Open Archives Initiative (OAI) develops and promotes  interoperability solutions that aim to facilitate the efficient  dissemination of content. The roots of the OAI lie in the E-Print  community. Over the last year its focus has been extended to  include all content providers. This paper describes the recent  history of the OAI -- its origins in promoting E-Prints, the  broadening of its focus, the details of its technical standard for  metadata harvesting, the applications of this...',\n",
       " \"Microarray analysis shows that some microRNAs downregulate large numbers of target mRNAs.. MicroRNAs (miRNAs) are a class of noncoding RNAs that post-transcriptionally regulate gene expression in plants and animals. To investigate the influence of miRNAs on transcript levels, we transfected miRNAs into human cells and used microarrays to examine changes in the messenger RNA profile. Here we show that delivering miR-124 causes the expression profile to shift towards that of brain, the organ in which miR-124 is preferentially expressed, whereas delivering miR-1 shifts the profile towards that of muscle, where miR-1 is preferentially expressed. In each case, about 100 messages were downregulated after 12 h. The 3' untranslated regions of these messages had a significant propensity to pair to the 5' region of the miRNA, as expected if many of these messages are the direct targets of the miRNAs. Our results suggest that metazoan miRNAs can reduce the levels of many of their target transcripts, not just the amount of protein deriving from these transcripts. Moreover, miR-1 and miR-124, and presumably other tissue-specific miRNAs, seem to downregulate a far greater number of targets than previously appreciated, thereby helping to define tissue-specific gene expression in humans.\",\n",
       " 'Characterizing browsing strategies in the World-Wide Web. This paper presents the results of a study conducted at Georgia Institute of Technology that captured client-side user events of NCSA&#039;s XMosaic. Actual user behavior, as determined from clientside log file analysis, supplemented our understanding of user navigation strategies as well as provided real interface usage data. Log file analysis also yielded design and usability suggestions for WWW pages, sites and browsers. The methodology of the study and findings are discussed along with future research directions.  Keywords  Hypertext Navigation, Log Files, User Modeling  Introduction  With the prolific growth of the World-Wide Web (WWW) [Berners-Lee et.al, 1992] in the past year there has been an increased demand for an understanding of the WWW audience. Several studies exist that determine demographics and some behavioral characteristics of WWW users via selfselection [Pitkow and Recker 1994a &amp; 1994b]. Though highly informative, such studies only provide high level trends in Web use (e...',\n",
       " 'Supporting Social Navigation on the World-Wide Web. This paper discusses a navigation behavior on Internet information services, in particular the World Wide Web, which is characterized by pointing out of information using various communication tools. We call this behavior social navigation as it is based on communication and interaction with other users, be that through email, or any other means of communication. Social navigation phenomena are quite common although most current tools (like Web browsers or email clients) offer very little...',\n",
       " 'Using information scent to model user information needs and actions and the Web. On the Web, users typically forage for information by navigating from page to page along Web links. Their surfing patterns or actions are guided by their information needs. Researchers need tools to explore the complex interactions between user needs, user actions, and the structures and contents of the Web. In this paper, we describe two computational methods for understanding the relationship between user needs and user actions. First, for a particular pattern of surfing, we seek to infer the associated information need. Second, given an information need, and some pages as starting pints, we attempt to predict the expected surfing patterns. The algorithms use a concept called &ldquo;information scent&rdquo;, which is the subjective sense of value and cost of accessing a page based on perceptual cues. We present an empirical evaluation of these two algorithms, and show their effectiveness.',\n",
       " \"The scent of a site: a system for analyzing and predicting information scent, usage, and usability of a Web site. Designers and researchers of users' interactions with the World Wide Web need tools that permit the rapid exploration of hypotheses about complex interactions of user goals, user behaviors, and Web site designs. We present an architecture and system for the analysis and prediction of user behavior and Web site usability. The system integrates research on human information foraging theory, a reference model of information visualization and Web data-mining techniques. The system also incorporates new methods of Web site visualization (Dome Tree, Usage Based Layouts), a new predictive modeling technique for Web site use (Web User Flow by Information Scent, WUFIS), and new Web usability metrics.\",\n",
       " 'Composition Patterns: An Approach to Designing Reusable Aspects. Requirements such as distribution or tracing have an impact on multiple classes in a system. They are cross-cutting requirements, or aspects. Their support is, by necessity, scattered across those multiple classes. A look at an individual class may also show support for cross-cutting requirements tangled up with the core responsibilities of that class. Scattering and tangling make object-oriented software difficult to understand, extend and reuse. Though design is an important activity within the software lifecycle with well-documented benefits, those benefits are reduced when cross-cutting requirements are present. This paper presents a means to mitigate these problems by separating the design of cross-cutting requirements into composition patterns. Composition patterns require extensions to the UML, and are based on a combination of the subject-oriented model for composing separate, overlapping designs, and UML templates. This paper also demonstrates how composition patterns map to one programming model that provides a solution for separation of cross-cutting requirements in code-aspect-oriented programming. This mapping serves to illustrate that separation of aspects may be maintained throughout the software lifecycle.',\n",
       " \"Early Aspects: A Model for Aspect-Oriented Requirements Engineering. Effective RE must reconcile the need to achieve separation of concerns with the need to satisfy broadly scoped requirements and constraints. Techniques such as use cases and viewpoints help achieve separation of stakeholders' concerns but ensuring their consistency with global requirements and constraints is largely unsupported. We build on recent work that has emerged from the aspect-oriented programming (AOP) community to propose a general model for aspect oriented requirements engineering (AORE). The model supports separation of crosscutting functional and non-functional properties at the requirements level. We argue that early separation of such crosscutting properties supports effective determination of their mapping and influence on artefacts at later development stages. A realisation of the model based on a case study of a toll collection system is presented.\",\n",
       " \"Labeling images with a computer game. We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.\",\n",
       " \"A representation of the hazard rate of elapsed time in macaque area LIP.. The capacity to anticipate the timing of environmental cues allows us to allocate sensory resources at the right time and prepare actions. Such anticipation requires knowledge of elapsed time and of the probability that an event will occur. Here we show that neurons in the parietal cortex represent the probability, as a function of time, that a salient event is likely to occur. Rhesus monkeys were trained to make eye movements to peripheral targets after a light dimmed. Within a block of trials, the 'go' times were drawn from either a bimodal or unimodal distribution of random numbers. Neurons in the lateral intraparietal area showed anticipatory activity that revealed an internal representation of both elapsed time and the probability that the 'go' signal was about to occur (termed the hazard rate). The results indicate that the parietal cortex contains circuitry for representing the time structure of environmental cues over a range of seconds.\",\n",
       " 'Matching Behavior and the Representation of Value in the Parietal Cortex. Psychologists and economists have long appreciated the contribution of reward history and expectation to decision-making. Yet we know little about how specific histories of choice and reward lead to an internal representation of the \"value\" of possible actions. We approached this problem through an integrated application of behavioral, computational, and physiological techniques. Monkeys were placed in a dynamic foraging environment in which they had to track the changing values of alternative choices through time. In this context, the monkeys\\' foraging behavior provided a window into their subjective valuation. We found that a simple model based on reward history can duplicate this behavior and that neurons in the parietal cortex represent the relative value of competing actions predicted by this model.',\n",
       " 'Markov Games as a Framework for Multi-Agent Reinforcement Learning. 2 DEFINITIONS In the Markov decision process (MDP) formaliza-An MDP [Howard, 1960] is defined by a set of states, tion of reinforcement learning, a single adaptive S, and actions,A. A transition function,T:SA! agent interacts with an environment defined by a PD(S), defines the effects of the various actions on the probabilistic transition function. In this solipsis-state of the environment. (PD(S)represents the set of tic view, secondary agents can only be part of the discrete probability distributions over the setS.) The reward environment and are therefore fixed in their be-function,R:SA!&lt;, specifies the agent’s task. havior. The framework of Markov games allows In broad terms, the agent’s objective is to find a policy us to widen this view to include multiple adap-mapping its interaction history to a current choice of action tive agents with interacting or competing goals. so as to maximize the expected sum of discounted reward, This paper considers a step in this direction in EfP1j=0jrt+jg, wherert+jis the reward receivedjsteps which exactly two agents with diametrically op-into the future. A discount factor, 0&lt;1 controls how posed goals share an environment. It describes much effect future rewards have on the optimal decisions, a Q-learning-like algorithm for finding optimal with small values of emphasizing near-term gain and policies and demonstrates its application to a sim-larger values giving significant weight to later rewards. ple two-player game in which the optimal policy is probabilistic. In its general form, a Markov game, sometimes called a stochastic game [Owen, 1982], is defined by a set of states, S, and a collection of action sets,A1;:::;Ak, one for each 1',\n",
       " 'Learning to Predict by the Methods of Temporal Differences. This article introduces a class of incremental learning procedures specialized for prediction – that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel&#039;s checker player, Holland&#039;s bucket brigade, and the author&#039;s Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.',\n",
       " 'Model-Driven Data Acquisition in Sensor Networks. Declarative queries are proving to be an attractive paradigm for ineracting with networks of wireless sensors. The metaphor that \"the sensornet is a database\" is problematic, however, because sensors do not exhaustively represent the data in the real world. In order to map the raw sensor readings onto physical reality, a model of that reality is required to complement the readings. In this paper, we enrich interactive sensor querying with statistical modeling techniques. We demonstrate that such models can help provide answers that are both more meaningful, and, by introducing approximations with probabilistic confidences, significantly more efficient to compute in both time and energy. Utilizing the combination of a model and live data acquisition raises the challenging optimization problem of selecting the best sensor readings to acquire, balancing the increase in the confidence of our answer against the communication and data acquisition costs in the network. We describe an exponential time algorithm for finding the optimal solution to this optimization problem, and a polynomial-time heuristic for identifying solutions that perform well in practice. We evaluate our approach on several real-world sensor-network data sets, taking into account the real measured data and communication quality, demonstrating that our model-based approach provides a high-fidelity representation of the real phenomena and leads to significant performance gains versus traditional data acquisition techniques.',\n",
       " 'Complex queries in dht-based peer-to-peer networks. Recently a new generation of P2P systems, offering distributed hash table (DHT) functionality, have been proposed. These systems greatly improve the scalability and exact-match accuracy of P2P systems, but offer only the exact-match query facility. This paper outlines a research agenda for building complex query facilities on top of these DHT-based P2P systems. We describe the issues involved and outline our research plan and current status.',\n",
       " 'Neural correlates of decision variables in parietal cortex.. Decision theory proposes that humans and animals decide what to do in a given situation by assessing the relative value of each possible response. This assessment can be computed, in part, from the probability that each action will result in a gain and the magnitude of the gain expected. Here we show that the gain (or reward) a monkey can expect to realize from an eye-movement response modulates the activity of neurons in the lateral intraparietal area, an area of primate cortex that is thought to transform visual signals into eye-movement commands. We also show that the activity of these neurons is sensitive to the probability that a particular response will result in a gain. When animals can choose freely between two alternative responses, the choices subjects make and neuronal activation in this area are both correlated with the relative amount of gain that the animal can expect from each response. Our data indicate that a decision-theoretic model may provide a powerful new framework for studying the neural processes that intervene between sensation and action.',\n",
       " \"Internet indirection infrastructure. Attempts to generalize the Internet's point-to-point communication abstraction to provide services like multicast, anycast, and mobility have faced challenging technical problems and deployment barriers. To ease the deployment of such services, this paper proposes a general, overlay-based Internet Indirection Infrastructure ( i 3) that offers a rendezvous-based communication abstraction. Instead of explicitly sending a packet to a destination, each packet is associated with an identifier; this identifier is then used by the receiver to obtain delivery of the packet. This level of indirection decouples the act of sending from the act of receiving, and allows  i 3 to efficiently support a wide variety of fundamental communication services. To demonstrate the feasibility of this approach, we have designed and built a prototype based on the Chord lookup protocol.\",\n",
       " \"Self: The power of simplicity. Self is a new object-oriented language for exploratory programming based on a small number of simple and concrete ideas: prototypes, slots, and behavior. Prototypes combine inheritance and instantiation to provide a framework that is simpler and more flexible than most object-oriented languages. Slots unite variables and procedures into a single construct. This permits the inheritance hierarchy to take over the function of lexical scoping in conventional languages. Finally, because Self does not distinguish state from behavior, it narrows the gaps between ordinary objects, procedures, and closures. Self's simplicity and expressiveness offer new insights into object-oriented computation.\",\n",
       " 'Neural computations that underlie decisions about sensory stimuli.. Decision-making behavior has been studied extensively, but the neurophysiological mechanisms responsible for this remarkable cognitive ability are just beginning to be understood. Here we propose neural computations that can account for the formation of categorical decisions about sensory stimuli by accumulating information over time into a single quantity: the logarithm of the likelihood ratio favoring one alternative over another. We also review electrophysio-logical studies that have identified brain structures that may be involved in computing this sort of decision variable. The ideas presented constitute a framework for understanding how and where perceptual decisions are formed in the brain.',\n",
       " 'The Blocker Tag: Selective Blocking of RFID Tags for Consumer Privacy. We propose the use of \"selective blocking\" by \"blocker tags\" as a way of protecting consumers from unwanted scanning of RFID tags attached to items they may be carrying or wearing. While an ordinary RFID tag is a simple, cheap (e.g. five-cent) passive device intended as an \"electronic bar-code\" for use in supply-chain management, a blocker tag is a cheap passive RFID device that can simulate many ordinary RFID tags simultaneously. When carried by a consumer, a blocker tag thus \"blocks\" RFID readers. It can do so universally by simulating all possible RFID tags. Or a blocker tag can block selectively by simulating only selected subsets of ID codes, such as those by a particular manufacturer, or those in a designated \"privacy zone.\" We believe that this approach, when used with appropriate care, provides a very attractive alternative for addressing privacy concerns raised by the potential (and likely) widespread use of RFID tags in consumer products. We also discuss possible abuses arising from blocker tags, and means for detecting and dealing with them. Copyright 2003 ACM.',\n",
       " \"An introduction to quantum computing for non-physicists. Richard Feynman's observation that certain quantum mechanical effects cannot be simulated efficiently on a computer led to speculation that computation in general could be done more efficiently if it used these quantum effects. This speculation proved justified when Peter Shor described a polynomial time quantum algorithm for factoring intergers. In quantum systems, the computational space increases exponentially with the size of the system, which enables exponential parallelism. This parallelism could lead to exponentially faster quantum algorithms than possible classically. The catch is that accessing the results, which requires measurement, proves tricky and requires new nontraditional programming techniques. The aim of this paper is to guide computer  scientists through the barriers that separate quantum computing from conventional computing. We introduce basic principles of quantum mechanics to explain where the power of quantum computers comes from and why it is difficult to harness. We describe quantum cryptography, teleportation, and dense coding. Various approaches to exploiting the power of quantum parallelism are explained. We conclude with a discussion of quantum error correction.\",\n",
       " 'Spike Transmission and Synchrony Detection in Networks of GABAergic Interneurons. The temporal pattern and relative timing of action potentials among neocortical neurons may carry important information. However, how cortical circuits detect or generate coherent activity remains unclear. Using paired recordings in rat neocortical slices, we found that the firing of fast-spiking cells can reflect the spiking pattern of single-axon pyramidal inputs. Moreover, this property allowed groups of fast-spiking cells interconnected by electrical and gamma -aminobutyric acid (GABA)-releasing (GABAergic) synapses to detect the relative timing of their excitatory inputs. These results indicate that networks of fast-spiking cells may play a role in the detection and promotion of synchronous activity within the neocortex.',\n",
       " 'Generalized Integrate-and-Fire Models of Neuronal Activity Approximate Spike Trains of a Detailed Model to a High Degree of Accuracy. We demonstrate that single-variable integrate-and-fire models can quantitatively capture the dynamics of a physiologically detailed model for fast-spiking cortical neurons. Through a systematic set of approximations, we reduce the conductance-based model to 2 variants of integrate-and-fire models. In the first variant (nonlinear integrate-and-fire model), parameters depend on the instantaneous membrane potential, whereas in the second variant, they depend on the time elapsed since the last spike [Spike Response Model (SRM)]. The direct reduction links features of the simple models to biophysical features of the full conductance-based model. To quantitatively test the predictive power of the SRM and of the nonlinear integrate-and-fire model, we compare spike trains in the simple models to those in the full conductance-based model when the models are subjected to identical randomly fluctuating input. For random current input, the simple models reproduce 70-80 percent of the spikes in the full model (with temporal precision of {+/-}2 ms) over a wide range of firing frequencies. For random conductance injection, up to 73 percent of spikes are coincident. We also present a technique for numerically optimizing parameters in the SRM and the nonlinear integrate-and-fire model based on spike trains in the full conductance-based model. This technique can be used to tune simple models to reproduce spike trains of real neurons.',\n",
       " 'Advances in dataflow programming languages. Many developments have taken place within dataflow programming languages in the past decade. In particular, there has been a great deal of activity and advancement in the field of dataflow visual programming languages. The motivation for this article is to review the content of these recent developments and how they came about. It is supported by an initial review of dataflow programming in the 1970s and 1980s that led to current topics of research. It then discusses how dataflow programming evolved toward a hybrid von Neumann dataflow formulation, and adopted a more coarse-grained approach. Recent trends toward dataflow visual programming languages are then discussed with reference to key graphical dataflow languages and their development environments. Finally, the article details four key open topics in dataflow programming languages.',\n",
       " 'Protein docking using a genetic algorithm. A genetic algorithm (GA) for protein-protein docking is described, in which the proteins are represented by dot surfaces calculated using the Connolly program. The GA is used to move the surface of one protein relative to the other to locate the area of greatest surface complementarity between the two. Surface dots are deemed complementary if their normals are opposed, their Connolly shape type is complementary, and their hydrogen bonding or hydrophobic potential is fulfilled. Overlap of the protein interiors is penalized. The GA is tested on 34 large protein-protein complexes where one or both proteins has been crystallized separately. Parameters are established for which 30 of the complexes have at least one near-native solution ranked in the top 100. We have also successfully reassembled a 1,400-residue heptamer based on the top-ranking GA solution obtained when docking two bound subunits. Proteins 2001;44:44-56. &copy; 2001 Wiley-Liss, Inc.',\n",
       " 'Building interpreters by composing monads. : We exhibit a set of functions coded in Haskell that can be used as building blocks to construct a variety of interpreters for Lisp-like languages. The building blocks are joined merely through functional composition. Each building block contributes code to support a specific feature, such as numbers, continuations, functions calls, or nondeterminism. The result of composing some number of building blocks is a parser, an interpreter, and a printer that support exactly the expression forms and...',\n",
       " 'Responses of neurons in primary and inferior temporal visual cortices to natural scenes.. The primary visual cortex (V1) is the first cortical area to receive visual input, and inferior temporal (IT) areas are among the last along the ventral visual pathway. We recorded, in area V1 of anaesthetized cats and area IT of awake macaque monkeys, responses of neurons to videos of natural scenes. Responses were analysed to test various hypotheses concerning the nature of neural coding in these two regions. A variety of spike-train statistics were measured including spike-count distributions, interspike interval distributions, coefficients of variation, power spectra, Fano factors and different sparseness measures. All statistics showed non-Poisson characteristics and several revealed self-similarity of the spike trains. Spike-count distributions were approximately exponential in both visual areas for eight different videos and for counting windows ranging from 50 ms to 5 seconds. The results suggest that the neurons maximize their information carrying capacity while maintaining a fixed long-term-average firing rate, or equivalently, minimize their average firing rate for a fixed information carrying capacity.',\n",
       " \"The notion of dimension in geometry and algebra. This talk reviews some mathematical and physical ideas related to the notion of dimension. After a brief historical introduction, various modern constructions from fractal geometry, noncommutative geometry, and theoretical physics are invoked and compared.  Glenn Gould disapproved of his own recording of Goldberg variations.  ``There is a lot of piano playing going on there, and I  mean that as the most disparaging comment possible.''  NYRB, Oct. 7, 2004, p. 10\",\n",
       " 'Analysis of the increase and decrease algorithms for congestion avoidance in computer networks. Congestion avoidance mechanisms allow a network to operate in the optimal region of low delay and high throughput, thereby, preventing the network from becoming congested. This is different from the traditional congestion control mechanisms that allow the network to recover from the congested state of high delay and low throughput. Both congestion avoidance and congestion control mechanisms are resource management problems in which the system senses its state and feeds this back to its users who adjust their controls. The key component of any congestion control avoidance scheme is the algorithm (or control function) used by the users to increase or decrease their load (window or rate). We abstractly characterize a wide class of such increase/decrease algorithms and compare them using several different performance metrics. The key metrics are efficiency, fairness, convergence time, and size of oscillations. It is shown that a simple additive increase and multiplicative decrease algorithm satisfies the sufficient conditions for convergence to an efficient and fair state regardless of the starting state of the network. This is the algorithm finally chosen for implementation in the congestion avoidance scheme recommended for Digital Networking Architecture and OSI Transport Class 4 Networks.',\n",
       " \"Congestion Avoidance and Control. In October of '86, the Internet had the first of what became a series of 'congestion collapses'. During this period, the data throughput from LBL to UC Berkeley (sites separated by 400 yards and three IMP hops) dropped from 32 Kbps to 40 bps. Mike Karels 1  and I were fascinated by this sudden factor-of-thousand drop in bandwidth and embarked on an investigation of why things had gotten so bad. We wondered, in particular, if the 4.3BSD (Berkeley UNIX) TCP was mis-behaving or if it could be tuned to work better under abysmal network conditions. The answer to both of these questions was &ldquo;yes&rdquo;.   Since that time, we have put seven new algorithms into the 4BSD TCP:    round-trip-time variance estimation   exponential retransmit timer backoff   slow-start   more aggressive receiver ack policy   dynamic window sizing on congestion   Karn's clamped retransmit backoff   fast retransmit Our measurements and the reports of beta testers suggest that the final product is fairly good at dealing with congested conditions on the Internet.     This paper is a brief description of ( i ) - ( v ) and the rationale behind them. ( vi ) is an algorithm recently developed by Phil Karn of Bell Communications Research, described in [KP87]. ( viii ) is described in a soon-to-be-published RFC.   Algorithms ( i ) - ( v ) spring from one observation: The flow on a TCP connection (or ISO TP-4 or Xerox NS SPP connection) should obey a 'conservation of packets' principle. And, if this principle were obeyed, congestion collapse would become the exception rather than the rule. Thus congestion control involves finding places that violate conservation and fixing them.   By 'conservation of packets' I mean that for a connection 'in equilibrium', i.e., running stably with a full window of data in transit, the packet flow is what a physicist would call 'conservative': A new packet isn't put into the network until an old packet leaves. The physics of flow predicts that systems with this property should be robust in the face of congestion. Observation of the Internet suggests that it was not particularly robust. Why the discrepancy?   There are only three ways for packet conservation to fail:    The connection doesn't get to equilibrium, or   A sender injects a new packet before an old packet has exited, or   The equilibrium can't be reached because of resource limits along the path. In the following sections, we treat each of these in turn.\",\n",
       " 'Principles of programming with complex objects and collection types. We present a new principle for the development of database query languages that the primitive operations should be organized around types. Viewing a relational database as consisting of sets of records, this principle dectates that we should investigate separately operations for records and sets. There are two immediate advantages of this approach, which is partly inspired by basic ideas from category theoryl. First, it provides a language for structures in which record and set types may be freely combined: nested relations or complex objects. Second, the fundamental operations for sets are closely related to those for other \"collection types\" such as bags or lists, and this suggests how database languages may be uniformly extended to these new types. the most general operation on sets, that of structural recursion, is one in which not all programs are well-defined. In looking for limited forms of this operation that always give rise to well-defined operations, we find a number of close connection with exiting database languages, notably those developed for complex objects. Moreover, even though the general paradigm of structural recursion is shown to be no more expressive than one of the existing languages for complex objects, it possesses certain properties of uniformity that make it a better candidate for an efficient, practical language. Thus rather than developing query languages by extending, for example, relational calculus, we advocate a very powerful paradigm in which a number of well-known languages are to be found as natural sublanguages.',\n",
       " 'A neuroeconomics approach to inferring utility functions in sensorimotor control.. Making choices is a fundamental aspect of human life. For over a century experimental economists have characterized the decisions people make based on the concept of a utility function. This function increases with increasing desirability of the outcome, and people are assumed to make decisions so as to maximize utility. When utility depends on several variables, indifference curves arise that represent outcomes with identical utility that are therefore equally desirable. Whereas in economics utility is studied in terms of goods and services, the sensorimotor system may also have utility functions defining the desirability of various outcomes. Here, we investigate the indifference curves when subjects experience forces of varying magnitude and duration. Using a two-alternative forced-choice paradigm, in which subjects chose between different magnitude–duration profiles, we inferred the indifference curves and the utility function. Such a utility function defines, for example, whether subjects prefer to lift a 4-kg weight for 30 s or a 1-kg weight for a minute. The measured utility function depends nonlinearly on the force magnitude and duration and was remarkably conserved across subjects. This suggests that the utility function, a central concept in economics, may be applicable to the study of sensorimotor control.',\n",
       " 'Position specific variation in the rate of evolution in transcription factor binding sites. BACKGROUND: The binding sites of sequence specific transcription factors are an important and relatively well-understood class of functional non-coding DNAs. Although a wide variety of experimental and computational methods have been developed to characterize transcription factor binding sites, they remain difficult to identify. Comparison of non-coding DNA from related species has shown considerable promise in identifying these functional non-coding sequences, even though relatively little is known about their evolution. RESULTS: Here we analyse the genome sequences of the budding yeasts Saccharomyces cerevisiae, S. bayanus, S. paradoxus and S. mikatae to study the evolution of transcription factor binding sites. As expected, we find that both experimentally characterized and computationally predicted binding sites evolve slower than surrounding sequence, consistent with the hypothesis that they are under purifying selection. We also observe position-specific variation in the rate of evolution within binding sites. We find that the position-specific rate of evolution is positively correlated with degeneracy among binding sites within S. cerevisiae. We test theoretical predictions for the rate of evolution at positions where the base frequencies deviate from background due to purifying selection and find reasonable agreement with the observed rates of evolution. Finally, we show how the evolutionary characteristics of real binding motifs can be used to distinguish them from artefacts of computational motif finding algorithms. CONCLUSION: As has been observed for protein sequences, the rate of evolution in transcription factor binding sites varies with position, suggesting that some regions are under stronger functional constraint than others. This variation likely reflects the varying importance of different positions in the formation of the protein-DNA complex. The characterization of the pattern of evolution in known binding sites will likely contribute to the effective use of comparative sequence data in the identification of transcription factor binding sites and is an important step toward understanding the evolution of functional non-coding DNA.',\n",
       " 'Interaction network containing conserved and essential protein complexes in Escherichia coli.. Proteins often function as components of multi-subunit complexes. Despite its long history as a model organism, no large-scale analysis of protein complexes in Escherichia coli has yet been reported. To this end, we have targeted DNA cassettes into the E. coli chromosome to create carboxy-terminal, affinity-tagged alleles of 1,000 open reading frames (approximately 23% of the genome). A total of 857 proteins, including 198 of the most highly conserved, soluble non-ribosomal proteins essential in at least one bacterial species, were tagged successfully, whereas 648 could be purified to homogeneity and their interacting protein partners identified by mass spectrometry. An interaction network of protein complexes involved in diverse biological processes was uncovered and validated by sequential rounds of tagging and purification. This network includes many new interactions as well as interactions predicted based solely on genomic inference or limited phenotypic data. This study provides insight into the function of previously uncharacterized bacterial proteins and the overall topology of a microbial interaction network, the core components of which are broadly conserved across Prokaryota.',\n",
       " 'Metagenes and molecular pattern discovery using matrix factorization. 10.1073/pnas.0308531101 We describe here the use of nonnegative matrix factorization (NMF), an algorithm based on decomposition by parts that can reduce the dimension of expression data from thousands of genes to a handful of metagenes. Coupled with a model selection mechanism, adapted to work for any stochastic clustering algorithm, NMF is an efficient method for identification of distinct molecular patterns and provides a powerful method for class discovery. We demonstrate the ability of NMF to recover meaningful biological information from cancer-related microarray data. NMF appears to have advantages over other methods such as hierarchical clustering or self-organizing maps. We found it less sensitive to a priori selection of genes or initial conditions and able to detect alternative or context-dependent patterns of gene expression in complex biological systems. This ability, similar to semantic polysemy in text, provides a general method for robust molecular pattern discovery.',\n",
       " 'Refractoriness and Neural Precision. The response of a spiking neuron to a stimulus is often characterized by its time-varying firing rate, estimated from a histogram of spike times. If the cell\\'s firing probability in each small time interval depends only on this firing rate, one predicts a highly variable response to repeated trials, whereas many neurons show much greater fidelity. Furthermore, the neuronal membrane is refractory immediately after a spike, so that the firing probability depends not only on the stimulus but also on the preceding spike train. To connect these observations, we investigated the relationship between the refractory period of a neuron and its firing precision. The light response of retinal ganglion cells was modeled as probabilistic firing combined with a refractory period: the instantaneous firing rate is the product of a \"free firing rate, \" which depends only on the stimulus, and a \"recovery function,\" which depends only on the time since the last spike. This recovery function vanishes for an absolute refractory period and then gradually increases to unity. In simulations, longer refractory periods were found to make the response more reproducible, eventually matching the precision of measured spike trains. Refractoriness, although often thought to limit the performance of neurons, may in fact benefit neuronal reliability. The underlying free firing rate derived by allowing for the refractory period often exceeded the observed firing rate by an order of magnitude and was found to convey information about the stimulus over a much wider dynamic range. Thus, the free firing rate may be the preferred variable for describing the response of a spiking neuron.',\n",
       " \"Maximum likelihood estimation of a stochastic integrate-and-fire neural encoding model.. We examine a cascade encoding model for neural response in which a linear filtering stage is followed by a noisy, leaky, integrate-and-fire spike generation mechanism. This model provides a biophysically more realistic alternative to models based on Poisson (memoryless) spike generation, and can effectively reproduce a variety of spiking behaviors seen in vivo. We describe the maximum likelihood estimator for the model parameters, given only extracellular spike train responses (not intracellular voltage data). Specifically, we prove that the log-likelihood function is concave and thus has an essentially unique global maximum that can be found using gradient ascent techniques. We develop an efficient algorithm for computing the maximum likelihood solution, demonstrate the effectiveness of the resulting estimator with numerical simulations, and discuss a method of testing the model's validity using time-rescaling and density evolution techniques.\",\n",
       " 'A Spike-Train Probability Model. Poisson processes usually provide adequate descriptions of the irregularity in neuron spike times after pooling the data across large numbers of trials, as is done in constructing the peristimulus time histogram. When probabilities are needed to describe the behavior of neurons within individual trials, however, Poisson process models are often inadequate. In principle, an explicit formula gives the probability density of a single spike train in great generality, but without additional assumptions, the firing-rate intensity function appearing in that formula cannot be estimated. We propose a simple solution to this problem, which is to assume that the time at which a neuron fires is determined probabilistically by, and only by, two quantities: the experimental clock time and the elapsed time since the previous spike. We show that this model can be fitted with standard methods and software and that it may used successfully to fit neuronal data.',\n",
       " \"The Time-Rescaling Theorem and Its Application to Neural Spike Train Data Analysis. doi: 10.1162/08997660252741149 Measuring agreement between a statistical model and a spike train data series, that is, evaluating goodness of fit, is crucial for establishing the model's validity prior to using it to make inferences about a particular neural system. Assessing goodness-of-fit is a challenging problem for point process neural spike train models, especially for histogram-based models such as perstimulus time histograms (PSTH) and rate functions estimated by spike train smoothing. The time-rescaling theorem is a well-known result in probability theory, which states that any point process with an integrable conditional intensity function may be transformed into a Poisson process with unit rate. We describe how the theorem may be used to develop goodness-of-fit tests for both parametric and histogram-based point process models of neural spike trains. We apply these tests in two examples: a comparison of PSTH, inhomogeneous Poisson, and inhomogeneous Markov interval models of neural spike trains from the supplementary eye field of a macque monkey and a comparison of temporal and spatial smoothers, inhomogeneous Poisson, inhomogeneous gamma, and inhomogeneous inverse gaussian models of rat hippocampal place cell spiking activity. To help make the logic behind the time-rescaling theorem more accessible to researchers in neuroscience, we present a proof using only elementary probability theory arguments.We also show how the theorem may be used to simulate a general point process model of a spike train. Our paradigm makes it possible to compare parametric and histogram-based neural spike train models directly. These results suggest that the time-rescaling theorem can be a valuable tool for neural spike train data analysis.\",\n",
       " 'Multi-Robot Task Allocation: Analyzing the Complexity and Optimality of Key Architectures. Important theoretical aspects of multi-robot coordination mechanisms have, to date, been largely ignored. To address part of this negligence, we focus on the problem of multi-robot task allocation. We give a formal, domain-independent, statement of the problem and show it to be an instance of another, well-studied, optimization problem. In this light, we analyze several recently proposed approaches to multi-robot task allocation, describing their fundamental characteristics in such a way that they can be objectively studied, compared, and evaluated.',\n",
       " 'Studying Cooperation and Conflict between Authors. The Internet has fostered an unconventional and powerful style of collaboration: wiki web sites, where every visitor has the power to become an editor. In this paper we investigate the dynamics of Wikipedia, a prominent, thriving wiki. We make three contributions. First, we introduce a new exploratory data analysis tool, the history flow visualization, which is effective in revealing patterns within the wiki context and which we believe will be useful in other collaborative situations as...',\n",
       " 'i d e a n t: A del.icio.us study. Working within the constraints of a very limited data sample, this study attempts to identify some of the information management and meaning construction practices of an online distributed classification (a.k.a. free tagging or ethnoclassification) community. Specifically, this study seeks to investigate the social and communicative practices that emerge when users are encouraged to share web links with one another by using a metadata keyword, or tag, to demark a social group, apart from using other tags to classify links according to an emergent taxonomy.',\n",
       " \"Activity in posterior parietal cortex is correlated with the relative subjective desirability of action.. Behavioral studies suggest that making a decision involves representing the overall desirability of all available actions and then selecting that action that is most desirable. Physiological studies have proposed that neurons in the parietal cortex play a role in selecting movements for execution. To test the hypothesis that these parietal neurons encode the subjective desirability of making particular movements, we exploited Nash's game theoretic equilibrium, during which the subjective desirability of multiple actions should be equal for human players. Behavior measured during a strategic game suggests that monkeys' choices, like those of humans, are guided by subjective desirability. Under these conditions, activity in the parietal cortex was correlated with the relative subjective desirability of actions irrespective of the specific combination of reward magnitude, reward probability, and response probability associated with each action. These observations may help place many recent findings regarding the posterior parietal cortex into a common conceptual framework.\",\n",
       " 'Representation of time by neurons in the posterior parietal cortex of the macaque.. The neural basis of time perception is unknown. Here we show that neurons in the posterior parietal cortex (area LIP) represent elapsed time relative to a remembered duration. We trained rhesus monkeys to report whether the duration of a test light was longer or shorter than a remembered \"standard\" (316 or 800 ms) by making an eye movement to one of two choice targets. While timing the test light, the responses of LIP neurons signaled changes in the monkey\\'s perception of elapsed time. The variability of the neural responses explained the monkey\\'s uncertainty about its temporal judgments. Thus, in addition to their role in spatial processing and sensorimotor integration, posterior parietal neurons encode signals related to the perception of time.',\n",
       " \"So How Does the Mind Work?. In my book How the Mind Works, I defended the theory that the human mind is a naturally selected system of organs of computation. Jerry Fodor claims that 'the mind doesn't work that way'(in a book with that title) because (1) Turing Machines cannot duplicate humans' ability to perform abduction (inference to the best explanation); (2) though a massively modular system could succeed at abduction, such a system is implausible on other grounds; and (3) evolution adds nothing to our understanding of the mind. In this review I show that these arguments are flawed. First, my claim that the mind is a computational system is different from the claim Fodor attacks (that the mind has the architecture of a Turing Machine); therefore the practical limitations of Turing Machines are irrelevant. Second, Fodor identifies abduction with the cumulative accomplishments of the scientific community over millennia. This is very different from the accomplishments of human common sense, so the supposed gap between human cognition and computational models may be illusory. Third, my claim about biological specialization, as seen in organ systems, is distinct from Fodor's own notion of encapsulated modules, so the limitations of the latter are irrelevant. Fourth, Fodor's arguments dismissing of the relevance of evolution to psychology are unsound.\",\n",
       " 'Featherweight Java: A Minimal Core Calculus for Java and GJ. Several recent studies have introduced lightweight versions of Java: reduced languages in which complex features like threads and reflection are dropped to enable rigorous arguments about key properties such as type safety. We carry this process a step further, omitting almost all features of the full language (including interfaces and even assignment) to obtain a small calculus, Featherweight Java, for which rigorous proofs are not only possible but easy. Featherweight Java bears a similar relation to Java as the lambda-calculus does to languages such as ML and Haskell. It offers a similar computational \"feel,\" providing classes, methods, fields, inheritance, and dynamic typecasts with a semantics closely following Java\\'s. A proof of type safety for Featherweight Java thus illustrates many of the interesting features of a safety proof for the full language, while remaining pleasingly compact. The minimal syntax, typing rules, and operational semantics of Featherweight Java make it a handy tool for studying the consequences of extensions and variations. As an illustration of its utility in this regard, we extend Featherweight Java with  generic classes  in the style of GJ (Bracha, Odersky, Stoutamire, and Wadler) and give a detailed proof of type safety. The extended system formalizes for the first time some of the key features of GJ.',\n",
       " 'Stochastic nature of precisely timed spike patterns in visual system neuronal responses.. It is not clear how information related to cognitive or psychological processes is carried by or represented in the responses of single neurons. One provocative proposal is that precisely timed spike patterns play a role in carrying such information. This would require that these spike patterns have the potential for carrying information that would not be available from other measures such as spike count or latency. We examined exactly timed (1-ms precision) triplets and quadruplets of spikes in the stimulus-elicited responses of lateral geniculate nucleus (LGN) and primary visual cortex (V1) neurons of the awake fixating rhesus monkey. Large numbers of these precisely timed spike patterns were found. Information theoretical analysis showed that the precisely timed spike patterns carried only information already available from spike count, suggesting that the number of precisely timed spike patterns was related to firing rate. We therefore examined statistical models relating precisely timed spike patterns to response strength. Previous statistical models use observed properties of neuronal responses such as the peristimulus time histogram, interspike interval, and/or spike count distributions to constrain the parameters of the model. We examined a new stochastic model, which unlike previous models included all three of these constraints and unlike previous models predicted the numbers and types of observed precisely timed spike patterns. This shows that the precise temporal structures of stimulus-elicited responses in LGN and V1 can occur by chance. We show that any deviation of the spike count distribution, no matter how small, from a Poisson distribution necessarily changes the number of precisely timed spike patterns expected in neural responses. Overall the results indicate that the fine temporal structure of responses can only be interpreted once all the coarse temporal statistics of neural responses have been taken into account.',\n",
       " \"Attention and the subjective expansion of time. During brief, dangerous events, such as car accidents and robberies, many people report that events seem to pass in slow motion, as if time had slowed down. We have measured a similar, although less dramatic, effect in response to unexpected, nonthreatening events. We attribute the subjective expansion of time to the engagement of attention and its influence on the amount of perceptual information processed. We term the effect time's subjective expansion (TSE) and examine here the objective temporal dynamics of these distortions. When a series of stimuli are shown in succession, the low-probability oddball stimulus in the series tends to last subjectively longer than the high-probability stimulus even when they last the same objective duration. In particular, (1) there is a latency of at least 120 msec between stimulus onset and the onset of TSE, which may be preceded by subjective temporal contraction; (2) there is a peak in TSE at which subjective time is particularly distorted at a latency of 225 msec after stimulus onset; and (3) the temporal dynamics of TSE are approximately the same in the visual and the auditory domains. Two control experiments (in which the methods of magnitude estimation and stimulus reproduction were used) replicated the temporal dynamics of TSE revealed by the method of constant stimuli, although the initial peak was not apparent with these methods. In addition, a third, control experiment (in which the method of single stimuli was used) showed that TSE in the visual domain can occur because of semantic novelty, rather than image novelty per se. Overall, the results support the view that attentional orienting underlies distortions in perceived duration.\",\n",
       " \"Response of neurons in the lateral intraparietal area during a combined visual discrimination reaction time task.. Decisions about the visual world can take time to form, especially when information is unreliable. We studied the neural correlate of gradual decision formation by recording activity from the lateral intraparietal cortex (area LIP) of rhesus monkeys during a combined motion-discrimination reaction-time task. Monkeys reported the direction of random-dot motion by making an eye movement to one of two peripheral choice targets, one of which was within the response field of the neuron. We varied the difficulty of the task and measured both the accuracy of direction discrimination and the time required to reach a decision. Both the accuracy and speed of decisions increased as a function of motion strength. During the period of decision formation, the epoch between onset of visual motion and the initiation of the eye movement response, LIP neurons underwent ramp-like changes in their discharge rate that predicted the monkey's decision. A steeper rise in spike rate was associated with stronger stimulus motion and shorter reaction times. The observations suggest that neurons in LIP integrate time-varying signals that originate in the extrastriate visual cortex, accumulating evidence for or against a specific behavioral response. A threshold level of LIP activity appears to mark the completion of the decision process and to govern the tradeoff between accuracy and speed of perception.\",\n",
       " 'Obol: integrating language and meaning in bio-ontologies. Ontologies are intended to capture and formalize a domain of knowledge. The ontologies comprising the Open Biological Ontologies {(OBO)} project, which includes the Gene Ontology {(GO),} are formalizations of various domains of biological knowledge. Ontologies within {OBO} typically lack computable definitions that serve to differentiate a term from other similar terms. The computer is unable to determine the meaning of a term, which presents problems for tools such as automated reasoners. Reasoners can be of enormous benefit in managing a complex ontology. {OBO} term names frequently implicitly encode the kind of definitions that can be used by computational tools, such as automated reasoners. The definitions encoded in the names are not easily amenable to computation, because the names are ostensibly natural language phrases designed for human users. These names are highly regular in their grammar, and can thus be treated as valid sentences in some formal or computable {language.With} a description of the rules underlying this formal language, term names can be parsed to derive computable definitions, which can then be reasoned over. This paper describes the effort to elucidate that language, called Obol, and the attempts to reason over the resulting definitions. The current implementation finds unique non-trivial definitions for around half of the terms in the {GO,} and has been used to find 223 missing relationships, which have since been added to the ontology. Obol has utility as an ontology maintenance tool, and as a means of generating computable definitions for a whole {ontology.The} software is available under an open-source license from: http://www.fruitfly. org/{\\\\\\\\textasciitilde}cjm/obol. Supplementary material for this article can be found at: http://www. interscience.wiley.com/jpages/1531-6912/suppmat.',\n",
       " 'Neural control of voluntary movement initiation.. When humans respond to sensory stimulation, their reaction times tend to be long and variable relative to neural transduction and transmission times. The neural processes responsible for the duration and variability of reaction times are not understood. Single-cell recordings in a motor area of the cerebral cortex in behaving rhesus monkeys (Macaca mulatta) were used to evaluate two alternative mathematical models of the processes that underlie reaction times. Movements were initiated if and only if the neural activity reached a specific and constant threshold activation level. Stochastic variability in the rate at which neural activity grew toward that threshold resulted in the distribution of reaction times. This finding elucidates a specific link between motor behavior and activation of neurons in the cerebral cortex.',\n",
       " \"Neural basis of a perceptual decision in the parietal cortex (area LIP) of the rhesus monkey.. We recorded the activity of single neurons in the posterior parietal cortex (area LIP) of two rhesus monkeys while they discriminated the direction of motion in random-dot visual stimuli. The visual task was similar to a motion discrimination task that has been used in previous investigations of motion-sensitive regions of the extrastriate cortex. The monkeys were trained to decide whether the direction of motion was toward one of two choice targets that appeared on either side of the random-dot stimulus. At the end of the trial, the monkeys reported their direction judgment by making an eye movement to the appropriate target. We studied neurons in LIP that exhibited spatially selective persistent activity during delayed saccadic eye movement tasks. These neurons are thought to carry high-level signals appropriate for identifying salient visual targets and for guiding saccadic eye movements. We arranged the motion discrimination task so that one of the choice targets was in the LIP neuron's response field (RF) while the other target was positioned well away from the RF. During motion viewing, neurons in LIP altered their firing rate in a manner that predicted the saccadic eye movement that the monkey would make at the end of the trial. The activity thus predicted the monkey's judgment of motion direction. This predictive activity began early in the motion-viewing period and became increasingly reliable as the monkey viewed the random-dot motion. The neural activity predicted the monkey's direction judgment on both easy and difficult trials (strong and weak motion), whether or not the judgment was correct. In addition, the timing and magnitude of the response was affected by the strength of the motion signal in the stimulus. When the direction of motion was toward the RF, stronger motion led to larger neural responses earlier in the motion-viewing period. When motion was away from the RF, stronger motion led to greater suppression of ongoing activity. Thus the activity of single neurons in area LIP reflects both the direction of an impending gaze shift and the quality of the sensory information that instructs such a response. The time course of the neural response suggests that LIP accumulates sensory signals relevant to the selection of a target for an eye movement.\",\n",
       " 'A Role for Neural Integrators in Perceptual Decision Making. Decisions based on uncertain information may benefit from an accumulation of information over time. We asked whether such an accumulation process may underlie decisions about the direction of motion in a random dot kinetogram. To address this question we developed a computational model of the decision process using ensembles of neurons whose spiking activity mimics neurons recorded in the extrastriate visual cortex (area MT or V5) and a sensorimotor association area of the parietal lobe (area LIP). The model instantiates the hypothesis that neurons in sensorimotor association areas compute the time integral of sensory signals from the visual cortex, construed as evidence for or against a proposition, and that the decision is made when the integrated evidence reaches a threshold. The model explains a variety of behavioral and physiological measurements obtained from monkeys. 10.1093/cercor/bhg097',\n",
       " 'Psychology and neurobiology of simple decisions.. Patterns of neural firing linked to eye movement decisions show that behavioral decisions are predicted by the differential firing rates of cells coding selected and nonselected stimulus alternatives. These results can be interpreted using models developed in mathematical psychology to model behavioral decisions. Current models assume that decisions are made by accumulating noisy stimulus information until sufficient information for a response is obtained. Here, the models, and the techniques used to test them against response-time distribution and accuracy data, are described. Such models provide a quantitative link between the time-course of behavioral decisions and the growth of stimulus information in neural firing data.',\n",
       " 'Neural basis of deciding, choosing and acting.. The ability and opportunity to make decisions and carry out effective actions in pursuit of goals is central to intelligent life. Recent research has provided significant new insights into how the brain arrives at decisions, makes choices, and produces and evaluates the consequences of actions. In fact, by monitoring or manipulating specific neurons, certain choices can now be predicted or manipulated.',\n",
       " 'Neural correlates of decision processes: neural and mental chronometry.. Recent studies aim to explain the duration and variability of behavioral reaction time in terms of neural processes. The time taken to make choices is occupied by at least two processes. Neurons in sensorimotor structures accumulate evidence that leads to alternative categorizations, while other neurons within these structures prepare and initiate overt responses. These distinct stages of stimulus encoding and response preparation support variable but flexible behavior.',\n",
       " \"How to Tell When Simpler, More Unified, or Less Ad Hoc Theories Will Provide More Accurate Predictions. Traditional analyses of the curve fitting problem maintain that the data do not indicate what form the fitted curve should take. Rather, this issue is said to be settled by prior probabilities, by simplicity, or by a background theory. In this paper, we describe a result due to Akaike [1973], which shows how the data can underwrite an inference concerning the curve's form based on an estimate of how predictively accurate it will be. We argue that this approach throws light on the theoretical virtues of parsimoniousness, unification, and non ad hocness, on the dispute about Bayesianism, and on empiricism and scientific realism. 10.1093/bjps/45.1.1\",\n",
       " \"Neural computation of log likelihood in control of saccadic eye movements.. The latency between the appearance of a visual target and the start of the saccadic eye movement made to look at it varies from trial to trial to an extent that is inexplicable in terms of ordinary 'physiological' processes such as synaptic delays and conduction velocities. An alternative interpretation is that it represents the time needed to decide whether a target is in fact present: decision processes are necessarily stochastic, because they depend on extracting information from noisy sensory signals. In one such model, the presence of a target causes a signal in a decision unit to rise linearly at a rate r from its initial value s0 until it reaches a fixed threshold theta, when a saccade is initiated. One can regard this decision signal as a neural estimate of the log likelihood of the hypothesis that the target is present, the threshold being the significance criterion or likelihood level at which the target is presumed to be present. Experiments manipulating the prior probability of the target's appearing confirm this notion: the latency distribution then changes in the way expected if s0 simply reflects the prior log likelihood of the stimulus.\",\n",
       " \"The influence of urgency on decision time.. A fruitful quantitative approach to understanding how the brain makes decisions has been to look at the time needed to make a decision, and how it is affected by factors such as the supply of information, or an individual's expectations. This approach has led to a model of decision-making, the LATER model, that is consistent with recent neurophysiological data. The LATER model explains the observed variability of reaction times (RTs) and correctly predicts the effects of altered expectations. Can it also predict what happens when the urgency of making the response changes? The authors asked 8 Ss (aged 20-53 yrs) to make eye movements to low-visibility targets either as fast or as accurately as possible, and found that the model does indeed predict the timing of their responses: The degree of urgency seems to influence the criterion level at which a decision signal triggers a response. (PsycINFO Database Record (c) 2000 APA, all rights reserved)\",\n",
       " 'Coordinated Reactivation of Distributed Memory Traces in Primate Neocortex. Conversion of new memories into a lasting form may involve the gradual refinement and linking together of neural representations stored widely throughout neocortex. This consolidation process may require coordinated reactivation of distributed components of memory traces while the cortex is \"offline,\" i.e., not engaged in processing external stimuli. Simultaneous neural ensemble recordings from four sites in the macaque neocortex revealed such coordinated reactivation. In motor, somatosensory, and parietal cortex (but not prefrontal cortex), the behaviorally induced correlation structure and temporal patterning of neural ensembles within and between regions were preserved, confirming a major tenet of the trace-reactivation theory of memory consolidation.',\n",
       " 'A cellular mechanism of reward-related learning.. Positive reinforcement helps to control the acquisition of learned behaviours. Here we report a cellular mechanism in the brain that may underlie the behavioural effects of positive reinforcement. We used intracranial self-stimulation (ICSS) as a model of reinforcement learning, in which each rat learns to press a lever that applies reinforcing electrical stimulation to its own substantia nigra. The outputs from neurons of the substantia nigra terminate on neurons in the striatum in close proximity to inputs from the cerebral cortex on the same striatal neurons. We measured the effect of substantia nigra stimulation on these inputs from the cortex to striatal neurons and also on how quickly the rats learned to press the lever. We found that stimulation of the substantia nigra (with the optimal parameters for lever-pressing behaviour) induced potentiation of synapses between the cortex and the striatum, which required activation of dopamine receptors. The degree of potentiation within ten minutes of the ICSS trains was correlated with the time taken by the rats to learn ICSS behaviour. We propose that stimulation of the substantia nigra when the lever is pressed induces a similar potentiation of cortical inputs to the striatum, positively reinforcing the learning of the behaviour by the rats.',\n",
       " 'Phase locking of single neuron activity to theta oscillations during working memory in monkey extrastriate visual cortex.. Working memory has been linked to elevated single neuron discharge in monkeys and to oscillatory changes in the human EEG, but the relation between these effects has remained largely unexplored. We addressed this question by measuring local field potentials and single unit activity simultaneously from multiple electrodes placed in extrastriate visual cortex while monkeys were performing a working memory task. We describe a significant enhancement in theta band energy during the delay period. Theta oscillations had a systematic effect on single neuron activity, with neurons emitting more action potentials near their preferred angle of each theta cycle. Sample-selective delay activity was enhanced if only action potentials emitted near the preferred theta angle were considered. Our results suggest that extrastriate visual cortex is involved in short-term maintenance of information and that theta oscillations provide a mechanism for structuring the recurrent interaction between neurons in different brain regions that underlie working memory.',\n",
       " 'Folksonomies - Cooperative Classification and Communication Through Shared Metadata. This paper examines user-&#8205;generated metadata as implemented and applied in two web services designed to share and organize digital media to better understand grassroots classification. Metadata - data about data - allows systems to collocate related information, and helps users find relevant information. The creation of metadata has generally been approached in two ways: professional creation and author creation. In libraries and other organizations, creating metadata, primarily in the form of catalog records, has traditionally been the domain of dedicated professionals working with complex, detailed rule sets and vocabularies. The primary problem with this approach is scalability and its impracticality for the vast amounts of content being produced and used, especially on the World Wide Web. The apparatus and tools built around professional cataloging systems are generally too complicated for anyone without specialized training and knowledge. A second approach is for metadata to be created by authors. The movement towards creator described documents was heralded by SGML, the WWW, and the Dublin Core Metadata Initiative. There are problems with this approach as well - often due to inadequate or inaccurate description, or outright deception. This paper examines a third approach: user-&#8205;created metadata, where users of the documents and media create metadata for their own individual use that is also shared throughout a community.',\n",
       " \"Dynamics of the hippocampal ensemble code for space. Ensemble recordings of 73 to 148 rat hippocampal neurons were used to predict accurately the animals' movement through their environment, which confirms that the hippocampus transmits an ensemble code for location. In a novel space, the ensemble code was initially less robust but improved rapidly with exploration. During this period, the activity of many inhibitory cells was suppressed, which suggests that new spatial information creates conditions in the hippocampal circuitry that are conducive to the synaptic modification presumed to be involved in learning. Development of a new population code for a novel environment did not substantially alter the code for a familiar one, which suggests that the interference between the two spatial representations was very small. The parallel recording methods outlined here make possible the study of the dynamics of neuronal interactions during unique behavioral events.\",\n",
       " 'Reactivation of hippocampal ensemble memories during sleep.. Simultaneous recordings were made from large ensembles of hippocampal \"place cells\" in three rats during spatial behavioral tasks and in slow-wave sleep preceding and following these behaviors. Cells that fired together when the animal occupied particular locations in the environment exhibited an increased tendency to fire together during subsequent sleep, in comparison to sleep episodes preceding the behavioral tasks. Cells that were inactive during behavior, or that were active but had non-overlapping spatial firing, did not show this increase. This effect, which declined gradually during each post-behavior sleep session, may result from synaptic modification during waking experience. Information acquired during active behavior is thus re-expressed in hippocampal circuits during sleep, as postulated by some theories of memory consolidation.',\n",
       " 'Temporal structure in neuronal activity during working memory in macaque parietal cortex.. A number of cortical structures are reported to have elevated single unit firing rates sustained throughout the memory period of a working memory task. How the nervous system forms and maintains these memories is unknown but reverberating neuronal network activity is thought to be important. We studied the temporal structure of single unit (SU) activity and simultaneously recorded local field potential (LFP) activity from area LIP in the inferior parietal lobe of two awake macaques during a memory-saccade task. Using multitaper techniques for spectral analysis, which play an important role in obtaining the present results, we find elevations in spectral power in a 50--90 Hz (gamma) frequency band during the memory period in both SU and LFP activity. The activity is tuned to the direction of the saccade providing evidence for temporal structure that codes for movement plans during working memory. We also find SU and LFP activity are coherent during the memory period in the 50--90 Hz gamma band and no consistent relation is present during simple fixation. Finally, we find organized LFP activity in a 15--25 Hz frequency band that may be related to movement execution and preparatory aspects of the task. Neuronal activity could be used to control a neural prosthesis but SU activity can be hard to isolate with cortical implants. As the LFP is easier to acquire than SU activity, our finding of rich temporal structure in LFP activity related to movement planning and execution may accelerate the development of this medical application.',\n",
       " 'Automatic Methods for Predicting Functionally Important Residues. Sequence analysis is often the first guide for the prediction of residues in a protein family that may have functional significance. A few methods have been proposed which use the division of protein families into subfamilies in the search for those positions that could have some functional significance for the whole family, but at the same time which exhibit the specificity of each subfamily ({\\\\tt{}\"{}}Tree-determinant residues{\\\\tt{}\"{}}). However, there are still many unsolved questions like the best division of a protein family into subfamilies, or the accurate detection of sequence variation patterns characteristic of different subfamilies. Here we present a systematic study in a significant number of protein families, testing the statistical meaning of the Tree-determinant residues predicted by three different methods that represent the range of available approaches. The first method takes as a starting point a phylogenetic representation of a protein family and, following the principle of Relative Entropy from Information Theory, automatically searches for the optimal division of the family into subfamilies. The second method looks for positions whose mutational behavior is reminiscent of the mutational behavior of the full-length proteins, by directly comparing the corresponding distance matrices. The third method is an automation of the analysis of distribution of sequences and amino acid positions in the corresponding multidimensional spaces using a vector-based principal component analysis. These three methods have been tested on two non-redundant lists of protein families: one composed by proteins that bind a variety of ligand groups, and the other composed by proteins with annotated functionally relevant sites. In most cases, the residues predicted by the three methods show a clear tendency to be close to bound ligands of biological relevance and to those amino acids described as participants in key aspects of protein function. These three automatic methods provide a wide range of possibilities for biologists to analyze their families of interest, in a similar way to the one presented here for the family of proteins related with ras-p21.',\n",
       " 'A common reference frame for movement plans in the posterior parietal cortex.. Orchestrating a movement towards a sensory target requires many computational processes, including a transformation between reference frames. This transformation is important because the reference frames in which sensory stimuli are encoded often differ from those of motor effectors. The posterior parietal cortex has an important role in these transformations. Recent work indicates that a significant proportion of parietal neurons in two cortical areas transforms the sensory signals that are used to guide movements into a common reference frame. This common reference frame is an eye-centred representation that is modulated by eye-, head-, body- or limb-position signals. A common reference frame might facilitate communication between different areas that are involved in coordinating the movements of different effectors. It might also be an efficient way to represent the locations of different sensory targets in the world.',\n",
       " 'The Protein Kinase Complement of the Human Genome. We have catalogued the protein kinase complement of the human genome (the \"kinome\") using public and proprietary genomic, complementary DNA, and expressed sequence tag (EST) sequences. This provides a starting point for comprehensive analysis of protein phosphorylation in normal and disease states, as well as a detailed view of the current state of human genome analysis through a focus on one large gene family. We identify 518 putative protein kinase genes, of which 71 have not previously been reported or described as kinases, and we extend or correct the protein sequences of 56 more kinases. New genes include members of well-studied families as well as previously unidentified families, some of which are conserved in model organisms. Classification and comparison with model organism kinomes identified orthologous groups and highlighted expansions specific to human and other lineages. We also identified 106 protein kinase pseudogenes. Chromosomal mapping revealed several small clusters of kinase genes and revealed that 244 kinases map to disease loci or cancer amplicons.',\n",
       " 'Information processing with population codes.. Information is encoded in the brain by populations or clusters of cells, rather than by single cells. This encoding strategy is known as population coding. Here we review the standard use of population codes for encoding and decoding information, and consider how population codes can be used to support neural computations such as noise removal and nonlinear mapping. More radical ideas about how population codes may directly represent information about stimulus uncertainty are also discussed.',\n",
       " 'Computational approaches to sensorimotor transformations. Behaviors such as sensing an object and then moving your eyes or your hand toward it require that sensory information be used to help generate a motor command, a process known as a sensorimotor transformation. Here we review models of sensorimotor transformations that use a flexible intermediate representation that relies on basis functions. The use of basis functions as an intermediate is borrowed from the theory of nonlinear function approximation. We show that this approach provides a unifying insight into the neural basis of three crucial aspects of sensorimotor transformations, namely, computation, learning and short-term memory. This mathematical formalism is consistent with the responses of cortical neurons and provides a fresh perspective on the issue of frames of reference in spatial representations.',\n",
       " 'Efficient computation and cue integration with noisy population codes.. The brain represents sensory and motor variables through the activity of large populations of neurons. It is not understood how the nervous system computes with these population codes, given that individual neurons are noisy and thus unreliable. We focus here on two general types of computation, function approximation and cue integration, as these are powerful enough to handle a range of tasks, including sensorimotor transformations, feature extraction in sensory systems and multisensory integration. We demonstrate that a particular class of neural networks, basis function networks with multidimensional attractors, can perform both types of computation optimally with noisy neurons. Moreover, neurons in the intermediate layers of our model show response properties similar to those observed in several multimodal cortical areas. Thus, basis function networks with multidimensional attractors may be used by the brain to compute efficiently with population codes.',\n",
       " 'Decoding neuronal spike trains: how important are correlations?. It has been known for >30 years that neuronal spike trains exhibit correlations, that is, the occurrence of a spike at one time is not independent of the occurrence of spikes at other times, both within spike trains from single neurons and across spike trains from multiple neurons. The presence of these correlations has led to the proposal that they might form a key element of the neural code. Specifically, they might act as an extra channel for information, carrying messages about events in the outside world that are not carried by other aspects of the spike trains, such as firing rate. Currently, there is no general consensus about whether this proposal applies to real spike trains in the nervous system. This is largely because it has been hard to separate information carried in correlations from that not carried in correlations. Here we propose a framework for performing this separation. Specifically, we derive an information-theoretic cost function that measures how much harder it is to decode neuronal responses when correlations are ignored than when they are taken into account. This cost function can be readily applied to real neuronal data.',\n",
       " 'Retinal ganglion cells act largely as independent encoders.. Correlated firing among neurons is widespread in the visual system. Neighbouring neurons, in areas from retina to cortex, tend to fire together more often than would be expected by chance. The importance of this correlated firing for encoding visual information is unclear and controversial. Here we examine its importance in the retina. We present the retina with natural stimuli and record the responses of its output cells, the ganglion cells. We then use information theoretic techniques to measure the amount of information about the stimuli that can be obtained from the cells under two conditions: when their correlated firing is taken into account, and when their correlated firing is ignored. We find that more than 90% of the information about the stimuli can be obtained from the cells when their correlated firing is ignored. This indicates that ganglion cells act largely independently to encode information, which greatly simplifies the problem of decoding their activity.',\n",
       " 'Inference and computation with population codes.. In the vertebrate nervous system, sensory stimuli are typically encoded through the concerted activity of large populations of neurons. Classically, these patterns of activity have been treated as encoding the value of the stimulus (e.g., the orientation of a contour), and computation has been formalized in terms of function approximation. More recently, there have been several suggestions that neural computation is akin to a Bayesian inference process, with population activity patterns representing uncertainty about stimuli in the form of probability distributions (e.g., the probability density function over the orientation of a contour). This paper reviews both approaches, with a particular emphasis on the latter, which we see as a very promising framework for future modeling and experimental work.',\n",
       " \"MapReduce: Simplified Data Processing on Large Clusters. MapReduce is a programming model and an associated implementation for processing and generating large data sets.  Users specify a _map_ function that processes a key/value pair to generate a set of intermediate key/value pairs, and a _reduce_ function that merges all intermediate values associated with the same intermediate key.  Many real world tasks are expressible in this model, as shown in the paper. <P> Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program's execution across a set of machines, handling machine failures, and managing the required inter- machine communication.  This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system.  <P> Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google's clusters every day.     <P>\",\n",
       " 'Subject-oriented programming: a critique of pure objects. Object-oriented technology is often described in terms of an interwoven troika of themes: encapsulation, polymorphism and inheritance. But these themes are firmly ties with the concept of identity. If object-oriented technology is to be successfully scaled down the development of independent applications to development of integrated suites of applications, it must relax its emphasis on the object. The technology must recognize more directly that a multiplicity of subjective views delocalizes the concept of object, and must emphasize more the binding concept of identity to tie them together.',\n",
       " \"Stretched exponential distributions in nature and economy: “fat tails” with characteristic scales. To account quantitatively for many reported  natural  fat tail distributions in Nature and Economy, we propose the stretched exponential family as a complement to the often used power law distributions. It has many advantages, among which to be economical with only two adjustable parameters with clear physical interpretation. Furthermore, it derives from a simple and generic mechanism in terms of multiplicative processes. We show that stretched exponentials describe very well the distributions of radio and light emissions from galaxies, of US GOM OCS oilfield reserve sizes, of World, US and French agglomeration sizes, of country population sizes, of daily Forex US-Mark and Franc-Mark price variations, of Vostok (near the south pole) temperature variations over the last 400 000 years, of the Raup-Sepkoski's kill curve and of citations of the most cited physicists in the world. We also discuss its potential for the distribution of earthquake sizes and fault displacements. We suggest physical interpretations of the parameters and provide a short toolkit of the statistical properties of the stretched exponentials. We also provide a comparison with other distributions, such as the shifted linear fractal, the log-normal and the recently introduced parabolic fractal distributions.\",\n",
       " 'Active Appearance Models. We demonstrate a novel method of interpreting images using an Active Appearance Model (AAM). An AAM contains a statistical model of the shape and grey-level appearance of the object of interest which can generalise to almost any valid example. During a training phase we learn the relationship between model parameter displacements and the residual errors induced between a training image and a synthesised model example. To match to an image we measure the current residuals and use the model to predict changes to the current parameters, leading to a better fit. A good overall match is obtained in a few iterations, even from poor starting estimates. We describe the technique in detail and give results of quantitative performance tests. We anticipate that the AAM algorithm will be an important method for locating deformable objects in many applications.',\n",
       " 'Synergy, Redundancy, and Independence in Population Codes. A key issue in understanding the neural code for an ensemble of neurons is the nature and strength of correlations between neurons and how these correlations are related to the stimulus. The issue is complicated by the fact that there is not a single notion of independence or lack of correlation. We distinguish three kinds: (1) activity independence; (2) conditional independence; and (3) information independence. Each notion is related to an information measure: the information between cells, the information between cells given the stimulus, and the synergy of cells about the stimulus, respectively. We show that these measures form an interrelated framework for evaluating contributions of signal and noise correlations to the joint information conveyed about the stimulus and that at least two of the three measures must be calculated to characterize a population code. This framework is compared with others recently proposed in the literature. In addition, we distinguish questions about how information is encoded by a population of neurons from how that information can be decoded. Although information theory is natural and powerful for questions of encoding, it is not sufficient for characterizing the process of decoding. Decoding fundamentally requires an error measure that quantifies the importance of the deviations of estimated stimuli from actual stimuli. Because there is no a priori choice of error measure, questions about decoding cannot be put on the same level of generality as for encoding.',\n",
       " 'Noise, neural codes and cortical organization.. Cortical circuitry must facilitate information transfer in accordance with a neural code. In this article we examine two candidate neural codes: information is represented in the spike rate of neurons, or information is represented in the precise timing of individual spikes. These codes can be distinguished by examining the physiological basis of the highly irregular interspike intervals typically observed in cerebral cortex. Recent advances in our understanding of cortical microcircuitry suggest that the timing of neuronal spikes conveys little, if any, information. The cortex is likely to propagate a noisy rate code through redundant, patchy interconnections.',\n",
       " 'Redundancy reduction revisited.. Soon after Shannon defined the concept of redundancy it was suggested that it gave insight into mechanisms of sensory processing, perception, intelligence and inference. Can we now judge whether there is anything in this idea, and can we see where it should direct our thinking? This paper argues that the original hypothesis was wrong in over-emphasizing the role of compressive coding and economy in neuron numbers, but right in drawing attention to the importance of redundancy. Furthermore there is a clear direction in which it now points, namely to the overwhelming importance of probabilities and statistics in neuroscience. The brain has to decide upon actions in a competitive, chance-driven world, and to do this well it must know about and exploit the non-random probabilities and interdependences of objects and events signalled by sensory messages. These are particularly relevant for Bayesian calculations of the optimum course of action. Instead of thinking of neural representations as transformations of stimulus energies, we should regard them as approximate estimates of the probable truths of hypotheses about the current environment, for these are the quantities required by a probabilistic brain working on Bayesian principles.',\n",
       " 'Scale-free networks from varying vertex intrinsic fitness.. A new mechanism leading to scale-free networks is proposed in this Letter. It is shown that, in many cases of interest, the connectivity power-law behavior is neither related to dynamical properties nor to preferential attachment. Assigning a quenched fitness value x(i) to every vertex, and drawing links among vertices with a probability depending on the fitnesses of the two involved sites, gives rise to what we call a good-get-richer mechanism, in which sites with larger fitness are more likely to become hubs (i.e., to be highly connected).',\n",
       " 'Vertex intrinsic fitness: how to produce arbitrary scale-free networks.. We study a recent model of random networks based on the presence of an intrinsic character of the vertices called fitness. The vertex fitnesses are drawn from a given probability distribution density. The edges between pairs of vertices are drawn according to a linking probability function depending on the fitnesses of the two vertices involved. We study here different choices for the probability distribution densities and the linking functions. We find that, irrespective of the particular choices, the generation of scale-free networks is straightforward. We then derive the general conditions under which scale-free behavior appears. This model could then represent a possible explanation for the ubiquity and robustness of such structures.',\n",
       " 'Empiricism, rationalism and positivism in library and information science. Purpose – The purpose of this paper is to examine the importance and influence of the epistemologies: “empiricism”, “rationalism” and “positivism” in library and information science (LIS). Design/methodology/approach – First, outlines the historical development of these epistemologies, by discussing and identifying basic characteristics in them and by introducing the criticism that has been raised against these views. Second, their importance for and influence in LIS have been examined. Findings – The findings of this paper are that it is not a trivial matter to define those epistemologies and to characterise their influence. Many different interpretations exist and there is no consensus regarding current influence of positivism in LIS. Arguments are put forward that empiricism and positivism are still dominant within LIS and specific examples of the influence on positivism in LIS are provided. A specific analysis is made of the empiricist view of information seeking and it is shown that empiricism may be regarded as a normative theory of information seeking and knowledge organisation. Originality/value – The paper discusses basic theoretical issues that are important for the further development of LIS as a scholarly field.',\n",
       " 'The nature of human altruism.. Some of the most fundamental questions concerning our evolutionary origins, our social relations, and the organization of society are centred around issues of altruism and selfishness. Experimental evidence indicates that human altruism is a powerful force and is unique in the animal world. However, there is much individual heterogeneity and the interaction between altruists and selfish individuals is vital to human cooperation. Depending on the environment, a minority of altruists can force a majority of selfish individuals to cooperate or, conversely, a few egoists can induce a large number of altruists to defect. Current gene-based evolutionary theories cannot explain important patterns of human altruism, pointing towards the importance of both theories of cultural evolution as well as gene-culture co-evolution.',\n",
       " 'Altruistic punishment in humans. Human cooperation is an evolutionary puzzle. Unlike other creatures, people frequently cooperate with genetically unrelated strangers, often in large groups, with people they will never meet again, and when reputation gains are small or absent. These patterns of cooperation cannot be explained by the nepotistic motives associated with the evolutionary theory of kin selection and the selfish motives associated with signalling theory or the theory of reciprocal altruism. Here we show experimentally that the altruistic punishment of defectors is a key motive for the explanation of cooperation. Altruistic punishment means that individuals punish, although the punishment is costly for them and yields no material gain. We show that cooperation flourishes if altruistic punishment is possible, and breaks down if it is ruled out. The evidence indicates that negative emotions towards defectors are the proximate mechanism behind altruistic punishment. These results suggest that future study of the evolution of human cooperation should include a strong focus on explaining altruistic punishment.',\n",
       " \"Dynamic Analysis of Neural Encoding by Point Process Adaptive Filtering. doi: 10.1162/089976604773135069 Neural receptive fields are dynamic in that with experience, neurons change their spiking responses to relevant stimuli. To understand how neural systems adapt the irrepresentations of biological information, analyses of receptive field plasticity from experimental measurements are crucial. Adaptive signal processing, the well-established engineering discipline for characterizing the temporal evolution of system parameters, suggests a framework for studying the plasticity of receptive fields. We use the Bayes' rule Chapman-Kolmogorov paradigm with a linear state equation and point process observation models to derive adaptive filters appropriate for estimation from neural spike trains. We derive point process filter analogues of the Kalman filter, recursive least squares, and steepest-descent algorithms and describe the properties of these new fil-ters. We illustrate our algorithms in two simulated data examples. The first is a study of slow and rapid evolution of spatial receptive fields in hippocampal neurons. The second is an adaptive decoding study in which a signal is decoded from ensemble neural spiking activity as the recep-tive fields of the neurons in the ensemble evolve. Our results provide a paradigm for adaptive estimation for point process observations and suggest a practical approach for constructing filtering algorithms to track neural receptive field dynamics on a millisecond timescale.\",\n",
       " 'Understanding user goals in web search. Previous work on understanding user web search behavior has focused on how people search and what they are searching for, but not why they are searching. In this paper, we describe a framework for understanding the underlying goals of user searches, and our experience in using the framework to manually classify queries from a web search engine. Our analysis suggests that so-called navigational\" searches are less prevalent than generally believed while a previously unexplored \"resource-seeking\" goal may account for a large fraction of web searches. We also illustrate how this knowledge of user search goals might be used to improve future web search engines.',\n",
       " 'A robust layered control system for a mobile robot. A new architecture for controlling mobile robots is described. Layers of control system are built to let the robot operate at increasing levels of competence. Layers are made up of asynchronous modules that communicate over low-bandwidth channels. Each module is an instance of a fairly simple computational machine. Higher-level layers can subsume the roles of lower levels by suppressing their outputs. However, lower levels continue to function as higher levels are added. The result is a robust and flexible robot control system. The system has been used to control a mobile robot wandering around unconstrained laboratory areas and computer machine rooms. Eventually it is intended to control a robot that wanders the office areas of our laboratory, building maps of its surroundings using an onboard arm to perform simple tasks.',\n",
       " 'Cellular mechanisms contributing to response variability of cortical neurons in vivo.. Cortical neurons recorded in vivo exhibit highly variable responses to the repeated presentation of the same stimulus. To further understand the cellular mechanisms underlying this phenomenon, we performed intracellular recordings from neurons in cat striate cortex in vivo and examined the relationships between spontaneous activity and visually evoked responses. Activity was assessed on a trial-by-trial basis by measuring the membrane potential (Vm) fluctuations and spike activity during brief epochs immediately before and after the onset of an evoked response. We found that the response magnitude, expressed as a change in Vm relative to baseline, was linearly correlated with the preceding spontaneous Vm. This correlation was enhanced when the cells were hyperpolarized to reduce the activation of voltage-gated conductances. The output of the cells, expressed as spike counts and latencies, was only moderately correlated with fluctuations in the preceding spontaneous Vm. Spike-triggered averaging of Vm revealed that visually evoked action potentials arise from transient depolarizations having a rise time of approximately 10 msec. Consistent with this, evoked spike count was found to be linearly correlated with the magnitude of Vm fluctuations in the gamma (20-70 Hz) frequency band. We also found that the threshold of visually evoked action potentials varied over a range of approximately 10 mV. Examination of simultaneously recorded intracellular and extracellular activity revealed a correlation between Vm depolarization and spike discharges in adjacent cells. Together these results demonstrate that response variability is attributable largely to coherent fluctuations in cortical activity preceding the onset of a stimulus, but also to variations in action potential threshold and the magnitude of high-frequency fluctuations evoked by the stimulus.',\n",
       " 'Reliability of spike timing in neocortical neurons. It is not known whether the variability of neural activity in the cerebral cortex carries information or reflects noisy underlying mechanisms. In an examination of the reliability of spike generation using recordings from neurons in rat neocortical slices, the precision of spike timing was found to depend on stimulus transients. Constant stimuli led to imprecise spike trains, whereas stimuli with fluctuations resembling synaptic activity produced spike trains with timing reproducible to less than 1 millisecond. These data suggest a low intrinsic noise level in spike generation, which could allow cortical neurons to accurately transform synaptic input into spike sequences, supporting a possible role for spike timing in the processing of cortical information by the neocortex.',\n",
       " 'The Variable Discharge of Cortical Neurons: Implications for Connectivity, Computation, and Information Coding. Cortical neurons exhibit tremendous variability in the number and temporal distribution of spikes in their discharge patterns. Furthermore, this variability appears to be conserved over large regions of the cerebral cortex, suggesting that it is neither reduced nor expanded from stage to stage within a processing pathway. To investigate the principles underlying such statistical homogeneity, we have analyzed a model of synaptic integration incorporating a highly simplified integrate and fire mechanism with decay. We analyzed a \"high-input regime\" in which neurons receive hundreds of excitatory synaptic inputs during each interspike interval. To produce a graded response in this regime, the neuron must balance excitation with inhibition. We find that a simple integrate and fire mechanism with balanced excitation and inhibition produces a highly variable interspike interval, consistent with experimental data. Detailed information about the temporal pattern of synaptic inputs cannot be recovered from the pattern of output spikes, and we infer that cortical neurons are unlikely to transmit information in the temporal pattern of spike discharge. Rather, we suggest that quantities are represented as rate codes in ensembles of 50-100 neurons. These column-like ensembles tolerate large fractions of common synaptic input and yet covary only weakly in their spike discharge. We find that an ensemble of 100 neurons provides a reliable estimate of rate in just one interspike interval (10-50 msec). Finally, we derived an expression for the variance of the neural spike count that leads to a stable propagation of signal and noise in networks of neurons[---]that is, conditions that do not impose an accumulation or diminution of noise. The solution implies that single neurons perform simple algebra resembling averaging, and that more sophisticated computations arise by virtue of the anatomical convergence of novel combinations of inputs to the cortical column from external sources.',\n",
       " 'The highly irregular firing of cortical cells is inconsistent with temporal integration of random EPSPs. How random is the discharge pattern of cortical neurons? We examined recordings from primary visual cortex (V1; Knierim and Van Essen, 1992) and extrastriate cortex (MT; Newsome et al., 1989a) of awake, behaving macaque monkey and compared them to analytical predictions. For nonbursting cells firing at sustained rates up to 300 Hz, we evaluated two indices of firing variability: the ratio of the variance to the mean for the number of action potentials evoked by a constant stimulus, and the rate-normalized coefficient of variation (Cv) of the interspike interval distribution. Firing in virtually all V1 and MT neurons was nearly consistent with a completely random process (e.g., Cv approximately 1). We tried to model this high variability by small, independent, and random EPSPs converging onto a leaky integrate-and-fire neuron (Knight, 1972). Both this and related models predicted very low firing variability (Cv << 1) for realistic EPSP depolarizations and membrane time constants. We also simulated a biophysically very detailed compartmental model of an anatomically reconstructed and physiologically characterized layer V cat pyramidal cell (Douglas et al., 1991) with passive dendrites and active soma. If independent, excitatory synaptic input fired the model cell at the high rates observed in monkey, the Cv and the variability in the number of spikes were both very low, in agreement with the integrate-and-fire models but in strong disagreement with the majority of our monkey data. The simulated cell only produced highly variable firing when Hodgkin-Huxley-like currents (INa and very strong IDR) were placed on distal dendrites. Now the simulated neuron acted more as a millisecond-resolution detector of dendritic spike coincidences than as a temporal integrator. We argue that neurons that act as temporal integrators over many synaptic inputs must fire very regularly. Only in the presence of either fast and strong dendritic nonlinearities or strong synchronization among individual synaptic events will the degree of predicted variability approach that of real cortical neurons.',\n",
       " ' BBC News .  Africa   BBC Wap use flourishing in Africa BBC Wap use flourishing in Africa  Mobile use in Africa is leapfrogging PC technology Africa, in particular Nigeria, is dominating international mobile phone access to the BBC\\'s website. According to July\\'s statistics, 61% of the BBC\\'s international Wap users came from Nigeria and 19% from South Africa.  \"Wap is the one platform where African countries continue to appear in the top five in our statistics,\" said BBC developer Gareth Owen.  Africa is the world\\'s largest-growing mobile phone market with unreliable landlines encouraging the growth.  Wap technology - which stands for wireless application protocol - allows people to access basic information on the internet, like news summaries, through their mobile phone handset.   I\\'m in Uganda and the only access I have 2 the outside world is this pinhole 2 info Ugandan texter to the BBC  Do you need a computer? Ringing in changes in Nigeria  According to the BBC\\'s statistics, page views for Wap usage are growing at 100% year on year.  UK users account for 65% of Wap traffic; and international usage for 35%. Mobile phone providers in many African countries have only recently begun rolling out Wap-enabled handsets.  And the large take up of BBC news via mobiles in Nigeria contrasts starkly with the relatively small number of users accessing the internet via pcs - hampered by slow and unreliable landlines.  The BBC\\'s Technology correspondent Mark Ward says that in many places on the continent PC ownership is low but PC literacy surprisingly high.  Internet cafes tend to be very popular, as much a meeting place as well as a place where people access their email, he says.    The BBC receives regular messages of thanks from people in Africa, who say the only access they have to news is via their mobiles.  \"I\\'m in Uganda and the only access I have 2 the outside world is this pinhole 2 info cause I don\\'t have access to TV. Thanx,\" said one texter from Uganda. The country accounted for 7% of BBC Wap usage in July.  Other top countries helping account for the 58m Wap page views in July were Jamaica, Singapore and Israel.  In the UK, the BBC has about a 20% share of the market with a reach of 1.2m users monthly.',\n",
       " 'The part-time parliament. Digital Equipment Corporation Recent archaeological discoveries on the island of Paxos reveal that the parliament functioned despite the peripatetic propensity of its part-time legislators. The legislators maintained consistent copies of the parliamentary record, despite their frequent forays from the chamber and the forgetfulness of their messengers. The Paxon parliament’s protocol provides a new way of implementing the state-machine approach to the design of distributed systems.',\n",
       " 'How to Build a Highly Available System Using Consensus. Abstract. Lamport showed that a replicated deterministic state machine is a general way to implement a highly available system, given a consensus algorithm that the replicas can use to agree on each input. His Paxos algorithm is the most fault-tolerant way to get consensus without real-time guarantees. Because general consensus is expensive, practical systems reserve it for emergencies and use leases (locks that time out) for most of the computing. This paper explains the general scheme for efficient highly available computing, gives a general method for understanding concurrent and fault-tolerant programs, and derives the Paxos algorithm as an example of the method. 1',\n",
       " \"RAxML-III: a fast program for maximum likelihood-based inference of large phylogenetic trees.. Motivation: The computation of large phylogenetic trees with statistical models such as maximum likelihood or bayesian inference is computationally extremely intensive. It has repeatedly been demonstrated that these models are able to recover the true tree or a tree which is topologically closer to the true tree more frequently than less elaborate methods such as parsimony or neighbor joining. Due to the combinatorial and computational complexity the size of trees which can be computed on a Biologist's PC workstation within reasonable time is limited to trees containing approximately 100 taxa.               Results: In this paper we present the latest release of our program RAxML-III for rapid maximum likelihood-based inference of large evolutionary trees which allows for computation of 1.000-taxon trees in less than 24 hours on a single PC processor. We compare RAxML-III to the currently fastest implementations for maximum likelihood and bayesian inference: PHYML and MrBayes. Whereas RAxML-III performs worse than PHYML and MrBayes on synthetic data it clearly outperforms both programs on all real data alignments used in terms of speed and final likelihood values.               Availability Supplementary information: RAxML-III including all alignments and final trees mentioned in this paper is freely available as open source code at http://wwwbode.cs.tum/~stamatak                           Contact:                stamatak@cs.tum.edu\",\n",
       " 'Natural signal statistics and sensory gain control. We describe a form of nonlinear decomposition that is well-suited for efficient encoding of natural signals. Signals are initially decomposed using a bank of linear filters. Each filter response is then rectified and divided by a weighted sum of rectified responses of neighboring filters. We show that this decomposition, with parameters optimized for the statistics of a generic ensemble of natural images or sounds, provides a good characterization of the nonlinear response properties of typical neurons in primary visual cortex or auditory nerve, respectively. These results suggest that nonlinear response properties of sensory neurons are not an accident of biological implementation, but have an important functional role.',\n",
       " 'NATURAL IMAGE STATISTICS AND NEURAL REPRESENTATION. ▪ Abstract\\u2002 It has long been assumed that sensory neurons are adapted, through both evolutionary and developmental processes, to the statistical properties of the signals to which they are exposed. Attneave (1954), Barlow (1961) proposed that information theory could provide a link between environmental statistics and neural responses through the concept of coding efficiency. Recent developments in statistical modeling, along with powerful computational tools, have enabled researchers to study more sophisticated statistical models for visual images, to validate these models empirically against large sets of data, and to begin experimentally testing the efficient coding hypothesis for both individual neurons and populations of neurons.',\n",
       " 'A Subscribable Peer-to-Peer RDF Repository for Distributed Metadata Management. In this paper, we present a scalable peer-to-peer RDF repository, named RDFPeers, which stores each triple in a multi-attribute addressable network by applying globally known hash functions. Queries can be efficiently routed to the nodes that store matching triples. RDFPeers also enables users to selectively subscribe to RDF content. In RDFPeers, both the neighbors per node and the routing hops for triple insertion, most query resolution and triple subscription are logarithmic to the network size. Our experiments with real-world RDF data demonstrated that the triple-storing load among nodes differs by less than an order of magnitude.',\n",
       " 'Koorde: A simple degree-optimal distributed hash table. Koorde is a new distributed hash table (DHT) based on Chord 15 and the de Bruijn graphs 2. While inheriting the simplicity of Chord, Koorde meets various lower bounds, such as $$O(\\\\log n)$$ hops per lookup request with only 2 neighbors per node (where n is the number of nodes in the DHT), and $$O(\\\\log n/\\\\log\\\\log n)$$ hops per lookup request with $$O(\\\\log n)$$ neighbors per node.',\n",
       " 'Chord: A Scalable Peer-To-Peer Lookup Service for Internet Applications. Efficiently determining the node that stores a data item in a distributed network is an important and challenging problem. This paper describes the motivation and design of the Chord system, a decentralized lookup service that stores key/value pairs for such networks. The Chord protocol takes as input an m-bit identifier (derived by hashing a higher-level application specific key), and returns the node that stores the value corresponding to that key. Each Chord node is identified by an m-bit identifier and each node stores the key identifiers in the system closest to the node&#039;s identifier. Each node maintains an m-entry routing table that allows it to look up keys efficiently. Results from theoretical analysis, simulations, and experiments show that Chord is incrementally scalable, with insertion and lookup costs scaling logarithmically with the number of Chord nodes.',\n",
       " \"Visual Information Retrieval. {<I>Visual Information Retrieval</I>, written by the head of the Systems and Information Department at the University of Florence in Italy, is a must-have for anyone developing systems for cataloging, searching, and retrieving visual-based data.<p> A growing percentage of stored computer information is visual media, whether it's still images or video clips. This kind of data has a very different format and structure than text-based data, and cataloging, sorting, and searching non-text-based data presents a tremendous challenge to the contemporary database programmer.<p> This is not a book for the casual programmer. It offers high-level suggestions on how to build the architecture for such systems, key theories on how to represent this kind of visual content, how to build similarity models, and indexing methods.<p> It continues with examples of how to index and catalog still images based on color, texture, shape, and spatial relationship similarity.<p> Chapter 6 details the problems and current solutions for content-based video retrieval. Detecting sharp transitions, how to analyze compressed and uncompressed streams, and gradual transition detection are just a few of the problems presented. The solutions presented are practical and fascinating.<p> <I>Visual Information Retrieval</I> is a clearly written, although sometimes dense, handbook. The author uses a generous amount of examples of mathematical formulas, illustrations, and color plates. Clearly not a book for every database programmer--but a mandatory reference book for anyone building visual retrieval systems. <I>--Mike Caputo</I>} {<p>The increasing use of multimedia in computer applications has increased the relevance of visual databases.  These visual databases require new methods for archiving and retrieving information, as traditional approaches used previously to index textual data are no longer appropriate.  Visual Information Retrieval concentrates on solutions for representation, indexing, and querying by content of visual information, reviewing the main approaches and techniques available.  Single image indexing, querying and retrieval by content, video segmentation, annotation, and content-based indexing are all examined.  The book will appeal to practitioners and graduates/researchers involved in visual database issues in multimedia and image processing.}\",\n",
       " 'Content-Based Classification, Search, and Retrieval of Audio. Many audio and multimedia applications would benefit from the ability to classify and search for audio based on characteristics of the audio rather than by keywords. These include multimedia databases and file systems, digital libraries, automatic segmentation or indexing of video (for example, news or sports footage) using the audio soundtrack, surveillance, and sound browsers for effects designers and musicians. This article describes an audio analysis, search, and classification engine that reduces sounds to perceptual and acoustical features. Sounds can then be searched or retrieved by any one or a combination of the features, by specifying previously learned classes based on these features, or by selecting or entering reference sounds and asking the engine to retrieve sounds that are similar (or dissimilar) to them. We present examples of this engine as it would be used in some of the application areas listed above. Readers may contact Erling Wold at Muscle Fish LLC, 2550 Ninth Street, Suite 207B, Berkeley, CA 94710, e-mail erling@musclefish.com.',\n",
       " \"Indivisible labor and the business cycle. A growth model with shocks to technology is studied. Labor is indivisible, so all variability in hours worked is due to fluctuations in the number employed. We find that, unlike previous equilibrium models of the business cycle, this economy displays large fluctuations in hours worked and relatively small fluctuations in productivity. This finding is independent of individuals' willingness to substitute leisure across time. This and other findings are the result of studying and comparing summary statistics describing this economy, an economy with divisible labor, and post-war U.S. time series.\",\n",
       " 'The Bargaining Problem. A new treatment is presented of a classical economic problem, one which occurs in many forms, as bargaining, bilateral monopoly, etc. It may also be regarded as a nonzero-sum two-person game. In this treatment a few general assumptions are made concerning the behavior of a single individual and of a group of two individuals in certain economic environments. From these, the solution (in the sense of this paper) of the classical problem may be obtained. In the terms of game theory, values are found for the game.',\n",
       " 'Perfect Equilibrium in a Bargaining Model. Two players have to reach an agreement on the partition of a pie of size 1. Each has to make in turn, a proposal as to how it should be divided. After one player has made an offer, the other must decide either to accept it, or to reject it and continue the bargaining. Several properties which the players\\' preferences possess are assumed. The Perfect Equilibrium Partitions (P.E.P.) are characterized in all the modesls satisfying these assumptions. Specially, it is proved that when every player bears a fixed bargaining cost for each period (c\"1 and c\"2), then: (i) if c\"1 < c\"2 the only P.E.P. gives all the pie to 1; (ii) if c\"1 > c\"2 the only P.E.P. gives to 1; only c\"2. In the case where each player has a fixed discounting factor (@d\"1 and @d\"2) the only P.E.P. is (1 - @d\"2)/(1 - @d\"1@d\"2).',\n",
       " 'Revisiting the paxos algorithm. The  algorithm is an efficient and highly fault-tolerant algorithm, devised by Lamport, for reaching consensus in a distributed system. Although it appears to be practical, it seems to be not widely known or understood. This paper contains a new presentation of the  algorithm, based on a formal decomposition into several interacting components. It also contains a correctness proof and a time performance and fault-tolerance analysis. The formal framework used for the presentation of the algorithm is provided by the Clock General Timed Automaton (Clock GTA) model. The Clock GTA provides a systematic way of describing timing-based systems in which there is a notion of \"normal\" timing behavior, but that do not necessarily always exhibit this \"normal\" timing behavior.',\n",
       " 'Foundations of Swarm Intelligence: From Principles to Practice. Swarm Intelligence (SI) is a relatively new paradigm being applied in a host of research settings to improve the management and control of large numbers of interacting entities such as communication, computer and sensor networks, satellite constellations and more. Attempts to take advantage of this paradigm and mimic the behavior of insect swarms however often lead to many different implementations of SI. The rather vague notions of what constitutes self-organized behavior lead to rather ad hoc approaches that make it difficult to ascertain just what SI is, assess its true potential and more fully take advantage of it. This article provides a set of general principles for SI research and development. A precise definition of self-organized behavior is described and provides the basis for a more axiomatic and logical approach to research and development as opposed to the more prevalent ad hoc approach in using SI concepts. The concept of Pareto optimality is utilized to capture the notions of efficiency and adaptability. A new concept, Scale Invariant Pareto Optimality is described and entails symmetry relationships and scale invariance where Pareto optimality is preserved under changes in system states. This provides a mathematical way to describe efficient tradeoffs of efficiency between different scales and further, mathematically captures the notion of the graceful degradation of performance so often sought in complex systems.',\n",
       " 'TimeSpace: activity-based temporal visualisation of personal information spaces. Users’ personal information spaces are characterized by their content, organisation, and ongoing user interaction with them. They are fluid entities, evolving over time, and supporting multiple user activities that may require different perspectives of the same underlying information structure. Increasing storage capacity of computing devices and ready access to networked resources puts users at risk of information overload, and presents increasing challenges in organising and accessing their information. The hierarchical model of information organisation currently dominates personal computing, and is realised for the user in interfaces that help to manage and access filestore hierarchies. Such a model provides limited inherent support for what users do—carry out a range of interleaved activities over time. In this paper, we describe the TimeSpace system, which provides perspectives on a user’s information resources based on activities and temporal attributes of the information. TimeSpace can be used alongside, or in place of, existing systems and models (such as the Microsoft Windows hierarchical file model). User interaction with an information space is non-intrusively observed and then represented automatically in TimeSpace. Visualisations provide overviews of user activity on multiple projects and detailed views of activity within particular projects, allowing navigation forward and backward in time. An observational study of use of the system revealed positive user views of the utility of temporal, activity-oriented workspaces in real world contexts alongside existing tools. Participants appreciated being offered a different perspective on their electronic information collection, one that visually shows the composition and development of their information space. They were interested in using the system for current and long-term work as well as for archiving information, as the visualisations provide a context for their work and give an overview of all their work in progress. The ideas embodied by the system and its visualisations show promise and raise a number of issues for further exploration. In future work, these ideas will be adapted and extended to support users in managing their information spaces across multiple personal devices, locations and time.',\n",
       " 'Aging and Death in an Organism That Reproduces by Morphologically Symmetric Division. In macroscopic organisms, aging is often obvious; in single-celled organisms, where there is the greatest potential to identify the molecular mechanisms involved, identifying and quantifying aging is harder. The primary results in this area have come from organisms that share the traits of a visibly asymmetric division and an identifiable juvenile phase. As reproductive aging must require a differential distribution of aged and young components between parent and offspring, it has been postulated that organisms without these traits do not age, thus exhibiting functional immortality. Through automated time-lapse microscopy, we followed repeated cycles of reproduction by individual cells of the model organism Escherichia coli, which reproduces without a juvenile phase and with an apparently symmetric division. We show that the cell that inherits the old pole exhibits a diminished growth rate, decreased offspring production, and an increased incidence of death. We conclude that the two supposedly identical cells produced during cell division are functionally asymmetric; the old pole cell should be considered an aging parent repeatedly producing rejuvenated offspring. These results suggest that no life strategy is immune to the effects of aging, and therefore immortality may be either too costly or mechanistically impossible in natural organisms.',\n",
       " 'Neutral microepidemic evolution of bacterial pathogens. Understanding bacterial population genetics is vital for interpreting the response of bacterial populations to selection pressures such as antibiotic treatment or vaccines targeted at only a subset of strains. The evolution of transmissible bacteria occurs by mutation and localized recombination and is influenced by epidemiological as well as molecular processes. We demonstrate that the observed population genetic structure of three important human pathogens, Streptococcus pneumoniae, Neisseria meningitidis, and Staphylococcus aureus, can be explained by using a simple evolutionary model that is based on neutral mutational drift, modulated by recombination, and which incorporates the impact of epidemic transmission in local populations. The predictions of this neutral \"microepidemic\" model are found to closely fit observed genetic relatedness distributions of bacteria sampled from their natural population, and it provides estimates of the relative rate of recombination that agree well with empirical estimates. The analysis suggests the emergence of neutral bacterial population structure from overlapping microepidemics within clustered host populations and provides insight into the nature and size distribution of these clusters. These findings challenge the assumption that strains of bacterial pathogens differ markedly in relative fitness.',\n",
       " 'A tutorial on learning with bayesian networks. A Bayesian network is a graphical model that encodes probabilistic relationships among variables of interest. When used in conjunction with statistical techniques, the graphical model has several advantages for data analysis. One, because the model encodes dependencies among all variables, it readily handles situations where some data entries are missing. Two, a Bayesian network can be used to learn causal relationships, and hence can be used to gain understanding about a problem domain and to predict the consequences of intervention. Three, because the model has both a causal and probabilistic semantics, it is an ideal representation for combining prior knowledge (which often comes in causal form) and data. Four, Bayesian statistical methods in conjunction with bayesian networks offer an efficient and principled approach for avoiding the overfitting of data. In this paper, we discuss methods for constructing Bayesian networks from prior knowledge and summarize Bayesian statistical methods for using data to improve these models. With regard to the latter task, we describe methods for learning both the parameters and structure of a Bayesian network, including techniques for learning with incomplete data. In addition, we relate Bayesian-network methods for learning to techniques for supervised and unsupervised learning. We illustrate the graphical-modeling approach using a real-world case study.',\n",
       " \"The Bayesian brain: the role of uncertainty in neural coding and computation.  To use sensory information efficiently to make judgments and guide action in the world, the brain must represent and use information about uncertainty in its computations for perception and action. Bayesian methods have proven successful in building computational theories for perception and sensorimotor control, and psychophysics is providing a growing body of evidence that human perceptual computations are ‘Bayes' optimal’. This leads to the ‘Bayesian coding hypothesis’: that the brain represents sensory information probabilistically, in the form of probability distributions. Several computational schemes have recently been proposed for how this might be achieved in populations of neurons. Neurophysiological data on the hypothesis, however, is almost non-existent. A major challenge for neuroscientists is to test these ideas experimentally, and so determine whether and how neurons code information about sensory uncertainty.\",\n",
       " \"Bayesian integration in sensorimotor learning. When we learn a new motor skill, such as playing an approaching tennis ball, both our sensors and the task possess variability. Our sensors provide imperfect information about the ball's velocity, so we can only estimate it. Combining information from multiple modalities can reduce the error in this estimate1, 2, 3, 4. On a longer time scale, not all velocities are a priori equally probable, and over the course of a match there will be a probability distribution of velocities. According to bayesian theory5, 6, an optimal estimate results from combining information about the distribution of velocities—the prior—with evidence from sensory feedback. As uncertainty increases, when playing in fog or at dusk, the system should increasingly rely on prior knowledge. To use a bayesian strategy, the brain would need to represent the prior distribution and the level of uncertainty in the sensory feedback. Here we control the statistical variations of a new sensorimotor task and manipulate the uncertainty of the sensory feedback. We show that subjects internally represent both the statistical distribution of the task and their sensory uncertainty, combining them in a manner consistent with a performance-optimizing bayesian process4, 5. The central nervous system therefore employs probabilistic models during sensorimotor learning.\",\n",
       " 'Highly Conserved Non-Coding Sequences Are Associated with Vertebrate Development. In addition to protein coding sequence, the human genome contains a significant amount of regulatory DNA, the identification of which is proving somewhat recalcitrant to both in silico and functional methods. An approach that has been used with some success is comparative sequence analysis, whereby equivalent genomic regions from different organisms are compared in order to identify both similarities and differences. In general, similarities in sequence between highly divergent organisms imply functional constraint. We have used a whole-genome comparison between humans and the pufferfish, Fugu rubripes, to identify nearly 1,400 highly conserved non-coding sequences. Given the evolutionary divergence between these species, it is likely that these sequences are found in, and furthermore are essential to, all vertebrates. Most, and possibly all, of these sequences are located in and around genes that act as developmental regulators. Some of these sequences are over 90&#37; identical across more than 500 bases, being more highly conserved than coding sequence between these two species. Despite this, we cannot find any similar sequences in invertebrate genomes. In order to begin to functionally test this set of sequences, we have used a rapid in vivo assay system using zebrafish embryos that allows tissue-specific enhancer activity to be identified. Functional data is presented for highly conserved non-coding sequences associated with four unrelated developmental regulators (SOX21, PAX6, HLXB9, and SHH), in order to demonstrate the suitability of this screen to a wide range of genes and expression patterns. Of 25 sequence elements tested around these four genes, 23 show significant enhancer activity in one or more tissues. We have identified a set of non-coding sequences that are highly conserved throughout vertebrates. They are found in clusters across the human genome, principally around genes that are implicated in the regulation of development, including many transcription factors. These highly conserved non-coding sequences are likely to form part of the genomic circuitry that uniquely defines vertebrate development.',\n",
       " 'Unreliable failure detectors for reliable distributed systems. We introduce the concept of unreliable failure detectors and study how they can be used to solve Consensus in asynchronous systems with crash failures. We characterise unreliable failure detectors in terms of two properties&mdash;completeness and accuracy. We show that Consensus can be solved even with unreliable failure detectors that make an infinite number of mistakes, and determine which ones can be used to solve Consensus despite any  number of crashes, and which ones require a majority of correct processes. We prove that Consensus and Atomic Broadcast are reducible to each other in asynchronous systems with crash failures; thus, the above results also apply to Atomic Broadcast. A companion paper shows that one of the failure detectors introduced here is the weakest failure detector for solving Consensus [Chandra et al. 1992].',\n",
       " 'Counting probability distributions: differential geometry and model selection.. A central problem in science is deciding among competing explanations of data containing random errors. We argue that assessing the ‘‘complexity’’ of explanations is essential to a theoretically wellfounded model selection procedure. We formulate model complexity in terms of the geometry of the space of probability distributions. Geometric complexity provides a clear intuitive understanding of several extant notions of model complexity. This approach allows us to reconceptualize the model selection problem as one of counting explanations that lie close to the ‘‘truth.’’ We demonstrate the usefulness of the approach by applying it to the recovery of models in psychophysics.',\n",
       " \"The neighbor-joining method: a new method for reconstructing phylogenetic trees.. A new method called the neighbor-joining method is proposed for reconstructing phylogenetic trees from evolutionary distance data. The principle of this method is to find pairs of operational taxonomic units (OTUs [= neighbors]) that minimize the total branch length at each stage of clustering of OTUs starting with a starlike tree. The branch lengths as well as the topology of a parsimonious tree can quickly be obtained by using this method. Using computer simulation, we studied the efficiency of this method in obtaining the correct unrooted tree in comparison with that of five other tree-making methods: the unweighted pair group method of analysis, Farris's method, Sattath and Tversky's method, Li's method, and Tateno et al.'s modified Farris method. The new, neighbor-joining method and Sattath and Tversky's method are shown to be generally better than the other methods.\",\n",
       " 'Measuring genome evolution. The determination of complete genome sequences provides us with an opportunity to describe and analyze evolution at the comprehensive level of genomes. Here we compare nine genomes with respect to their protein coding genes at two levels: () we compare genomes as “bags of genes” and measure the fraction of orthologs shared between genomes and () we quantify correlations between genes with respect to their relative positions in genomes. Distances between the genomes are related to their divergence times, measured as the number of amino acid substitutions per site in a set of 34 orthologous genes that are shared among all the genomes compared. We establish a hierarchy of rates at which genomes have changed during evolution. Protein sequence identity is the most conserved, followed by the complement of genes within the genome. Next is the degree of conservation of the order of genes, whereas gene regulation appears to evolve at the highest rate. Finally, we show that some genomes are more highly organized than others: they show a higher degree of the clustering of genes that have orthologs in other genomes.',\n",
       " \"Writing a research paper. SummaryThe aim of this article is to help those embarking on research to communicate effectively through writing, and to improve their chances of getting a paper published. The quality of a paper's research content is judged by originality, importance and scientific validity. Advice should be sought on a project's potential for high-quality research content before taking up the research. When readers have difficulties in understanding a paper, the problem more often lies with presentation and structure than with its scientific content. Readers expect information to be presented in a certain way and when this does not happen they may misinterpret what the writer intended.\",\n",
       " 'Spike train dynamics predicts theta-related phase precession in hippocampal pyramidal cells. According to the temporal coding hypothesis1, neurons encode information by the exact timing of spikes. An example of temporal coding is the hippocampal phase precession phenomenon, in which the timing of pyramidal cell spikes relative to the theta rhythm shows a unidirectional forward precession during spatial behaviour2, 3. Here we show that phase precession occurs in both spatial and non-spatial behaviours. We found that spike phase correlated with instantaneous discharge rate, and precessed unidirectionally at high rates, regardless of behaviour. The spatial phase precession phenomenon is therefore a manifestation of a more fundamental principle governing the timing of pyramidal cell discharge. We suggest that intrinsic properties of pyramidal cells have a key role in determining spike times, and that the interplay between the magnitude of dendritic excitation and rhythmic inhibition of the somatic region is responsible for the phase assignment of spikes4, 5.',\n",
       " 'Toward a basic framework for webometrics. In this article, we define webometrics within the framework of informetric studies and bibliometrics, as belonging to library and information science, and as associated with cybermetrics as a generic subfield. We develop a consistent and detailed link typology and terminology and make explicit the distinction among different Web node levels when using the proposed conceptual framework. As a consequence, we propose a novel diagram notation to fully appreciate and investigate link structures between Web nodes in webometric analyses. We warn against taking the analogy between citation analyses and link analyses too far.',\n",
       " 'Learning Bayesian Networks: The Combination of Knowledge and Statistical Data. { We describe a Bayesian approach for learning Bayesian networks from a combination of prior knowledge and statistical data. First and foremost, we develop a methodology for assessing informative priors needed for learning. Our approach is derived from a set of assumptions made previously as well as the assumption of likelihood equivalence, which says that data should not help to discriminate network structures that represent the same assertions of conditional independence. We show that likelihood equivalence when combined with previously made assumptions implies that the user\\\\&\\\\\\\\#039;s priors for network parameters can be encoded in a single Bayesian network for the next case to be seen\\\\&\\\\\\\\#151;a prior network\\\\&\\\\\\\\#151;and a single measure of confidence for that network. Second, using these priors, we show how to compute the relative posterior probabilities of network structures given data. Third, we describe search methods for identifying network structures with high posterior probabilities. We describe polynomial algorithms for finding the highest-scoring network structures in the special case where every node has at most k = 1 parent. For the general case (k \\\\&\\\\\\\\#062; 1), which is NP-hard, we review heuristic search algorithms including local search, iterative local search, and simulated annealing. Finally, we describe a methodology for evaluating Bayesian-network learning algorithms, and apply this approach to a comparison of various approaches.}',\n",
       " 'A Bayesian Method for the Induction of Probabilistic Networks from Data. \\t\\t\\tThis paper presents a Bayesian method for constructing probabilistic networks from databases. In particular, we focus on constructing Bayesian belief networks. Potential applications include computer-assisted hypothesis testing, automated scientific discovery, and automated construction of probabilistic expert systems. We extend the basic method to handle missing data and hidden (latent) variables. We show how to perform probabilistic inference by averaging over the inferences of multiple belief networks. Results are presented of a preliminary evaluation of an algorithm for constructing a belief network from a database of cases. Finally, we relate the methods in this paper to previous work, and we discuss open problems.',\n",
       " 'The Value of Reputation on eBay: A Controlled Experiment. We conducted the first randomized controlled field experiment of an Internet reputation mechanism.  A high-reputation, established eBay dealer sold matched pairs of lots -- batches of vintage postcards -- under his regular identity and under new seller identities (also operated by him). As predicted, the established identity fared better.  The difference in buyers’ willingness-to-pay was 8.1% of the selling price. A subsidiary experiment followed the same format, but compared sales by relatively new sellers with and without negative feedback. Surprisingly, one or two negative feedbacks for our new sellers did not affect buyers’ willingness-to-pay.',\n",
       " 'Goal-Oriented Requirements Engineering: A Guided Tour. Goals capture, at different levels of abstraction, the various objectives the system under consideration should achieve. Goal-oriented requirements engineering is concerned with the use of goals for eliciting, elaborating, structuring, specifying, analyzing, negotiating, documenting, and modifying requirements. This area has received increasing attention over the past few years. The paper reviews various research efforts undertaken along this line of research. The arguments in favor of goal orientation are first briefly discussed. The paper then com-pares the main approaches to goal modeling, goal specification and goal-based reasoning in the many activities of the requirements engineering process. To make the discussion more concrete, a real case study is used to suggest what a goal-oriented requirements engineering method may look like. Experience with such approaches and tool support are briefly discussed as well.',\n",
       " 'Cognitive design elements to support the construction of a mental model during software exploration. The scope of software visualization tools which exist for the navigation, analysis and presentation of software information varies widely. One class of tools, which we refer to as Software exploration tools, provides graphical representations of static software structures linked to textual views of the program source code and documentation. This paper describes a hierarchy of cognitive issues which should be considered during the design of a software exploration tool. The hierarchy of cognitive design elements is derived through the examination of program comprehension cognitive models. Examples of how existing tools address each of these issues are provided. In addition, this paper demonstrates how these cognitive design elements may be applied to the design of an effective interface for software exploration.',\n",
       " 'Risk, ambiguity, and the Savage axioms. The article examines a class of choice-situations in which many otherwise reasonable people neither wish nor tend to conform to the Savage postulates of uncertainty, nor to the other axiom sets that have been devised. But the implications of such a finding, if true, are not wholly destructive. First, both the predictive and normative use of the Savage or equivalent postulates might be improved by avoiding attempts to apply them in certain, specifiable circumstances where they do not seem acceptable. Second it is hoped that in such circumstances that certain proposals for alternative decision rules and nonprobabilistic descriptions of uncertainty might prove fruitful.',\n",
       " 'Gender, Identity, and Language Use in Teenage Blogs. This study examines issues of online identity and language use among male and female teenagers who created and maintained weblogs, personal journals made publicly accessible on the World Wide Web. Online identity and language use were examined in terms of the disclosure of personal information, sexual identity, emotive features, and semantic themes. Male and female teenagers presented themselves similarly in their blogs, often revealing personal information such as their real names, ages, and locations. Males more so than females used emoticons, employed an active and resolute style of language, and were more likely to present themselves as gay. The results suggest that teenagers stay closer to reality in their online expressions of self than has previously been suggested, and that these explorations involve issues, such as learning about their sexuality, that commonly occur during the adolescent years.',\n",
       " 'CLUSTAL W: improving the sensitivity of progressive multiple sequence alignment through sequence weighting, position-specific gap penalties and weight matrix choice.. The sensitivity of the commonly used progressive multiple sequence alignment method has been greatly improved for the alignment of divergent protein sequences. Firstly, individual weights are assigned to each sequence in a partial alignment in order to down-weight near-duplicate sequences and up-weight the most divergent ones. Secondly, amino acid substitution matrices are varied at different alignment stages according to the divergence of the sequences to be aligned. Thirdly, residue-specific gap penalties and locally reduced gap penalties in hydrophilic regions encourage new gaps in potential loop regions rather than regular secondary structure. Fourthly, positions in early alignments where gaps have been opened receive locally reduced gap penalties to encourage the opening up of new gaps at these positions. These modifications are incorporated into a new program, CLUSTAL W which is freely available.',\n",
       " 'Improved microbial gene identification with GLIMMER. The {GLIMMER} system for microbial gene identification finds approximately 97-98\\\\% of all genes in a genome when compared with published annotation. {T}his paper reports on two new results: (i) significant technical improvements to {GLIMMER} that improve its accuracy still further, and (ii) a comprehensive evaluation that demonstrates that the accuracy of the system is likely to be higher than previously recognized. {A} significant proportion of the genes missed by the system appear to be hypothetical proteins whose existence is only supported by the predictions of other programs. {W}hen the analysis is restricted to genes that have significant homology to genes in other organisms, {GLIMMER} misses <1\\\\% of known genes.',\n",
       " 'The PROSITE database, its status in 2002.. PROSITE [Bairoch and Bucher (1994) Nucleic Acids Res., 22, 3583-3589; Hofmann et al. (1999) Nucleic Acids Res., 27, 215-219] is a method of identifying the functions of uncharacterized proteins translated from genomic or cDNA sequences. The PROSITE database (http://www.expasy.org/prosite/) consists of biologically significant patterns and profiles designed in such a way that with appropriate computational tools it can rapidly and reliably help to determine to which known family of proteins (if any) a new sequence belongs, or which known domain(s) it contains.',\n",
       " 'tRNAscan-SE: a program for improved detection of transfer RNA genes in genomic sequence.. We describe a program, tRNAscan-SE, which identifies 99-100% of transfer RNA genes in DNA sequence while giving less than one false positive per 15 gigabases. Two previously described tRNA detection programs are used as fast, first-pass prefilters to identify candidate tRNAs, which are then analyzed by a highly selective tRNA covariance model. This work represents a practical application of RNA covariance models, which are general, probabilistic secondary structure profiles based on stochastic context-free grammars. tRNAscan-SE searches at approximately 30 000 bp/s. Additional extensions to tRNAscan-SE detect unusual tRNA homologues such as selenocysteine tRNAs, tRNA-derived repetitive elements and tRNA pseudogenes.',\n",
       " 'Microbial gene identification using interpolated Markov models. This paper describes a new system, {GLIMMER}, for finding genes in microbial genomes. {I}n a series of tests on {H}aemophilus influenzae , {H}elicobacter pylori and other complete microbial genomes, this system has proven to be very accurate at locating virtually all the genes in these sequences, outperforming previous methods. {A} conservative estimate based on experiments on {H}.pylori and {H}. influenzae is that the system finds >97% of all genes. {GLIMMER} uses interpolated {M}arkov models ({IMM}s) as a framework for capturing dependencies between nearby nucleotides in a {DNA} sequence. {A}n {IMM}-based method makes predictions based on a variable context; i.e., a variable-length oligomer in a {DNA} sequence. {T}he context used by {GLIMMER} changes depending on the local composition of the sequence. {A}s a result, {GLIMMER} is more flexible and more powerful than fixed-order {M}arkov methods, which have previously been the primary content-based technique for finding genes in microbial {DNA}.',\n",
       " 'On the minimal synchronism needed for distributed consensus. Reaching agreement is a primitive of distributed computing. Whereas this poses no problem in an ideal, failure-free environment, it imposes certain constraints on the capabilities of an actual system: A system is viable only if it permits the existence of consensus protocols tolerant to some number of failures. Fischer et al. have shown that in a completely asynchronous model, even one failure cannot be tolerated. In this paper their work is extended: Several critical system parameters, including various synchrony conditions, are identified and how varying these affects the number of faults that can be tolerated is examined. The proofs expose general heuristic principles that explain why consensus is possible in certain models but not possible in others.',\n",
       " 'The weakest failure detector for solving consensus. Abstract. We determine what information about failures is necessary and sufficient to solve Consensus in asynchronous distributed systems subject to crash failures. In Chandra and Toueg [1996], it is shown that eventually weak , a failure detector that provides surprisingly little information about which processes have crashed, is sufficient to solve Consensus in asynchronous systems with a majority of correct processes. In this paper, we prove that to solve Consensus, any failure detector has to provide at least as much information as eventual weak . Thus, eventual weak is indeed the weakest failure detector for solving Consensus in asynchronous systems with a majority of correct processes.',\n",
       " 'Consensus in the presence of partial synchrony. The concept of partial synchrony in a distributed system is introduced. Partial synchrony lies between the cases of a synchronous system and an asynchronous system. In a synchronous system, there is a known fixed upper bound &Dgr; on the time required for a message to be sent from one processor to another and a known fixed upper bound &PHgr; on the relative speeds of different processors. In an asynchronous system no fixed upper bounds &Dgr; and &PHgr; exist. In one version of partial synchrony, fixed bounds &Dgr; and &PHgr; exist, but they are not known a priori. The problem is to design protocols that work correctly in the partially synchronous system regardless of the actual values of the bounds &Dgr; and &PHgr;. In another version of partial synchrony, the bounds are known, but are only guaranteed to hold starting at some unknown time  T , and protocols must be designed to work correctly regardless of when time  T  occurs. Fault-tolerant consensus protocols are given for various cases of partial synchrony and various fault models. Lower bounds that show in most cases that our protocols are optimal with respect to the number of faults tolerated are also given. Our consensus protocols for partially synchronous processors use new protocols for fault-tolerant &ldquo;distributed clocks&rdquo; that allow partially synchronous processors to reach some approximately common notion of time.',\n",
       " 'Conditional Independence in Statistical Theory. Some simple heuristic properties of conditional independence are shown to form a conceptual framework for much of the theory of statistical inference. This framework is illustrated by an examination of the role of conditional independence in several diverse areas of the field of statistics. Topics covered include sufficiency and ancillarity, parameter identification, causal inference, prediction sufficiency, data selection mechanisms, invariant statistical models and a subjectivist approach to model-building.',\n",
       " \"Dark matter in the genome: evidence of widespread transcription detected by microarray tiling experiments.. Microarrays provide the opportunity to measure transcription from regions of the genome without bias towards the location of known genes. This technology thus offers an important source of genomic sequence annotation that is complementary to cDNA sequencing and computational gene-finding methods. Recent 'tiling' microarray experiments that assay transcription at regular intervals throughout the genome have shown evidence of large amounts of transcription outside the boundaries of known genes. This transcription is observed in polyadenylated RNA samples and appears to be derived from intergenic regions, from introns of known genes and from sequences antisense to known transcripts. In this article, we discuss different explanations for this phenomenon.\",\n",
       " 'Detection of parallel functional modules by comparative analysis of genome sequences. Parallel functional modules are separate sets of proteins in an organism that catalyze the same or similar biochemical reactions but act on different substrates or use different cofactors. They originate by gene duplication during evolution. Parallel functional modules provide versatility and complexity to organisms, and increase cellular flexibility and robustness. We have developed a four-step approach for genome-wide discovery of parallel modules from protein functional linkages. From ten genomes, we identified 37 cellular systems that consist of parallel functional modules. This approach recovers known parallel complexes and pathways, and discovers new ones that conventional homology-based methods did not previously reveal, as illustrated by examples of peptide transporters in Escherichia coli and nitrogenases in Rhodopseudomonas palustris. The approach untangles intertwined functional linkages between parallel functional modules and expands our ability to decode protein functions from genome sequences.',\n",
       " 'Functional annotation and network reconstruction through cross-platform integration of microarray data. The rapid accumulation of microarray data translates into a need for methods to effectively integrate data generated with different platforms. Here we introduce an approach, 2(nd)-order expression analysis, that addresses this challenge by first extracting expression patterns as meta-information from each data set (1(st)-order expression analysis) and then analyzing them across multiple data sets. Using yeast as a model system, we demonstrate two distinct advantages of our approach: we can identify genes of the same function yet without coexpression patterns and we can elucidate the cooperativities between transcription factors for regulatory network reconstruction by overcoming a key obstacle, namely the quantification of activities of transcription factors. Experiments reported in the literature and performed in our lab support a significant number of our predictions.',\n",
       " 'The genetic theory of adaptation: a brief history..  Theoretical studies of adaptation have exploded over the past decade. This work has been inspired by recent, surprising findings in the experimental study of adaptation. For example, morphological evolution sometimes involves a modest number of genetic changes, with some individual changes having a large effect on the phenotype or fitness. Here I survey the history of adaptation theory, focusing on the rise and fall of various views over the past century and the reasons for the slow development of a mature theory of adaptation. I also discuss the challenges that face contemporary theories of adaptation.',\n",
       " 'Genome-wide association studies: theoretical and practical concerns.. To fully understand the allelic variation that underlies common diseases, complete genome sequencing for many individuals with and without disease is required. This is still not technically feasible. However, recently it has become possible to carry out partial surveys of the genome by genotyping large numbers of common SNPs in genome-wide association studies. Here, we outline the main factors — including models of the allelic architecture of common diseases, sample size, map density and sample-collection biases — that need to be taken into account in order to optimize the cost efficiency of identifying genuine disease-susceptibility loci.',\n",
       " 'Genome-wide association studies for common diseases and complex traits. Genetic factors strongly affect susceptibility to common diseases and also influence disease-related quantitative traits. Identifying the relevant genes has been difficult, in part because each causal gene only makes a small contribution to overall heritability. Genetic association studies offer a potentially powerful approach for mapping causal genes with modest effects, but are limited because only a small number of genes can be studied at a time. Genome-wide association studies will soon become possible, and could open new frontiers in our understanding and treatment of disease. However, the execution and analysis of such studies will require great care.',\n",
       " \"Semantic email: theory and applications. This paper investigates how the vision of the Semantic Web can be carried over to the realm of email. We introduce a general notion of semantic email, in which an email message consists of a structured query or update coupled with corresponding explanatory text. Semantic email opens the door to a wide range of automated, email-mediated applications with formally guaranteed properties. In particular, this paper introduces a broad class of semantic email processes. For example, consider the process of sending an email to a program committee, asking who will attend the PC dinner, automatically collecting the responses, and tallying them up. We define both logical and decision-theoretic models where an email process is modeled as a set of updates to a data set on which we specify goals via certain constraints or utilities. We then describe a set of inference problems that arise while trying to satisfy these goals and analyze their computational tractability. In particular, we show that for the logical model it is possible to automatically infer which email responses are acceptable w.r.t. a set of constraints in polynomial time, and for the decision-theoretic model it is possible to compute the optimal message-handling policy in polynomial time. In addition, we show how to automatically generate explanations for a process's actions, and identify cases where such explanations can be generated in polynomial time. Finally, we discuss our publicly available implementation of semantic email and outline research challenges in this realm.1\",\n",
       " 'Social Dilemmas: The Anatomy of Cooperation. The study of social dilemmas is the study of the tension between individual and collective rationality. In a social dilemma, individually reasonable behavior leads to a situation in which everyone is worse off. The first part of this review is a discussion of categories of social dilemmas and how they are modeled. The key two-person social dilemmas (PrisonerÃ\\xads Dilemma, Assurance, Chicken) and multiple-person social dilemmas (public goods dilemmas and commons dilemmas) are examined. The second part is an extended treatment of possible solutions for social dilemmas. These solutions are organized into three broad categories based on whether the solutions assume egoistic actors and whether the structure of the situation can be changed: Motivational solutions assume actors are not completely egoistic and so give some weight to the outcomes of their partners. Strategic solutions assume egoistic actors, and neither of these categories of solutions involve changing the fundamental structure of the situation. Solutions that do involve changing the rules of the game are considered in the section on structural solutions. I conclude the review with a discussion of current research and directions for future work.',\n",
       " 'On the criteria to be used in decomposing systems into modules. This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a {Ã}*{Â}*{Â}*modularization{Ã}*{Â}*{Â}* is dependent upon the criteria used in dividing the system into modules. A system design problem is presented and both a conventional and unconventional decomposition are described. It is shown that the unconventional decompositions have distinct advantages for the goals outlined. The criteria used in arriving at the decompositions are discussed. The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases. An alternative approach to implementation which does not have this effect is sketched.',\n",
       " \"Flash: An efficient and portable Web server. This paper presents the design of a new Web server architecture called the asymmetric multiprocess event-driven (AMPED) architecture, and evaluates the performance of an implementation of this architecture, the Flash Web server.  The Flash Web server combines the high performance of single-process event-driven servers on cached workloads with the performance of multi-process and multi-threaded servers on disk-bound workloads.  Furthermore, the Flash Web server is easily portable since it achieves these results using facilites available in all modern operating systems. The performance of different Web server architectures is evaluated in the context of a single implementation in order to quantify the impact of a server's concurrency architecture on its performance.  Furthermore, the performance of Flash is compared with two widely-used Web servers, Apache and Zeus.  Results indicate that Flash can match or exceed the performance of existing Web servers by up to 50% across a wide range of real workloads.  We also present results that show the contribution of various optimizations embedded in Flash.\",\n",
       " 'Fast algorithm for detecting community structure in networks. It has been found that many networks display community structure -- groups of vertices within which connections are dense but between which they are sparser -- and highly sensitive computer algorithms have in recent years been developed for detecting such structure. These algorithms however are computationally demanding, which limits their application to small networks. Here we describe a new algorithm which gives excellent results when tested on both computer-generated and real-world networks and is much faster, typically thousands of times faster than previous algorithms. We give several example applications, including one to a collaboration network of more than 50000 physicists.',\n",
       " 'Finding community structure in very large networks. The discovery and analysis of community structure in networks is a topic of considerable recent interest within the physics community, but most methods proposed so far are unsuitable for very large networks because of their computational cost. Here we present a hierarchical agglomeration algorithm for detecting community structure which is faster than many competing algorithms: its running time on a network with n vertices and m edges is O (md log n) where d is the depth of the dendrogram describing the community structure. Many real-world networks are sparse and hierarchical, with m approximately n and d approximately log n, in which case our algorithm runs in essentially linear time, O (n log(2) n). As an example of the application of this algorithm we use it to analyze a network of items for sale on the web site of a large on-line retailer, items in the network being linked if they are frequently purchased by the same buyer. The network has more than 400 000 vertices and 2 x 10(6) edges. We show that our algorithm can extract meaningful communities from this network, revealing large-scale patterns present in the purchasing habits of customers.',\n",
       " 'NP-complete Problems and Physical Reality. Can NP-complete problems be solved efficiently in the physical universe? I survey proposals including soap bubbles, protein folding, quantum computing, quantum advice, quantum adiabatic algorithms, quantum-mechanical nonlinearities, hidden variables, relativistic time dilation, analog computing, Malament-Hogarth spacetimes, quantum gravity, closed timelike curves, and “anthropic computing. ” The section on soap bubbles even includes some “experimental ” results. While I do not believe that any of the proposals will let us solve NP-complete problems efficiently, I argue that by studying them, we can learn something not only about computation but also about physics. 1',\n",
       " 'Formal Ontology and Information Systems. Research on ontology is becoming increasingly widespread in the computer science community, and its importance is being recognized in a multiplicity of research fields and application areas, including knowledge engineering, database design and integration, information retrieval and extraction. We shall use the generic term “information systems”, in its broadest sense, to collectively refer to these application perspectives. We argue in this paper that so-called ontologies present their own methodological and architectural peculiarities: on the methodological side, their main peculiarity is the adoption of a highly interdisciplinary approach, while on the architectural side the most interesting aspect is the centrality of the role they can play in an information system, leading to the perspective of ontology-driven information systems.',\n",
       " 'Kernel Independent Component Analysis. \\t\\t\\tWe present a class of algorithms for independent component analysis (ICA) which use contrast functions based on canonical correlations in a reproducing kernel Hilbert space. On the one hand, we show that our contrast functions are related to mutual information and have desirable mathematical properties as measures of statistical dependence. On the other hand, building on recent developments in kernel methods, we show that these criteria and their derivatives can be computed efficiently. Minimizing these criteria leads to flexible and robust algorithms for ICA. We illustrate with simulations involving a wide variety of source distributions, showing that our algorithms outperform many of the presently known algorithms.',\n",
       " 'Local graph alignment and motif search in biological networks.. Interaction networks are of central importance in postgenomic molecular biology, with increasing amounts of data becoming available by high-throughput methods. Examples are gene regulatory networks or protein interaction maps. The main challenge in the analysis of these data is to read off biological functions from the topology of the network. Topological motifs, i.e., patterns occurring repeatedly at different positions in the network, have recently been identified as basic modules of molecular information processing. In this article, we discuss motifs derived from families of mutually similar but not necessarily identical patterns. We establish a statistical model for the occurrence of such motifs, from which we derive a scoring function for their statistical significance. Based on this scoring function, we develop a search algorithm for topological motifs called graph alignment, a procedure with some analogies to sequence alignment. The algorithm is applied to the gene regulation network of Escherichia coli.',\n",
       " 'A Translation Approach to Portable Ontology Specifications. To support the sharing and reuse of formally represented knowledge among AI systems, it is useful to define the common vocabulary in which shared knowledge is represented. A specification of a representational vocabulary for a shared domain of discourse––definitions of classes, relations, functions, and other objects––is called an ontology. This paper describes a mechanism for defining ontologies that are portable over representation systems. Definitions written in a standard format for predicate calculus are translated by a system called Ontolingua into specialized representations, including frame-based systems as well as relational languages. This allows researchers to share and reuse ontologies, while retaining the computational benefits of specialized implementations. We discuss how the translation approach to portability addresses several technical problems. One problem is how to accommodate the stylistic and organizational differences among representations while preserving declarative content. Another is how to translate from a very expressive language into restricted languages, remaining system-independent while preserving the computational efficiency of implemented systems. We describe how these problems are addressed by basing Ontolingua itself on an ontology of domain-independent, representational idioms.',\n",
       " 'OurGrid: An approach to easily assemble grids with equitable resource sharing. Available grid technologies like the Globus Toolkit make possible for one to run a parallel application on resources distributed across several administrative domains. Most grid computing users, however, don’t have access to more than a handful of resources onto which they can use this technologies. This happens mainly because gaining access to resources still depends on personal negotiations between the user and each resource owner of resources. To address this problem, we are developing the OurGrid resources sharing system, a peer-to-peer network of sites that share resources equitably in order to form a grid to which they all have access. The resources are shared accordingly to a network of favors model, in which each peer prioritizes those who have credit in their past history of bilateral interactions. The emergent behavior in the system is that peers that contribute more to the community are prioritized when they request resources. We expect, with OurGrid, to solve the access gaining problem for users of bag-of-tasks applications (those parallel applications whose tasks are independent).',\n",
       " \"Discouraging Free Riding in a Peer-to-Peer CPUSharing Grid. Grid computing has excited many with the promise of access to huge amounts of resources distributed across the globe. However, there are no largely adopted solutions for automatically assembling grids, and this limits the scale of today's grids. Some argue that this is due to the overwhelming complexity of the proposed economy-based solutions. Peer-to-peer grids have emerged as a less complex alternative. We are currently deploying OurGrid, one such peer-to-peer grid. OurGrid is a CPU-sharing grid that targets Bag-of-Tasks applications (i.e. parallel applications whose tasks are independent). In order to ease system deployment, OurGrid is based on a very lightweight autonomous reputation scheme. Free riding is an important issue for any peer-to-peer system. The aim of this paper is to show that OurGrid's reputation system successfully discourages free riding, making it in each peer's own interest to collaborate with the peer-to-peer community. We show this in two steps. First, we analyze the conditions under which a reputation scheme can discourage free riding in a CPU-sharing grid. Second, we show that OurGrid's reputation scheme satisfies these conditions, even in the presence of malicious peers. Unlike other distributed mechanisms for discouraging free riding, OurGrid's reputation scheme achieves this without requiring a shared cryptographic infrastructure or specialized storage.\",\n",
       " 'Inferring dynamic architecture of cellular networks using time series of gene expression, protein and metabolite data.. MOTIVATION: High-throughput technologies have facilitated the acquisition of large genomics and proteomics datasets. However, these data provide snapshots of cellular behavior, rather than help us reveal causal relations. Here, we propose how these technologies can be utilized to infer the topology and strengths of connections among genes, proteins and metabolites by monitoring time-dependent responses of cellular networks to experimental interventions. RESULTS: We demonstrate that all connections leading to a given network node, e.g. to a particular gene, can be deduced from responses to perturbations none of which directly influences that node, e.g. using strains with knock-outs to other genes. To infer all interactions from stationary data, each node should be perturbed separately or in combination with other nodes. Monitoring time series provides richer information and does not require perturbations to all nodes. Overall, the methods we propose are capable of deducing and quantifying functional interactions within and across cellular gene, signaling and metabolic networks. SUPPLEMENTARY INFORMATION: Supplementary material is available at http://www.dbi.tju.edu/bioinformatics2004.pdf',\n",
       " \"Control of internal and external noise in genetic regulatory networks.. Positive and negative feedback loops, for example, where a protein regulates its own transcription, play an important role in many genetic regulatory networks. Such systems will be subject to internal noise, which occurs due to the small number of molecules taking part in some reactions. This paper examines the effect of feedback loops on noise levels. Error growth techniques from nonlinear dynamics are used to estimate the variance of a system around a steady-state attractor. It is shown that variablity due to intrinsic stochasticity is directly linked to the stability of the steady state, and therefore to the system's resistance to external perturbations. The methods are demonstrated for a number of simple systems, including a genetic switch with homo-dimerizing regulatory protein, and an oscillator.\",\n",
       " 'The UNIX TimeSharing System. Unix is a generalpurpose, multiuser, interactive operating system for the larger Digital Equipment Corporation PDP11 and the Interdata 8/32 computers. It offers a number of features seldom found even in larger operating systems, including  i A hierarchical file system incorporating demountable volumes,  ii Compatible file, device, and interprocess I/O,  iii The ability to initiate asynchronous processes,  iv System command language selectable on a peruser basis,  v Over 100 subsystems including a dozen languages,  vi High degree of portability. This paper discusses the nature and implementation of the file system and of the user command interface.',\n",
       " 'A Framework for Representing Knowledge. This is a partial theory of thinking, combining a number of classical and modern concepts from psychology, linguistics, and AI. Whenever one encounters a new situation (or makes a substantial change in one\\'s viewpoint) he selects from memory a structure called a frame, a remembered framework to be adopted to fit reality by changing details as necessary. A frame is a data-structure for representing a stereotyped situation, like being in a certain kind of living room, or going to a child\\'s birthday party. Attached to each frame are several kinds of information. Some of this information is about how to use the frame. Some is about what one can expect to happen next. Some is about what to do if these expectations are not confirmed. The \"top levels\" of a frame are fixed, and represent things that are always true about the supposed situation. The lower levels have many \"alota\" that must be filled by specific instances or data. Collections of related frames are linked together into frame-systems. The effects of important actions are mirrored by transformations between the frames of a system. These are used to make certain kinds of calculations economical, to represent changes of emphasis and attention and to account for effectiveness of \"imagery\". In Vision, the different frames of a system describe the scene from different viewpoints, and the transformations between one frame and another represent the effects of moving from place to place. Other kinds of frame-systems can represent actions, cause-effect relations, or changes in conceptual viewpoint. The paper applies the frame-system idea also to problems of linguistic understanding: memory, acquisition and retrieval of knowledge, and a variety of ways to reason by analogy and jump to conclusions based on partial similarity matching.',\n",
       " 'A survey of medical image registration. The purpose of this paper is to present a survey of recent (published in 1993 or later) publications concerning medical image registration techniques. These publications will be classified according to a model based on nine salient criteria, the main dichotomy of which is extrinsic versus intrinsic methods. The statistics of the classification show definite trends in the evolving registration techniques, which will be discussed. At this moment, the bulk of interesting intrinsic methods is based on either segmented points or surfaces, or on techniques endeavouring to use the full information content of the images involved.',\n",
       " 'Effective erasure codes for reliable computer communication protocols. Reliable communication protocols require that all the intended recipients of a message receive the message intact. Automatic Repeat reQuest (ARQ) techniques are used in unicast protocols, but they do not scale well to multicast protocols with large groups of receivers, since segment losses tend to become uncorrelated thus greatly reducing the effectiveness of retransmissions. In such cases, Forward Error Correction (FEC) techniques can be used, consisting in the transmission of redundant...',\n",
       " 'Mining and summarizing customer reviews. Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques.',\n",
       " \"Does Training in Second-Language Word Recognition Skills Affect Reading Comprehension? An Experimental Study. Two classroom-based experiments investigated automatization of lexical access in a second language (L2) with a computer-based training, involving a Grade 8 population in the Netherlands, with Dutch first language (L1) and intermediate knowledge of L2 English. Results of the first experiment showed that the students' lexical access was faster and less variable for words on which they were trained than for words on which they were not trained. In the second experiment, lexical access for some words was accelerated but was not more automatic. There was no transfer of acceleration of lexical access to reading speed or to higher-order text comprehension. Various explanations for the findings are considered and the notion of automatization of L2 word recognition is discussed. Further research should follow up on this study, which is the first one to test a possible causal link between speed of word access and higher-order L2 reading comprehension.\",\n",
       " 'Decoding by Linear Programming. This paper considers a natural error correcting problem with real valued input/output. We wish to recover an input vector f∈R n  from corrupted measurements y=Af+e. Here, A is an m by n (coding) matrix and e is an arbitrary and unknown vector of errors. Is it possible to recover f exactly from the data y? We prove that under suitable conditions on the coding matrix A, the input f is the unique solution to the ℓ 1 -minimization problem (  x   ℓ1 :=Σ i  x i  ) min(g∈R n )   y - Ag   ℓ1  provided that the support of the vector of errors is not too large,   e   ℓ0 := {i:e i  ≠ 0} ≤ρ·m for some ρ>0. In short, f can be recovered exactly by solving a simple convex optimization problem (which one can recast as a linear program). In addition, numerical experiments suggest that this recovery procedure works unreasonably well; f is recovered exactly even in situations where a significant fraction of the output is corrupted. This work is related to the problem of finding sparse solutions to vastly underdetermined systems of linear equations. There are also significant connections with the problem of recovering signals from highly incomplete measurements. In fact, the results introduced in this paper improve on our earlier work. Finally, underlying the success of ℓ 1  is a crucial property we call the uniform uncertainty principle that we shall describe in detail.',\n",
       " 'Ontologies: a silver bullet for knowledge management and electronic commerce. {Ontologies have been developed and investigated for some time in artificial intelligence to facilitate knowledge sharing and reuse. More recently, the notion of ontologies has attracted attention from fields such as databases, intelligent information integration, cooperative information systems, information retrieval, electronic commerce, enterprise application integration, and knowledge management. This broadened interest in ontologies is based on the feature that they provide a machine-processable semantics of information sources that can be communicated among agents as well as between software artifacts and humans. This feature makes ontologies the backbone technology of the next web generation, i.e., the Semantic Web. Ontologies are currently applied in areas such as knowledge management in large company-wide networks and call centers, and in B2C, B2G, and B2B electronic commerce. In a nutshell, ontologies enable effective and efficient access to heterogeneous and distributed information sources. Given the increasing amount of information available online, this kind of support is becoming more important day by day. The author systematically introduces the notion of ontologies to the non-expert reader and demonstrates in detail how to apply this conceptual framework for improved intranet retrieval of corporate information and knowledge and for enhanced Internet-based electronic commerce. He also describes ontology languages (XML, RDF, and OWL) and ontology tools, and the application of ontologies. In addition to structural improvements, the second edition covers recent developments relating to the Semantic Web, and emerging web-based standard languages. _\\\\\\\\__\\\\\\\\__\\\\\\\\__\\\\\\\\__\\\\\\\\__}',\n",
       " 'The use and analysis of microarray data.. Functional genomics is the study of gene function through the parallel expression measurements of genomes, most commonly using the technologies of microarrays and serial analysis of gene expression. Microarray usage in drug discovery is expanding, and its applications include basic research and target discovery, biomarker determination, pharmacology, toxicogenomics, target selectivity, development of prognostic tests and disease-subclass determination. This article reviews the different ways to analyse large sets of microarray data, including the questions that can be asked and the challenges in interpreting the measurements.',\n",
       " 'Navigating gene expression using microarrays [mdash] a technology review. Parallel quantification of large numbers of messenger RNA transcripts using microarray technology promises to provide detailed insight into cellular processes involved in the regulation of gene expression. This should allow new understanding of signalling networks that operate in the cell and of the molecular basis and classification of disease. But can the technology deliver such far-reaching promises?',\n",
       " 'Data Mining. Group 14 used data-mining strategies to evaluate a number of issues, including appropriate diagnosis, haplotype estimation, genetic linkage and association studies, and type I error. Methods ranged from exploratory analyses, to machine learning strategies (neural networks, supervised learning, and tree-based methods), to false discovery rate control of type I errors. The general motivations were to find the \"story\" in the data and to summarize information from a multitude of measures. Several methods illustrated strategies for better trait definition, using summarization of related traits. In the few studies that sought to identify genes for alcoholism, there was little agreement among the different strategies, likely reflecting the complexities of the disease. Nevertheless, Group 14 found that these methods offered strategies to gain a better understanding of the complex pathways by which disease develops.',\n",
       " \"Probability density estimation for the interpretation of neural population codes. 1. Electrophysiological recording data from multiple cells in motor cortex and elsewhere often are interpreted using the population vector method pioneered by Georgopoulos and coworkers. This paper proposes an alternative method for interpreting coding across populations of cells that may succeed under circumstances in which the population vector fails. 2. Population codes are analyzed using probability theory to find the complete conditional probability density of a movement parameter given the firing pattern of a set of cells. 3. The conditional probability density when a single cell fires is proportional to the shape of the cell's tuning curve of firing rate in response to different movement parameters. 4. The conditional density when multiple cells fire is proportional to the product of their tuning curves. 5. Movement parameters can be estimated from the conditional density using statistical maximum likelihood or minimum mean-squared error methods. 6. Simulations show that density estimation correctly finds movement directions for nonuniform distributions of preferred directions and noncosine cell tuning curves, whereas the population vector method fails for these cases. 7. Probability methods thus provide a statistically based alternative to the population vector for interpreting electrophysiological recording data from multiple cells.\",\n",
       " 'Tuning curve sharpening for orientation selectivity: coding efficiency and the impact of correlations. Several studies have shown that the information conveyed by bell-shaped tuning curves increases as their width decreases, leading to the notion that sharpening of tuning curves improves population codes. This notion, however, is based on assumptions that the noise distribution is independent among neurons and independent of the tuning curve width. Here we reexamine these assumptions in networks of spiking neurons by using orientation selectivity as an example. We compare two principal classes of model: one in which the tuning curves are sharpened through cortical lateral interactions, and one in which they are not. We report that sharpening through lateral interactions does not improve population codes but, on the contrary, leads to a severe loss of information. In addition, the sharpening models generate complicated codes that rely extensively on pairwise correlations. Our study generates several experimental predictions that can be used to distinguish between these two classes of model.',\n",
       " 'Role of experience and oscillations in transforming a rate code into a temporal code.. In the vast majority of brain areas, the firing rates of neurons, averaged over several hundred milliseconds to several seconds, can be strongly modulated by, and provide accurate information about, properties of their inputs. This is referred to as the rate code. However, the biophysical laws of synaptic plasticity require precise timing of spikes over short timescales (<10 ms)1, 2. Hence it is critical to understand the physiological mechanisms that can generate precise spike timing in vivo, and the relationship between such a temporal code and a rate code. Here we propose a mechanism by which a temporal code can be generated through an interaction between an asymmetric rate code and oscillatory inhibition. Consistent with the predictions of our model, the rate3, 4 and temporal5-7 codes of hippocampal pyramidal neurons are highly correlated. Furthermore, the temporal code becomes more robust with experience. The resulting spike timing satisfies the temporal order constraints of hebbian learning. Thus, oscillations and receptive field asymmetry may have a critical role in temporal sequence learning.',\n",
       " 'Blending by Blogging: weblogs in blended learning initiatives. Weblogs (‘blogs’) are emerging in many educational contexts as vehicles for personal expression and the dissemination and critique of Internet materials. The study of the weblog phenomenon in itself can convey important insights about social construction; hundreds of thousands of blogs emerged worldwide within a fairly short time span without considerable direction from corporations or other institutions. Strategic approaches toward blended learning environments are often instructor-centered, with control of the mix of educational approaches in the instructor’s hands. In contrast, weblogs are a flexible medium that can be used in approaches that provide educational participants with a ‘middle space’ of options as to how to integrate face-to-face and online modes. Weblog construction encourages the development of individual, critical voices within the broader context of classroom interactions.',\n",
       " 'Anomaly detection of web-based attacks. Web-based vulnerabilities represent a substantial portion of the security exposures of computer networks. In order to detect known web-based attacks, misuse detection systems are equipped with a large number of signatures. Unfortunately, it is di#cult to keep up with the daily disclosure of web-related vulnerabilities, and, in addition, vulnerabilities may be introduced by installation-specific web-based applications. Therefore, misuse detection systems should be complemented with anomaly...',\n",
       " \"Diffusion of renewable energy technologies--barriers and stakeholders' perspectives. This paper presents the results of a survey administered to households, personnel belonging to industry and commercial establishments, and policy experts with the objective of eliciting their views on the barriers to the diffusion of renewable energy technologies (RETs). Taking the Maharashtra State, India, as a case study, the paper develops a systematic classification of barriers to the adoption of RETs (economic, technological, market and institutional) and ranking them based on the perceptions of various stakeholders. The results provide evidence of how the consumers receive RET information and make decisions using their limited analytical capabilities. The analysis is used to enhance the knowledge by introducing ideas based on behavioural theory. Not only do these ideas help understanding the consumer perspective, they also help develop policy interventions. The aim is to define each barrier and describe its mode of influence that will help to develop policy measures for the removal of each barrier.\",\n",
       " 'Humans integrate visual and haptic information in a statistically optimal fashion.. When a person looks at an object while exploring it with their hand, vision and touch both provide information for estimating the properties of the object. Vision frequently dominates the integrated visual±haptic percept, for example when judging size, shape or position, but in some circumstances the percept is clearly affected by haptics. Here we propose that a general principle, which minimizes variance in the final estimate, determines the degree to which vision or haptics dominates. This principle is realized by using maximum-likelihood estimation to combine the inputs. To investigate cue combination quantitatively, we first measured the variances associated with visual and haptic estimation of height.We then used these measurements to construct a maximum-likelihood integrator. This model behaved very similarly to humans in a visual±haptic task. Thus, the nervous system seems to combine visual and haptic information in a fashion that is similar to a maximum-likelihood integrator. Visual dominance occurs when the variance associated with visual estimation is lower than that associated with haptic estimation.',\n",
       " 'Experience-dependent asymmetric shape of hippocampal receptive fields.. We propose a novel parameter, namely, the skewness, or asymmetry, of the shape of a receptive field to characterize two properties of hippocampal place fields. First, a majority of hippocampal receptive fields on linear tracks are negatively skewed, such that during a single pass the firing rate is low as the rat enters the field but high as it exits. Second, while the place fields are symmetric at the beginning of a session, they become highly asymmetric with experience. Further experiments suggest that these results are likely to arise due to synaptic plasticity during behavior. Using a purely feed forward neural network model, we show that following repeated directional activation, NMDA-dependent long-term potentiation/long-term depotentiation (LTP/LTD) could result in an experience-dependent asymmetrization of receptive fields.',\n",
       " 'Coordinated interactions between hippocampal ripples and cortical spindles during slow-wave sleep.. Sleep is characterized by a structured combination of neuronal oscillations. In the hippocampus, slow-wave sleep (SWS) is marked by high-frequency network oscillations (approximately 200 Hz \"ripples\"), whereas neocortical SWS activity is organized into low-frequency delta (1-4 Hz) and spindle (7-14 Hz) oscillations. While these types of hippocampal and cortical oscillations have been studied extensively in isolation, the relationships between them remain unknown. Here, we demonstrate the existence of temporal correlations between hippocampal ripples and cortical spindles that are also reflected in the correlated activity of single neurons within these brain structures. Spindle-ripple episodes may thus constitute an important mechanism of cortico-hippocampal communication during sleep. This coactivation of hippocampal and neocortical pathways may be important for the process of memory consolidation, during which memories are gradually translated from short-term hippocampal to longer-term neocortical stores.',\n",
       " 'Memory of sequential experience in the hippocampus during slow wave sleep.. Rats repeatedly ran through a sequence of spatial receptive fields of hippocampal CA1 place cells in a fixed temporal order. A novel combinatorial decoding method reveals that these neurons repeatedly fired in precisely this order in long sequences involving four or more cells during slow wave sleep (SWS) immediately following, but not preceding, the experience. The SWS sequences occurred intermittently in brief ( approximately 100 ms) bursts, each compressing the behavioral sequence in time by approximately 20-fold. This rapid encoding of sequential experience is consistent with evidence that the hippocampus is crucial for spatial learning in rodents and the formation of long-term memories of events in time in humans.',\n",
       " 'Temporally structured replay of awake hippocampal ensemble activity during rapid eye movement sleep.. Human dreaming occurs during rapid eye movement (REM) sleep. To investigate the structure of neural activity during REM sleep, we simultaneously recorded the activity of multiple neurons in the rat hippocampus during both sleep and awake behavior. We show that temporally sequenced ensemble firing rate patterns reflecting tens of seconds to minutes of behavioral experience are reproduced during REM episodes at an equivalent timescale. Furthermore, within such REM episodes behavior-dependent modulation of the subcortically driven theta rhythm is also reproduced. These results demonstrate that long temporal sequences of patterned multineuronal activity suggestive of episodic memory traces are reactivated during REM sleep. Such reactivation may be important for memory processing and provides a basis for the electrophysiological examination of the content of dream states.',\n",
       " 'An architecture for biological information extraction and representation.. MOTIVATIONS: Technological advances in biomedical research are generating a plethora of heterogeneous data at a high rate. There is a critical need for extraction, integration and management tools for information discovery and synthesis from these heterogeneous data. RESULTS: In this paper, we present a general architecture, called ALFA, for information extraction and representation from diverse biological data. The ALFA architecture consists of: (i) a networked, hierarchical, hyper-graph object model for representing information from heterogeneous data sources in a standardized, structured format; and (ii) a suite of integrated, interactive software tools for information extraction and representation from diverse biological data sources. As part of our research efforts to explore this space, we have currently prototyped the ALFA object model and a set of interactive software tools for searching, filtering, and extracting information from scientific text. In particular, we describe BioFerret, a meta-search tool for searching and filtering relevant information from the web, and ALFA Text Viewer, an interactive tool for user-guided extraction, disambiguation, and representation of information from scientific text. We further demonstrate the potential of our tools in integrating the extracted information with experimental data and diagrammatic biological models via the common underlying ALFA representation. CONTACT: aditya_vailaya@agilent.com.',\n",
       " \"A Dynamic Model of Social Network Formation. We consider a dynamic social network model in which agents play repeated games in pairings determined by a stochastically evolving social network. Individual agents begin to interact at random, with the interactions modeled as games. The game payoffs determine which interactions are reinforced, and the network structure emerges as a consequence of the dynamics of the agents' learning behavior. We study this in a variety of game-theoretic conditions and show that the behavior is complex and sometimes dissimilar to behavior in the absence of structural dynamics. We argue that modeling network structure as dynamic increases realism without rendering the problem of analysis intractable. 10.1073/pnas.97.16.9340\",\n",
       " 'Whole-Genome Patterns of Common DNA Variation in Three Human Populations. Individual differences in DNA sequence are the genetic basis of human variability. We have characterized whole-genome patterns of common human DNA variation by genotyping 1,586,383 single-nucleotide polymorphisms (SNPs) in 71 Americans of European, African, and Asian ancestry. Our results indicate that these SNPs capture most common genetic variation as a result of linkage disequilibrium, the correlation among common SNP alleles. We observe a strong correlation between extended regions of linkage disequilibrium and functional genomic elements. Our data provide a tool for exploring many questions that remain regarding the causal role of common human DNA variation in complex human traits and for investigating the nature of genetic variation within and between human populations.',\n",
       " 'Culture and the Self: Implications for Cognition, Emotion, and Motivation. People in different cultures have strikingly different construals of the self, of others, and of the interdependence of the 2.  These construals can influence, and in many cases determine, the very nature of individual experience, including cognition, emotion, and motivation.  Many Asian cultures have distinct conceptions of individuality that insist on the fundamental relatedness of individuals to each other.  The emphasis is on attending to others, fitting in, and harmonious interdependence with them.  American culture neither assumes nor values such an overt connectedness among individuals.  In contrast, individuals seek to maintain their independence from others by attending to the self and by discovering and expressing their unique inner attributes.  As proposed herein, these construals are even more powerful than previously imagined.  Theories of the self from both psychology and anthropology are integrated to define in detail the difference between a construal of the self as independent and a construal of the self as interdependent.  Each of these divergent construals should have a set of specific consequences for cognition, emotion, and motivation; these consequences are proposed and relevant empirical literature is reviewed.  Focusing on differences in self-construals enables apparently inconsistent empirical findings to be reconciled, and raises questions about what have been thought to be culture-free aspects of cognition, emotion, and motivation.',\n",
       " 'Prospect Theory: An Analysis of Decision under Risk. This paper presents a critique of expected utility theory as a descriptive model of decision making under risk, and develops an alternative model, called prospect theory.  Choices among risky prospects exhibit several pervasive effects that are inconsistent with the basic tenets of utility theory.  In particular, people underweight outcomes that are merely probable in comparison with outcomes that are obtained with certainty.  This tendency, called the certainty effect, contributes to risk aversion in choices involving sure gains and to risk seeking in choices involving sure losses.  In addition, people generally discard components that are shared by all prospects under consideration.  This tendency, called that isolation effect, leads to inconsistent preferences when the same choice is presented in different forms.  An alternative theory of choice is developed, in which value is assigned to gains and losses rather than to final assets and in which probabilities are replaced by decision weights.  The value function is normally concave for gains, commonly convex for losses, and is generally steeper for losses than for gains.  Decision weights are generally lower than the corresponding probabilities, except in the range of low probabilities.  Overweighting of low probabilities may contribute to the attractiveness of both insurance and gambling.',\n",
       " 'Bee foraging in uncertain environments using predictive hebbian learning.. Recent work has identified a neuron with widespread projections to odour processing regions of the honeybee brain whose activity represents the reward value of gustatory stimuli. We have constructed a model of bee foraging in uncertain environments based on this type of neuron and a predictive form of hebbian synaptic plasticity. The model uses visual input from a simulated three-dimensional world and accounts for a wide range of experiments on bee learning during foraging, including risk aversion. The predictive model shows how neuromodulatory influences can be used to bias actions and control synaptic plasticity in a way that goes beyond standard correlational mechanisms. Although several behavioural models of conditioning in bees have been proposed, this model is based on the neural substrate and was tested in a simulation of bee flight.',\n",
       " 'An Intrusion-Detection Model. A model of a real-time intrusion-detection expert system capable of detecting break-ins, penetrations, and other forms of computer abuse is described. The model is based on the hypothesis that security violations can be detected by monitoring a system&#039;s audit records for abnormal patterns of system usage. The model includes profiles for representing the behavior of subjects with respect to objects in terms of metrics and statistical models, and rules for acquiring knowledge about this behavior from audit records and for detecting anomalous behavior. The model is independent of any particular system, application environment, system vulnerability, or type of intrusion, thereby providing a framework for a general-purpose intrusion-detection expert system.',\n",
       " 'Detecting Intrusions using System Calls: Alternative Data Models. Intrusion detection systems rely on a wide variety of observable data to distinguish between legitimate and illegitimate activities. We study one such observable-sequences of system calls into the kernel of an operating system. Using system-call data sets generated by several different programs, we compare the ability of different data modeling methods to represent normal behavior accurately and to recognize intrusions. We compare the following methods: simple enumeration of observed sequences; comparison of relative frequencies of different sequences; a rule induction technique; and hidden Markov models (HMMs). We discuss the factors affecting the performance of each method and conclude that for this particular problem, weaker methods than HMMs are likely sufficient',\n",
       " 'Intrusion Detection via Static Analysis. One of the primary challenges in intrusion detection is modelling typical application behavior so that we can recognize attacks by their atypical effects without raising too many false alarms. We show how static analysis may be used to automatically derive a model of application behavior. The result is a host-based intrusion detection system with three advantages: a high degree of automation, protection against a broad class of attacks based on corrupted code, and the elimination of false alarms. We report on our experience with a prototype implementation of this technique',\n",
       " 'Testing Intrusion detection systems: a critique of the 1998 and 1999 DARPA intrusion detection system evaluations as performed by Lincoln Laboratory. In 1998 and again in 1999, the Lincoln Laboratory of MIT conducted a comparative evaluation of intrusion detection systems (IDSs) developed under DARPA funding. While this evaluation represents a significant and monumental undertaking, there are a number of issues associated with its design and execution that remain unsettled. Some methodologies used in the evaluation are questionable and may have biased its results. One problem is that the evaluators have published relatively little concerning some of the more critical aspects of their work, such as validation of their test data. The appropriateness of the evaluation techniques used needs further investigation. The purpose of this article is to attempt to identify the shortcomings of the Lincoln Lab effort in the hope that future  efforts of this kind will be placed on a sounder footing. Some of the problems that the article points out might well be resolved if the evaluators were to publish a detailed description of their procedures and the rationale that led to their adoption, but other problems would clearly remain./par>',\n",
       " \"The Strength of Weak Ties. Analysis of social networks is suggested as a tool for linking micro and macro levels of sociological theory. The procedure is illustrated by elaboration of the macro implications of one aspect of small-scale interaction: the strength of dyadic ties. It is argued that the degree of overlap of two individuals' friendship networks varies directly with the strength of their tie to one another. The impact of this principle on diffusion of influence and information, mobility opportunity, and community organization is explored. Stress is laid on the cohesive power of weak ties. Most network models deal, implicitly, with strong ties, thus confining their applicability to small, well-defined groups. Emphasis on weak ties lends itself to discussion of relations between groups and to analysis of segments of social structure not easily defined in terms of primary groups.\",\n",
       " 'Information processing in the primate visual system: an integrated systems perspective. The primate visual system contains dozens of distinct areas in the cerebral cortex and several major subcortical structures. These subdivisions are extensively interconnected in a distributed hierarchical network that contains several intertwined processing streams. A number of strategies are used for efficient information processing within this hierarchy. These include linear and nonlinear filtering, passage through information bottlenecks, and coordinated use of multiple types of information. In addition, dynamic regulation of information flow within and between visual areas may provide the computational flexibility needed for the visual system to perform a broad spectrum of tasks accurately and at high resolution. 10.1126/science.1734518',\n",
       " \"A framework for defining logics. The Edinburgh Logical Framework (LF) provides a means to define (or present) logics. It is based on a general treatment of syntax, rules, and proofs by means of a typed -calculus with dependent types. Syntax is treated in a style similar to, but more general than, Martin-Löf's system of arities. The treatment of rules and proofs focuses on his notion of a judgement. Logics are represented in LF via a new principle, the judgements as types principle, whereby each judgement is identified with the ...\",\n",
       " 'A new notation for arrows. The categorical notion of monad, used by Moggi to structure denotational descriptions, has proved to be a powerful tool for structuring combinator libraries. Moreover, the monadic programming style provides a convenient syntax for many kinds of computation, so that each library defines a new sublanguage. Recently, several workers have proposed a generalization of monads, called variously “arrows ” or Freyd-categories. The extra generality promises to increase the power, expressiveness and efficiency of the embedded approach, but does not mesh as well with the native abstraction and application. Definitions are typically given in a point-free style, which is useful for proving general properties, but can be awkward for programming specific instances. In this paper we define a simple extension to the functional language Haskell that makes these new notions of computation more convenient to use. Our language is similar to the monadic style, and has similar reasoning properties. Moreover, it is extensible, in the sense that new combining forms can be defined as expressions in the host language. 1.',\n",
       " 'Generalising monads to arrows. this paper. Pleasingly, the arrow interface turned out to be applicable to other kinds of non-monadic library also, for example the fudgets library for graphical user interfaces [CH93], and a new library for programming active web pages. These applications will be described in sections 6 and 9. While arrows are a little less convenient to use than monads, they have significantly wider applicability. They can therefore be used to bring the benefits of monad-like programming to a much wider class of applications. 2 Background: Library Design Using Monads',\n",
       " \"A Point Process Framework for Relating Neural Spiking Activity to Spiking History, Neural Ensemble, and Extrinsic Covariate Effects. Multiple factors simultaneously affect the spiking activity of individual neurons. Determining the effects and relative importance of these factors is a challenging problem in neurophysiology. We propose a statistical framework based on the point process likelihood function to relate a neuron's spiking probability to three typical covariates: the neuron's own spiking history, concurrent ensemble activity, and extrinsic covariates such as stimuli or behavior. The framework uses parametric models of the conditional intensity function to define a neuron's spiking probability in terms of the covariates. The discrete time likelihood function for point processes is used to carry out model fitting and model analysis. We show that, by modeling the logarithm of the conditional intensity function as a linear combination of functions of the covariates, the discrete time point process likelihood function is readily analyzed in the generalized linear model (GLM) framework. We illustrate our approach for both GLM and non-GLM likelihood functions using simulated data and multivariate single-unit activity data simultaneously recorded from the motor cortex of a monkey performing a visuomotor pursuit-tracking task. The point process framework provides a flexible, computationally efficient approach for maximum likelihood estimation, goodness-of-fit assessment, residual analysis, model selection, and neural decoding. The framework thus allows for the formulation and analysis of point process models of neural spiking activity that readily capture the simultaneous effects of multiple covariates and enables the assessment of their relative importance. 10.1152/jn.00697.2004\",\n",
       " \"Generalized Additive Models. Generalized Additive Models: An Introduction with R imparts a thorough understanding of the theory and practical applications of GAMs and related advanced models, enabling informed use of these flexible tools. The author's approach is based on a framework of penalized regression splines, and he builds the necessary foundation through motivating chapters on linear and generalized linear models. Use of the freely available R software helps explain the theory and illustrates the practicalities of linear, generalized linear, and generalized additive models, as well as their mixed effect extensions. An R package containing the featured datasets is freely available on the Internet.\",\n",
       " 'Specificity and Stability in Topology of Protein Networks. Molecular networks guide the biochemistry of a living cell on multiple levels: Its metabolic and signaling pathways are shaped by the network of interacting proteins, whose production, in turn, is controlled by the genetic regulatory network. To address topological properties of these two networks, we quantified correlations between connectivities of interacting nodes and compared them to a null model of a network, in which all links were randomly rewired. We found that for both interaction and regulatory networks, links between highly connected proteins are systematically suppressed, whereas those between a highly connected and low-connected pairs of proteins are favored. This effect decreases the likelihood of cross talk between different functional modules of the cell and increases the overall robustness of a network by localizing effects of deleterious perturbations.',\n",
       " 'Scale-free brain functional networks. Functional magnetic resonance imaging (fMRI) is used to extract <em> functional networks</em> connecting correlated human brain sites. Analysis of the resulting networks in different tasks shows that: (a) the distribution of functional connections, and the probability of finding a link vs. distance are both scale-free, (b) the characteristic path length is small and comparable with those of equivalent random networks, and (c) the clustering coefficient is orders of magnitude larger than those of equivalent random networks. All these properties, typical of scale-free small world networks, reflect important functional information about brain states.',\n",
       " 'The Semantic Web. The entertainment system was belting out the Beatles\\' \"We Can Work It Out\" when the phone rang. When Pete answered, his phone turned the sound down by sending a message to all the other local devices that had a volume control. His sister, Lucy, was on the line from the doctor\\'s office: \"Mom needs to see a specialist and then has to have a series of physical therapy sessions. Biweekly or something. I\\'m going to have my agent set up the appointments.\" Pete immediately agreed to share the chauffeuring.',\n",
       " 'Business Process Execution Language for Web Services version 1.1. This document defines a notation for specifying business process behavior based on Web Services. This notation is called Business Process Execution Language for Web Services (abbreviated to BPEL4WS in the rest of this document). Processes in BPEL4WS export and import functionality by using Web Service interfaces exclusively. Business processes can be described in two ways. [1.] Executable business processes model actual behavior of a participant in a business interaction. [2.] Business protocols, in contrast, use process descriptions that specify the mutually visible message exchange behavior of each of the parties involved in the protocol, without revealing their internal behavior. The process descriptions for business protocols [2.] are called abstract processes. BPEL4WS is meant to be used to model the behavior of both executable [1.] and abstract processes [. BPEL4WS provides a language for the formal specification of business processes and business interaction protocols. By doing so, it extends the Web Services interaction model and enables it to support business transactions. BPEL4WS defines an interoperable integration model that should facilitate the expansion of automated process integration in both the intra-corporate and the business-to-business spaces.',\n",
       " 'Identification of prokaryotic and eukaryotic signal peptides and prediction of their cleavage sites. We have developed a new method for the identification of signal peptides and their cleavage sites based on neural networks trained on separate sets of prokaryotic and eukaryotic sequence. The method performs significantly better than previous prediction schemes and can easily be applied on genome-wide data sets. Discrimination between cleaved signal peptides and uncleaved N-terminal signal-anchor sequences is also possible, though with lower precision. Predictions can be made on a publicly available WWW server.',\n",
       " 'The framing of decisions and the psychology of choice. The psychological principles that govern the perception of decision problems and the evaluation of probabilities and outcomes produce predictable shifts of preference when the same problem is framed in different ways. Reversals of preference are demonstrated in choices regarding monetary outcomes, both hypothetical and real, and in questions pertaining to the loss of human lives. The effects of frames on preferences are compared to the effects of perspectives on perceptual appearance. The dependence of preferences on the formulation of decision problems is a significant concern for the theory of rational choice.',\n",
       " 'A generic protein purification method for protein complex characterization and proteome exploration.. We have developed a generic procedure to purify proteins expressed at their natural level under native conditions using a novel tandem affinity purification (TAP) tag. The TAP tag allows the rapid purification of complexes from a relatively small number of cells without prior knowledge of the complex composition, activity, or function. Combined with mass spectrometry, the TAP strategy allows for the identification of proteins interacting with a given target protein. The TAP method has been tested in yeast but should be applicable to other cells or organisms.',\n",
       " 'Judgment under Uncertainty: Heuristics and Biases. The thirty-five chapters in this book describe various judgmental heuristics and the biases they produce, not only in laboratory experiments but in important social, medical, and political situations as well. Individual chapters discuss the representativeness and availability heuristics, problems in judging covariation and control, overconfidence, multistage inference, social perception, medical diagnosis, risk perception, and methods for correcting and improving judgments under uncertainty. About half of the chapters are edited versions of classic articles; the remaining chapters are newly written for this book. Most review multiple studies or entire subareas of research and application rather than describing single experimental studies. This book will be useful to a wide range of students and researchers, as well as to decision makers seeking to gain insight into their judgments and to improve them.',\n",
       " 'Mfold web server for nucleic acid folding and hybridization prediction. 10.1093/nar/gkg595 The abbreviated name, â\\x80\\x98mfold web serverâ\\x80\\x99, describes a number of closely related software applications available on the World Wide Web (WWW) for the prediction of the secondary structure of single stranded nucleic acids. The objective of this web server is to provide easy access to RNA and DNA folding and hybridization software to the scientific community at large. By making use of universally available web GUIs (Graphical User Interfaces), the server circumvents the problem of portability of this software. Detailed output, in the form of structure plots with or without reliability information, single strand frequency plots and â\\x80\\x98energy dot plotsâ\\x80\\x99, are available for the folding of single sequences. A variety of â\\x80\\x98bulkâ\\x80\\x99 servers give less information, but in a shorter time and for up to hundreds of sequences at once. The portal for the mfold web server is http://www.bioinfo.rpi.edu/applications/mfold. This URL will be referred to as â\\x80\\x98MFOLDROOTâ\\x80\\x99.',\n",
       " 'Scrap your boilerplate: a practical design pattern for generic programming. We describe a design pattern for writing programs that traverse data structures built from rich mutually-recursive data types. Such programs often have a great deal of \"boilerplate\" code that simply walks the structure, hiding a small amount of \"real\" code that constitutes the reason for the traversal.Our technique allows most of this boilerplate to be written once and for all, or even generated mechanically, leaving the programmer free to concentrate on the important part of the algorithm. These generic programs are much more adaptive when faced with data structure evolution because they contain many fewer lines of type-specific code.Our approach is simple to understand, reasonably efficient, and it handles all the data types found in conventional functional programming languages. It makes essential use of rank-2 polymorphism, an extension found in some implementations of Haskell. Further it relies on a simple type-safe cast operator.',\n",
       " \"Generics for the masses. A generic function is a function that can be instantiated on many data types to obtain data type specific functionality. Examples of generic functions are the functions that can be derived in Haskell, such as show, read, and '=='. The recent years have seen a number of proposals that support the definition of generic functions. Some of the proposals define new languages, some define extensions to existing languages. As a common characteristic none of the proposals can be made to work within Haskell 98: they all require something extra, either a more sophisticated type system or an additional language construct. The purpose of this pearl is to show that one can, in fact, program generically within Haskell 98 obviating to some extent the need for fancy type systems or separate tools. Haskell's type classes are at the heart of this approach: they ensure that generic functions can be defined succinctly and, in particular, that they can be used painlessly.\",\n",
       " 'Scrap more boilerplate: reflection, zips, and generalised casts. Writing boilerplate code is a royal pain. Generic programming promises to alleviate this pain by allowing the programmer to write a generic \"recipe\" for boilerplate code, and use that recipe in many places. In earlier work we introduced the \"Scrap your boilerplate\" approach to generic programming, which exploits Haskell\\'s existing type-class mechanism to support generic transformations and queries.This paper completes the picture. We add a few extra \"introspective\" or \"reflective\" facilities, that together support a rich variety of serialisation and de-serialisation. We also show how to perform generic \"zips\", which at first appear to be somewhat tricky in our framework. Lastly, we generalise the ability to over-ride a generic function with a type-specific one.All of this can be supported in Haskell with independently-useful extensions: higher-rank types and type-safe cast. The GHC implementation of Haskell readily derives the required type classes for user-defined data types.',\n",
       " \"Associated types with class. Haskell's type classes allow ad-hoc overloading, or type-indexing, of  functions . A natural generalisation is to allow type-indexing of  data types  as well. It turns out that this idea directly supports a powerful form of abstraction called  associated types , which are available in C++ using traits classes. Associated types are useful in many applications, especially for self-optimising libraries that adapt their data representations and algorithms in a type-directed manner.In this paper, we introduce and motivate associated types as a rather natural generalisation of Haskell's existing type classes. Formally, we present a type system that includes a type-directed translation into an explicitly typed target language akin to System F; the existence of this translation ensures that the addition of associated data types to an existing Haskell compiler only requires changes to the front end.\",\n",
       " \"Ownership types for object encapsulation. Ownership types provide a statically enforceable way of specifying object encapsulation and enable local reasoning about program correctness in object-oriented languages. However, a type system that enforces strict object encapsulation is too constraining: it does not allow efficient implementation of important constructs like iterators. This paper argues that the right way to solve the problem is to allow objects of classes defined in the same module to have privileged access to each other's representations; we show how to do this for inner classes. This approach allows programmers to express constructs like iterators and yet supports local reasoning about the correctness of the classes, because a class and its inner classes together can be reasoned about as a module. The paper also sketches how we use our variant of ownership types to enable efficient software upgrades in persistent object stores.\",\n",
       " 'Basic local alignment search tool.. {A new approach to rapid sequence comparison, basic local alignment search tool (BLAST), directly approximates alignments that optimize a measure of local similarity, the maximal segment pair (MSP) score. Recent mathematical results on the stochastic properties of MSP scores allow an analysis of the performance of this method as well as the statistical significance of alignments it generates. The basic algorithm is simple and robust; it can be implemented in a number of ways and applied in a variety of contexts including straightforward DNA and protein sequence database searches, motif searches, gene identification searches, and in the analysis of multiple regions of similarity in long DNA sequences. In addition to its flexibility and tractability to mathematical analysis, BLAST is an order of magnitude faster than existing sequence comparison tools of comparable sensitivity.}',\n",
       " 'A computer perspective of membranes: molecular dynamics studies of lipid bilayer systems.. This paper was followed by two others from Stouch and coworkers, on the diffusion of small molecules in lipid membranes [70,97]. The membrane model explored in these studies is a fully hydrated DMPC bilayer, which was shown to compare favorably to most available experimental data [61]. The first paper deals with the diffusion proces of benzene. Four simulations were conducted; three of them with a single benzene molecule placed at different positions in the membrane, and another simulation with ...',\n",
       " \"General conditions for predictivity in learning theory.. Developing theoretical foundations for learning is a key step towards understanding intelligence. 'Learning from examples' is a paradigm in which systems (natural or artificial) learn a functional relationship from a training set of examples. Within this paradigm, a learning algorithm is a map from the space of training sets to the hypothesis space of possible functional solutions. A central question for the theory is to determine conditions under which a learning algorithm will generalize from its finite training set to novel examples. A milestone in learning theory was a characterization of conditions on the hypothesis space that ensure generalization for the natural class of empirical risk minimization (ERM) learning algorithms that are based on minimizing the error on the training set. Here we provide conditions for generalization in terms of a precise stability property of the learning process: when the training set is perturbed by deleting one example, the learned hypothesis does not change much. This stability property stipulates conditions on the learning map rather than on the hypothesis space, subsumes the classical theory for ERM algorithms, and is applicable to more general algorithms. The surprising connection between stability and predictivity has implications for the foundations of learning theory and for the design of novel algorithms, and provides insights into problems as diverse as language learning and inverse problems in physics and engineering.\",\n",
       " 'A Taxonomy of Incentive Patterns - the Design Space of Incentives for Cooperation. In ad hoc networks, devices must cooperate in order to compensate for the absence of infrastructure. Yet, autonomous devices are free to decide whether to cooperate or not. Hence, incentives are indispensable to induce cooperation between autonomous devices. Recently, several approaches have been suggested that stimulate cooperation among devices. In this report, we point out that these approaches fall short of exploiting the design space of incentives for cooperation. Therefore, we introduce incentive patterns as a means of systematically conceiving incentive schemes with respect to the specifics of the application environment. Based on economics, we derive several incentive patterns and discuss them with respect to a set of general characteristics. Consequently, we propose a taxonomy that classifies the derived incentive patterns. Lastly, we exemplify systematic design of incentive schemes in the context of our DIANE project.',\n",
       " 'How Social Structure Improves Distributed Reputation Systems - Three Hypotheses. Reputation systems provide an incentive for cooperation in artificial societies by keeping track of the behavior of autonomous entities. The self-organization of P2P systems demands for the distribution of the reputation system to the autonomous entities themselves. They may cooperate by issuing recommendations of other entities’ trustworthiness. The recipient of a recommendation has to assess its truthfulness and consistency before taking it into account. The current assessment methods are based on plausibility considerations that have several inherent limitations. In our previous work, we have suggested the application of non-repudiable tokens that overcome most of the limitations. However, there remain limitations that are not overcome or only partly  overcome. Therefore, in this paper, we propose social structure as a complementary means of overcoming the remaining limitations of plausibility considerations. For this purpose, we examine the properties of social structure and discuss how distributed reputation systems can make use of them. This leads us to the formulation of three hypotheses of how social structure overcomes the limitations of plausibility considerations. The hypotheses are tested by the means of simulation. The simulation results corroborate two hypotheses and indicate the validity of the third hypothesis.',\n",
       " 'Inferring Cellular Networks Using Probabilistic Graphical Models. High-throughput genome-wide molecular assays, which probe cellular networks from different perspectives, have become central to molecular biology. Probabilistic graphical models are useful for extracting meaningful biological insights from the resulting data sets. These models provide a concise representation of complex cellular networks by composing simpler submodels. Procedures based on well-understood principles for inferring such models from data facilitate a model-based methodology for analysis and discovery. This methodology and its capabilities are illustrated by several recent applications to gene expression data. 10.1126/science.1094068',\n",
       " 'OWL Web Ontology Language Overview. The {OWL} Web Ontology Language is designed for use by applications that need to process the content of information instead of just presenting information to humans. {OWL} facilitates greater machine interpretability of Web content than that supported by {XML,} {RDF,} and {RDF} Schema {(RDF-S)} by providing additional vocabulary along with a formal semantics. {OWL} has three increasingly-expressive sublanguages: {OWL} Lite, {OWL} {DL,} and {OWL} Full. This document is written for readers who want a first impression of the capabilities of {OWL.} It provides an introduction to {OWL} by informally describing the features of each of the sublanguages of {OWL.} Some knowledge of {RDF} Schema is useful for understanding this document, but not essential. After this document, interested readers may turn to the {OWL} Guide for more detailed descriptions and extensive examples on the features of {OWL.} The normative formal definition of {OWL} can be found in the {OWL} Semantics and Abstract Syntax.',\n",
       " 'Dynamo: a transparent dynamic optimization system. We describe the design and implementation of Dynamo, a software dynamic optimization system that is capable of transparently improving the performance of a native instruction stream as it executes on the processor. The input native instruction stream to Dynamo can be dynamically generated (by a JIT for example), or it can come from the execution of a statically compiled native binary. This paper evaluates the Dynamo system in the latter, more challenging situation, in order to emphasize the limits, rather than the potential, of the system. Our experiments demonstrate that even statically optimized native binaries can be accelerated Dynamo, and often by a significant degree. For example, the average performance of –O optimized SpecInt95 benchmark binaries created by the HP product C compiler is improved to a level comparable to their –O4 optimized version running without Dynamo. Dynamo achieves this by focusing its efforts on optimization opportunities that tend to manifest only at runtime, and hence opportunities that might be difficult for a static compiler to exploit. Dynamo’s operation is transparent in the sense that it does not depend on any user annotations or binary instrumentation, and does not require multiple runs, or any special compiler, operating system or hardware support. The Dynamo prototype presented here is a realistic implementation running on an HP PA-8000 workstation under the HPUX 10.20 operating system. 1.',\n",
       " 'Peer-to-Peer Communication Across Network Address Translators. J’fais des trous, des petits trous... toujours des petits trous- S. Gainsbourg Network Address Translation (NAT) causes well-known difficulties for peer-to-peer (P2P) communication, since the peers involved may not be reachable at any globally valid IP address. Several NAT traversal techniques are known, but their documentation is slim, and data about their robustness or relative merits is slimmer. This paper documents and analyzes one of the simplest but most robust and practical NAT traversal techniques, commonly known as “hole punching. ” Hole punching is moderately well-understood for UDP communication, but we show how it can be reliably used to set up peer-to-peer TCP streams as well. After gathering data on the reliability of this technique on a wide variety of deployed NATs, we find that about 82 % of the NATs tested support hole punching for UDP, and about 64 % support hole punching for TCP streams. As NAT vendors become increasingly conscious of the needs of important P2P applications such as Voice over IP and online gaming protocols, support for hole punching is likely to increase in the future. 1',\n",
       " 'Unraveling the Web services web: an introduction to SOAP, WSDL, and UDDI. Today, Web services are emerging to provide a systematic and extensible framework for application-to-application interaction, built on top of existing Web protocols and based on open XML standards. At this point, Web services technology is still emerging, and researchers are still developing important pieces. Developers can take advantage of the available specifications and tooling now incorporate more modules as the technology matures.',\n",
       " 'Global mapping of the yeast genetic interaction network.. A genetic interaction network containing approximately 1000 genes and approximately 4000 interactions was mapped by crossing mutations in 132 different query genes into a set of approximately 4700 viable gene yeast deletion mutants and scoring the double mutant progeny for fitness defects. Network connectivity was predictive of function because interactions often occurred among functionally related genes, and similar patterns of interactions tended to identify components of the same pathway. The genetic network exhibited dense local neighborhoods; therefore, the position of a gene on a partially mapped network is predictive of other genetic interactions. Because digenic interactions are common in yeast, similar networks may underlie the complex genetics associated with inherited phenotypes in other organisms.',\n",
       " 'Finders, keepers? The present and future perfect in support of personal information management. To keep or not to keep? People continually face variations of this decision as they encounter information. A large percentage of information encountered is clearly useless — junk e–mail, for example. Another portion of encountered information can be \"used up\" and disposed of in a single read — the weather report or a sports score, for example. That leaves a great deal of information in a middle ground. The information might be useful somewhere at sometime in the future. Decisions concerning whether and how to keep this information are an essential part of personal information management. Bad decisions either way can be costly. Information not kept or not kept properly may be unavailable later when it is needed. But keeping too much information can also be costly. The wrong information competes for attention and may obscure information more appropriate to the current task. These are the logical costs of a signal detection task. From this perspective, one approach in tool support is to try to decrease the costs of a false positive (keeping useless information) and a miss (not keeping useful information). But this reduction in the costs of keeping mistakes is likely to be bounded by fundamental limitations in the human ability to remember and to attend. A second approach suggested by the theory of signal detectability is relatively less explored: Develop tools that decrease the likelihood that \"keeping\" mistakes are made in the first place.',\n",
       " 'Modeling Multilevel Data Structures. Multilevel data are structures that consist of multiple units of analysis, one nested within the other. Such data are becoming quite common in political science and provide numerous opportunities for theory testing and development. Unfortunately, this type of data typically generates a number of statistical problems, of which clustering is particularly important. To exploit the opportunities offered by multilevel data, and to solve the statistical problems inherent in them, special statistical techniques are required. In this article, we focus on a technique that has become popular in educational statistics and sociology-multilevel analysis. In multilevel analysis, researchers build models that capture the layered structure of multilevel data, and determine how layers interact and impact a dependent variable of interest. Our objective in this article is to introduce the logic and statistical theory behind multilevel models, to illustrate how such models can be applied fruitfully in political science, and to call attention to some of the pitfalls in multilevel analysis.',\n",
       " \"The psychology of personal information management. A requirement of 'The Office of the Future' is that it provides us with an effective way of storing and retrieving information. But existing IT products go nowhere near supporting the variety of activities which can be observed in paper-based offices, and it is not surprising that concepts of the 'paperless office' are as far off as they were when the idea was first mooted. This paper illustrates how many of the issues involved in the automation of information management are essentially psychological in nature. These principally devolve upon the processes of recall, recognition and categorisation. Examples of existing information management techniques show how there is a trend to automate with a view to simulating office practices, or to develop according to the availability of technological solutions. Both of these are inefficient with respect to the user's psychological needs. A framework for developing user-oriented information management systems is discussed and relevant research issues presented.\",\n",
       " 'A probabilistic view of gene function.. Cells are controlled by the complex and dynamic actions of thousands of genes. With the sequencing of many genomes, the key problem has shifted from identifying genes to knowing what the genes do; we need a framework for expressing that knowledge. Even the most rigorous attempts to construct ontological frameworks describing gene function (e.g., the Gene Ontology project) ultimately rely on manual curation and are thus labor-intensive and subjective. But an alternative exists: the field of functional genomics is piecing together networks of gene interactions, and although these data are currently incomplete and error-prone, they provide a glimpse of a new, probabilistic view of gene function. We outline such a framework, which revolves around a statistical description of gene interactions derived from large, systematically compiled data sets. In this probabilistic view, pleiotropy is implicit, all data have errors and the definition of gene function is an iterative process that ultimately converges on the correct functions. The relationships between the genes are defined by the data, not by hand. Even this comprehensive view fails to capture key aspects of gene function, not least their dynamics in time and space, showing that there are limitations to the model that must ultimately be addressed.',\n",
       " \"Learning Bayesian Network Structure from Massive Datasets: The ''Sparse Candidate'' Algorithm. Learning Bayesian networks is often cast as an optimization problem, where the computational task is to find a structure that maximizes a statistically motivated score. By and large, existing learning tools address this optimization problem using standard heuristic search techniques. Since the search space is extremely large, such search procedures can spend most of the time examining candidates that are extremely unreasonable. This problem becomes critical when we deal with data sets that are large either in the number of instances, or the number of attributes. In this paper, we introduce an algorithm that achieves faster learning by restricting the search space. This iterative algorithm restricts the parents of each variable to belong to a small subset of candidates. We then search for a network that satisfies these constraints. The learned network is then used for selecting better candidates for the next iteration. We evaluate this algorithm both on synthetic and real-life data. Our...\",\n",
       " 'Expert Finding Systems for Organizations: Problem and Domain Analysis and the DEMOIR Approach. Computer systems that augment the process of finding the right expert for a given problem in an organization or world-wide are becoming feasible more than ever before, thanks to the prevalence of corporate Intranets and the Internet. This paper investigates such systems in two parts. We first explore the expert finding problem in depth, review and analyze existing systems in this domain, and suggest a domain model that can serve as a framework for design and development decisions. Based on our...',\n",
       " 'Searching social networks. A referral system is a multiagent system whose member agents are capable of giving and following referrals. The specific cases of interest arise where each agent has a user. The agents cooperate by giving and taking referrals so each can better help its user locate relevant information. This use of referrals mimics human interactions and can potentially lead to greater effectiveness and efficiency than in single-agent systems.Existing approaches consider what referrals may be given and treat the referring process simply as path search in a static graph. By contrast, the present approach understands referrals as arising in and influencing dynamic social networks, where the agents act autonomously based on local knowledge. This paper studies strategies using which agents may search dynamic social networks. It evaluates the proposed approach empirically for a community of AI scientists (partially derived from bibliographic data). Further, it presents a prototype system that assists users in finding other users in practical social networks.',\n",
       " \"A Multi-agent System for Collaborative Bookmarking. In this paper we describe a multi-agent system, called COWING that aims at enabling an organised group of people to share their results of information searching on the World Wide Web. Users normally save relevant sites they found in their private collections of bookmarks. The goal of the proposed system is to enable users to share their bookmarks, in an implicit, secure and effective way. By implicit we mean that users are not required to spend extra effort in order to use the system. Secure sharing refers to the capacity of each user to control who knows what about her/his own bookmark collections. Finally effectiveness is ensured by recommending users with relevant bookmarks that are computed by applying a distributed collaborative filtering algorithm. Recommended bookmarks are automatically inserted in the most appropriate bookmark folder in the user's local collection. When accessing a folder the user can evaluate (accept or reject) provided recommendations. Our system is a multi-agent system where each user is associated to an assistant agent, called a WING agent. A WING agent observes the user's behaviour when managing her/his own bookmark collection. It learns how the user classifies her/his bookmarks. A hybrid neural/case-based reasoning supervised classifier is used for this purpose. WING agents exchange bookmarks according to a defined collaboration protocol that protects associated users privacy.\",\n",
       " 'Indeterminacy in brain and behavior.. The central goal of modern science that evolved during the Enlightenment was the empirical reduction of uncertainty by experimental inquiry. Although there have been challenges to this view in the physical sciences, where profoundly indeterminate events have been identified at the quantum level, the presumption that physical phenomena are fundamentally determinate seems to have defined modern behavioral science. Programs like those of the classical behaviorists, for example, were explicitly anchored to a fully deterministic worldview, and this anchoring clearly influenced the experiments that those scientists chose to perform. Recent advances in the psychological, social, and neural sciences, however, have caused a number of scholars to begin to question the assumption that all of behavior can be regarded as fundamentally deterministic in character. Although it is not yet clear whether the generative mechanisms for human and animal behavior will require a philosophically indeterminate approach, it is clear that behavioral scientists of all kinds are beginning to engage the issues of indeterminacy that plagued physics at the beginning of the twentieth century.',\n",
       " 'Neuroeconomics: The Consilience of Brain and Decision. Economics, psychology, and neuroscience are converging today into a single, unified discipline with the ultimate aim of providing a single, general theory of human behavior. This is the emerging field of neuroeconomics in which consilience, the accordance of two or more inductions drawn from different groups of phenomena, seems to be operating. Economists and psychologists are providing rich conceptual tools for understanding and modeling behavior, while neurobiologists provide tools for the study of mechanism. The goal of this discipline is thus to understand the processes that connect sensation and action by revealing the neurobiological mechanisms by which decisions are made. This review describes recent developments in neuroeconomics from both behavioral and biological perspectives.',\n",
       " 'The neurobiology of visual-saccadic decision making.. Over the past two decades significant progress has been made toward understanding the neural basis of primate decision making, the biological process that combines sensory data with stored information to select and execute behavioral responses. The most striking progress in this area has been made in studies of visual-saccadic decision making, a system that is becoming a model for understanding decision making in general. In this system, theoretical models of efficient decision making developed in the social sciences are beginning to be used to describe the computations the brain must perform when it connects sensation and action. Guided in part by these economic models, neurophysiologists have been able to describe neuronal activity recorded from the brains of awake-behaving primates during actual decision making. These recent studies have examined the neural basis of decisions, ranging from those made in predictable sensorimotor tasks to those unpredictable decisions made when animals are engaged in strategic conflict. All of these experiments seem to describe a surprisingly well-integrated set of physiological mechanisms that can account for a broad range of behavioral phenomena. This review presents many of these recent studies within the emerging neuroeconomic framework for understanding primate decision making.ABSTRACT FROM AUTHOR',\n",
       " \"Making choices: the neurophysiology of visual-saccadic decision making.. {Imagine the decisions you might make while playing a simple game like `matching pennies.' At each play, you and your opponent, say the mathematician John vonNeumann, each lay down a penny heads or tails up. If both pennies show the same side, vonNeumann wins, if not, you win. Before each play, you have the subjective experience of deciding what to do: of choosing whether to play heads or tails. Although decisions like these are not yet understood at a physiological level, progress has been made towards understanding simple decision making in at least one model system: the primate neural architecture that uses visual data and prior knowledge about patterns in the environment to select and execute saccades. Both the visual system and the brainstem circuits that control saccadic eye movements are particularly well understood, making it possible for physiologists to begin to study the connections between these sensory and motor processes at a level of complexity that would be impossible in other less well understood systems.}\",\n",
       " 'Neural coding of basic reward terms of animal learning theory, game theory, microeconomics and behavioural ecology.. Neurons in a small number of brain structures detect rewards and reward-predicting stimuli and are active during the expectation of predictable food and liquid rewards. These neurons code the reward information according to basic terms of various behavioural theories that seek to explain reward-directed learning, approach behaviour and decision-making. The involved brain structures include groups of dopamine neurons, the striatum including the nucleus accumbens, the orbitofrontal cortex and the amygdala. The reward information is fed to brain structures involved in decision-making and organisation of behaviour, such as the dorsolateral prefrontal cortex and possibly the parietal cortex. The neural coding of basic reward terms derived from formal theories puts the neurophysiological investigation of reward mechanisms on firm conceptual grounds and provides neural correlates for the function of rewards in learning, approach behaviour and decision-making.',\n",
       " 'The Eyes Have It: A Task by Data Type Taxonomy for Information Visualizations. A useful starting point for designing advanced graphical user interfaces is the Visual Information-Seeking Mantra: overview first, zoom and filter, then details on demand. But this is only a starting point in trying to understand the rich and varied set of information visualizations that have been proposed in recent years. This paper offers a task by data type taxonomy with seven data types (one-, two-, three-dimensional data, temporal and multi-dimensional data, and tree and network data) and seven tasks (overview, zoom, filter, details-on-demand, relate, history, and extracts).',\n",
       " 'The challenge of information visualization evaluation. As the field of information visualization matures, the tools and ideas described in our research publications are reaching users. The reports of usability studies and controlled experiments are helpful to understand the potential and limitations of our tools, but we need to consider other evaluation approaches that take into account the long exploratory nature of users tasks, the value of potential discoveries or the benefits of overall awareness. We need better metrics and benchmark repositories to compare tools, and we should also seek reports of successful adoption and demonstrated utility.',\n",
       " 'A unified taxonomic framework for information visualization. We present a taxonomy for Information Visualization (IV) that characterizes it in terms of data, task, skill and context, as well as a number of dimensions that relate to the input and output hardware, the software tools, as well as user interactions and human perceptual abilities. We illustrate the utility of the taxonomy by focusing particularly on the information retrieval task and the importance of taking into account human perceptual capabilities and limitations. Although the relevance of Psychology to IV is often recognised, we have seen relatively little translation of psychological results and theory to practical IV applications.This paper targets the better development of information visualizations through the introduction of a framework delineating the major factors in interface development. We believe that higher quality visualizations will result from structured developments that take into account these considerations and that the framework will also serve to assist the development of effective evaluation and assessment processes.',\n",
       " 'Synchrony-dependent propagation of firing rate in iteratively constructed networks in vitro. The precise role of synchronous neuronal firing in signal encoding remains unclear. To examine what kinds of signals can be carried by synchrony, I reproduced a multilayer feedforward network of neurons in an in vitro slice preparation of rat cortex using an iterative procedure. When constant and time-varying frequency signals were delivered to the network, the firing of neurons in successive layers became progressively more synchronous. Notably, synchrony in the in vitro network developed even with uncorrelated input, persisted under a wide range of physiological conditions and was crucial for the stable propagation of rate signals. The firing rate was represented by a classical rate code in the initial layers, but switched to a synchrony-based code in the deeper layers.',\n",
       " 'Tracking evolving communities in large linked networks.. We are interested in tracking changes in large-scale data by periodically creating an agglomerative clustering and examining the evolution of clusters (communities) over time. We examine a large real-world data set: the NEC CiteSeer database, a linked network of >250,000 papers. Tracking changes over time requires a clustering algorithm that produces clusters stable under small perturbations of the input data. However, small perturbations of the CiteSeer data lead to significant changes to most of the clusters. One reason for this is that the order in which papers within communities are combined is somewhat arbitrary. However, certain subsets of papers, called natural communities, correspond to real structure in the CiteSeer database and thus appear in any clustering. By identifying the subset of clusters that remain stable under multiple clustering runs, we get the set of natural communities that we can track over time. We demonstrate that such natural communities allow us to identify emerging communities and track temporal changes in the underlying structure of our network data.',\n",
       " 'Robotics-Based Location Sensing Using Wireless Ethernet. A key subproblem in the construction of location-aware systems is the determination of the position of a mobile device. This article describes the design, implementation and analysis of a system for determining position inside a building from measured RF signal strengths of packets on an IEEE 802.11b wireless Ethernet network. Previous approaches to location-awareness with RF signals have been severely hampered by non-Gaussian signals, noise, and complex correlations due to multi-path effects, interference and absorption. The design of our system begins with the observation that determining position from complex, noisy and non-Gaussian signals is a wellstudied problem in the field of robotics. Using only off-the-shelf hardware, we achieve robust position estimation to within a meter in our experimental context and after adequate training of our system. We can also coarsely determine our orientation and can track our position as we move. Our results show that we can localize a stationary device to within 1.5 meters over 80 % of the time and track a moving device to within 1 meter over 50 % of the time. Both localization and tracking run in real-time. By applying recent advances in probabilistic inference of position and sensor fusion from noisy signals, we show that the RF emissions from base stations as measured by off-the-shelf wireless Ethernet cards are sufficiently rich in information to permit a mobile device to reliably track its location.',\n",
       " 'Analysis of a campus-wide wireless network. Understanding usage patterns in wireless local-area networks (WLANs) is critical for those who develop, deploy, and manage WLAN technology, as well as those who develop systems and application software for wireless networks. This paper presents results from the largest and most comprehensive trace of network activity in a large, production wireless LAN. For eleven weeks we traced the activity of nearly two thousand users drawn from a general campus population, using a campus-wide network of 476 access points spread over 161 buildings at Dartmouth College. Our study expands on those done by Tang and Baker, with a significantly larger and broader population. \\\\\\\\\\\\\\\\par We found that residential traffic dominated all other traffic, particularly in residences populated by newer students; students are increasingly choosing a wireless laptop as their primary computer. Although web protocols were the single largest component of traffic volume, network backup and file sharing contributed an unexpectedly large amount to the traffic. Although there was some roaming within a network session, we were surprised by the number of situations in which cards roamed excessively, unable to settle on one access point. Cross-subnet roams were an especial problem, because they broke IP connections, indicating the need for solutions that avoid or accommodate such roams.',\n",
       " 'A Transport Layer Approach for Achieving Aggregate Bandwidths on Multi-Homed Mobile Hosts. Due to the availability of a wide variety of wireless access technologies, a mobile host can potentially have subscriptions and access to more than one wireless network at a given time. In this paper, we consider such a multi-homed mobile host, and address the problem of achieving bandwidth aggregation by striping data across the multiple interfaces of the mobile host. We show that both link layer striping approaches and application layer techniques that stripe data across multiple TCP sockets do not achieve the optimal bandwidth aggregation due to a variety of factors specific to wireless networks. We propose an end-to-end transport layer approach called  pTCP  that effectively performs bandwidth aggregation on multi homed mobile hosts. We show through simulations that pTCP achieves the desired goals under a variety of network conditions.',\n",
       " 'Ariadne: a secure on-demand routing protocol for ad hoc networks. An ad hoc network is a group of wireless mobile computers (or nodes), in which individual nodes cooperate by forwarding packets for each other to allow nodes to communicate beyond direct wireless transmission range. Prior research in ad hoc networking has generally studied the routing problem in a non-adversarial setting, assuming a trusted environment. In this paper, we present attacks against routing in ad hoc networks, and we present the design and performance evaluation of a new secure on-demand ad hoc network routing protocol, called Ariadne. Ariadne prevents attackers or compromised nodes from tampering with uncompromised routes consisting of uncompromised nodes, and also prevents many types of Denial-of-Service attacks. In addition, Ariadne is efficient, using only highly efficient symmetric cryptographic primitives.',\n",
       " 'A problem-oriented classification of visualization techniques. Progress in scientific visualization could be accelerated if workers could more readily find visualization techniques relevant to a given problem. The authors describe an approach to this problem, based on a classification of visualization techniques, that is independent of particular application domains. A user breaks up a problem into subproblems, describes these subproblems in terms of the objects to be represented and the operations to be supported by a representation, locates applicable visualization techniques in a catalog, and combines these representations into a composite representation for the original problem. The catalog and its underlying classification provide a way for workers in different application disciplines to share methods',\n",
       " \"Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifolds.. The problem of dimensionality reduction arises in many fields of information processing, including machine learning, data compression, scientific visualization, pattern recognition, and neural computation. Here we describe locally linear embedding (LLE), an unsupervised learning algorithm that computes low dimensional, neighborhood preserving embeddings of high dimensional data. The data, assumed to be sampled from an underlying manifold, are mapped into a single global coordinate system of lower dimensionality. The mapping is derived from the symmetries of locally linear reconstructions, and the actual computation of the embedding reduces to a sparse eigenvalue problem. Notably, the optimizations in LLE---though capable of generating highly nonlinear embeddings---are simple to implement, and they do not involve local minima. In this paper, we describe the implementation of the algorithm in detail and discuss several extensions that enhance its performance. We present results of the algorithm applied to data sampled from known manifolds, as well as to collections of images of faces, lips, and handwritten digits. These examples are used to provide extensive illustrations of the algorithm's performance---both successes and failures---and to relate the algorithm to previous and ongoing work in nonlinear dimensionality reduction.\",\n",
       " 'Limits on Efficient Computation in the Physical World. More than a speculative technology, quantum computing seems to challenge our most basic intuitions about how the physical world should behave. In this thesis I show that, while some intuitions from classical computer science must be jettisoned in the light of modern physics, many others emerge nearly unscathed; and I use powerful tools from computational complexity theory to help determine which are which. In the first part of the thesis, I attack the common belief that quantum computing resembles classical exponential parallelism, by showing that quantum computers would face serious limitations on a wider range of problems than was previously known. In particular, any quantum algorithm that solves the collision problem—that of deciding whether a sequence of n integers is one-to-one or two-to-one—must query the sequence ô\\x80\\x80\\x80n1/5\\x01 times. This resolves a question that was open for years; previously no lower bound better than constant was known. A corollary is that there is no “black-box” quantum algorithm to break cryptographic hash functions or solve the Graph Isomorphism problem in polynomial time. I also show that relative to an oracle, quantum computers could not solve NP-complete problems in polynomial time, even with the help of nonuniform “quantum advice states”; and that any quantum algorithm needs ô\\x80\\x80\\x802n/4/n\\x01 queries to find a local minimum of a black-box function on the n-dimensional hypercube. Surprisingly, the latter result also leads to new classical lower bounds for the local search problem. Finally, I give new lower bounds on quantum one-way communication complexity, and on the quantum query complexity of total Boolean functions and recursive Fourier sampling. The second part of the thesis studies the relationship of the quantum computing model to physical reality. I first examine the arguments of Leonid Levin, Stephen Wolfram, and others who believe quantum computing to be fundamentally impossible. I find their arguments unconvincing without a “Sure/Shor separator”—a criterion that separates the already-verified quantum states from those that appear in Shor’s factoring algorithm. I argue that such a separator should be based on a complexity classification of quantum states, and go on to create such a classification. Next I ask what happens to the quantum computing model if we take into account that the speed of light is finite—and in particular, whether Grover’s algorithm still yields a quadratic speedup for searching a database. Refuting a claim by Benioff, I show that the surprising answer is yes. Finally, I analyze hypothetical models of computation that go even beyond quantum computing. I show that many such models would be as powerful as the complexity class PP, and use this fact to give a simple, quantum computing based proof that PP is closed under intersection. On the other hand, I also present one model—wherein we could sample the entire history of a hidden variable—that appears to be more powerful than standard quantum computing, but only slightly so.',\n",
       " 'Bridging Space over Time: Global Virtual Team Dynamics and Effectiveness. Global virtual teams are internationally distributed groups of people with an organizational mandate to make or implement decisions with international components and implications. They are typically assigned tasks that are strategically important and highly complex. They rarely meet in person, conducting almost all of their interaction and decision making using communications technology. Although they play an increasingly important role in multinational organizations, little systematic is known about their dynamics or effectiveness. This study built a grounded theory of global virtual team processes and performance over time. We built a template based on Adaptive Structuration Theory (DeSanctis and Poole 1994) to guide our research, and we conducted a case study, observing three global virtual teams over a period of 21 months. Data were gathered using multiple methods, and qualitative methods were used to analyze them and generate a theory of global virtual team dynamics and effectiveness. First, we propose that effective global virtual team interaction comprises a series of communication incidents, each configured by aspects of the team’s structural and process elements. Effective outcomes were associated with a fit among an interaction incident’s form, decision process, and complexity. Second, effective global virtual teams sequence these incidents to generate a deep rhythm of regular face-toface incidents interspersed with less intensive, shorter incidents using various media. These two insights are discussed with respect to other literature and are elaborated upon in several propositions. Implications for research and practice are also outlined.',\n",
       " 'PathBLAST: a tool for alignment of protein interaction networks.. PathBLAST is a network alignment and search tool for comparing protein interaction networks across species to identify protein pathways and complexes that have been conserved by evolution. The basic method searches for high-scoring alignments between pairs of protein interaction paths, for which proteins of the first path are paired with putative orthologs occurring in the same order in the second path. This technique discriminates between true- and false-positive interactions and allows for functional annotation of protein interaction pathways based on similarity to the network of another, well-characterized species. PathBLAST is now available at http://www.pathblast. org/ as a web-based query. In this implementation, the user specifies a short protein interaction path for query against a target protein--protein interaction network selected from a network database. PathBLAST returns a ranked list of matching paths fromthe target network along with a graphical view of these paths and the overlap among them. Target protein--protein interaction networks are currently available for Helicobacter pylori, Saccharomyces cerevisiae, Caenorhabditis elegans and Drosophila melanogaster. Just as BLAST enables rapid comparison of protein sequences between genomes, tools such as PathBLAST are enabling comparative genomics at the network level.',\n",
       " 'Conserved patterns of protein interaction in multiple species.. To elucidate cellular machinery on a global scale, we performed a multiple comparison of the recently available protein{\\\\^a}\\x80\\x93protein interaction networks of Caenorhabditis elegans, Drosophila melanogaster, and Saccharomyces cerevisiae. This comparison integrated protein interaction and sequence information to reveal 71 network regions that were conserved across all three species and many exclusive to the metazoans. We used this conservation, and found statistically significant support for 4,645 previously undescribed protein functions and 2,609 previously undescribed protein interactions. We tested 60 interaction predictions for yeast by two-hybrid analysis, confirming approximately half of these. Significantly, many of the predicted functions and interactions would not have been identified from sequence similarity alone, demonstrating that network comparisons provide essential biological information beyond what is gleaned from the genome.',\n",
       " 'The automatic construction of large-scale corpora for summarization research. Summarization research is notorious for its lack of adequate corpora: today, there exist only a few small collections of texts whose units have been manually annotated for textual importance. Given the cost and tediousness of the annotation process, it is very unlikely that we will ever manually annotate for textual importance sufficiently large corpora of texts. To circumvent this problem, we have developed an algorithm that constructs such corpora automatically. Our algorithm takes as input an hAbstract, Texti tuple and generates the corresponding Extract, i.e., the set of clauses (sentences) in the Text that were used to write the Abstract. The performance of the algorithm is shown to be close to that of humans by means of an empirical experiment. The experiment also suggests extraction strategies that could improve the performance of automatic summarization systems.',\n",
       " 'The Google File System. We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous distributed file systems, our design has been driven by observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use.',\n",
       " 'Knowledge management in action: integrating knowledge across communities. This paper offers a brief overview and critique of dominant approaches to knowledge management (KM) and its links with innovation. It then draws upon a case study example to offer a closer analysis of the link between KM and the development of communities of practice during processes of innovation. The paper argues, first, that in many cases innovation is an interactive process requiring knowledge and expertise from different functions and layers across the organization. In such cases critical problems concern the integration of knowledge across disparate communities, rather than the sharing of knowledge within communities. Second that if knowledge integration across communities is to develop, a more action-oriented perspective on KM and the development of KM tools is needed.',\n",
       " 'Understanding availability. This paper addresses a simple, yet fundamental question in the design of peer-to-peer systems: What does it mean when we say availability and how does this understanding impact the engineering of practical systems? We argue that existing measurements and models do not capture the complex time-varying nature of availability in todays peer-to-peer environments. Further, we show that unforeseen methodological shortcomings have dramatically biased previous analyses of this phenomenon. As the basis of our study, we empirically characterize the availability of a large peer-to-peer system over a period of 7 days, analyze the dependence of the underlying availability distributions, measure host turnover in the system, and discuss how these results may affect the design of high-availability peer-to-peer services.',\n",
       " 'Comparison of Database Replication Techniques Based on Total Order Broadcast. In this paper, we present a performance comparison of database replication techniques based on total order broadcast. While the performance of total order broadcast-based replication techniques has been studied in previous papers, this paper presents many new contributions. First, it compares with each other techniques that were presented and evaluated separately, usually by comparing them to a classical replication scheme like distributed locking. Second, the evaluation is done using a finer network model than previous studies. Third, the paper compares techniques that offer the same consistency criterion (one-copy serializability) in the same environment using the same settings. The paper shows that, while networking performance has little influence in a LAN setting, the cost of synchronizing replicas is quite high. Because of this, total order broadcast-based techniques are very promising as they minimize synchronization between replicas.',\n",
       " 'Excitatory cortical neurons form fine-scale functional networks.. The specificity of cortical neuron connections creates columns of functionally similar neurons spanning from the pia to the white matter. Here we investigate whether there is an additional, finer level of specificity that creates subnetworks of excitatory neurons within functional columns. We tested for fine-scale specificity of connections to cortical layer 2/3 pyramidal neurons in rat visual cortex by using cross-correlation analyses of synaptic currents evoked by photostimulation. Recording simultaneously from adjacent layer 2/3 pyramidal cells, we find that when they are connected to each other (20% of all recorded pairs) they share common input from layer 4 and within layer 2/3. When adjacent layer 2/3 neurons are not connected to each other, they share very little (if any) common excitatory input from layers 4 and 2/3. In contrast, all layer 2/3 neurons share common excitatory input from layer 5 and inhibitory input from layers 2/3 and 4, regardless of whether they are connected to each other. Thus, excitatory connections from layer 4 to layer 2/3 and within layer 2/3 form fine-scale assemblies of selectively interconnected neurons; inhibitory connections and excitatory connections from layer 5 link neurons across these fine-scale subnetworks. Relatively independent subnetworks of excitatory neurons are therefore embedded within the larger-scale functional architecture; this allows neighbouring neurons to convey information more independently than suggested by previous descriptions of cortical circuitry.',\n",
       " \"Different time courses of learning-related activity in the prefrontal cortex and striatum. To navigate our complex world, our brains have evolved a sophisticated ability to quickly learn arbitrary rules such as 'stop at red'. Studies in monkeys using a laboratory test of this capacity--conditional association learning--have revealed that frontal lobe structures (including the prefrontal cortex) as well as subcortical nuclei of the basal ganglia are involved in such learning. Neural correlates of associative learning have been observed in both brain regions, but whether or not these regions have unique functions is unclear, as they have typically been studied separately using different tasks. Here we show that during associative learning in monkeys, neural activity in these areas changes at different rates: the striatum (an input structure of the basal ganglia) showed rapid, almost bistable, changes compared with a slower trend in the prefrontal cortex that was more in accordance with slow improvements in behavioural performance. Also, pre-saccadic activity began progressively earlier in the striatum but not in the prefrontal cortex as learning took place. These results support the hypothesis that rewarded associations are first identified by the basal ganglia, the output of which 'trains' slower learning mechanisms in the frontal cortex.\",\n",
       " 'Functional cartography of complex metabolic networks. High-throughput techniques are leading to an explosive growth in the size of biological databases and creating the opportunity to revolutionize our understanding of life and disease. Interpretation of these data remains, however, a major scientific challenge. Here, we propose a methodology that enables us to extract and display information contained in complex networks1, 2, 3. Specifically, we demonstrate that we can find functional modules4, 5 in complex networks, and classify nodes into universal roles according to their pattern of intra- and inter-module connections. The method thus yields a ‘cartographic representation’ of complex networks. Metabolic networks6, 7, 8 are among the most challenging biological networks and, arguably, the ones with most potential for immediate applicability9. We use our method to analyse the metabolic networks of twelve organisms from three different superkingdoms. We find that, typically, 80% of the nodes are only connected to other nodes within their respective modules, and that nodes with different roles are affected by different evolutionary constraints and pressures. Remarkably, we find that metabolites that participate in only a few reactions but that connect different modules are more conserved than hubs whose links are mostly within a single module.',\n",
       " 'A survey of peer-to-peer content distribution technologies. Distributed computer architectures labeled “peer-to-peer” are designed for the sharing of computer resources (content, storage, CPU cycles) by direct exchange, rather than requiring the intermediation or support of a centralized server or authority. Peer-to-peer architectures are characterized by their ability to adapt to failures and accommodate transient populations of nodes while maintaining acceptable connectivity and performance.  Content distribution is an important peer-to-peer application on the Internet that has received considerable research attention. Content distribution applications typically allow personal computers to function in a coordinated manner as a distributed storage medium by contributing, searching, and obtaining digital content.  In this survey, we propose a framework for analyzing peer-to-peer content distribution technologies. Our approach focuses on nonfunctional characteristics suchas security, scalability, performance, fairness, and resource management potential, and examines the way in which these characteristics are reflected in—and affected by the architectural design decisions adopted by current peer-to-peer systems. We study current peer-to-peer systems and infrastructure technologies in terms of their distributed object location and routing mechanisms, their approach to content replication, caching and migration, their support for encryption, access control,authentication and identity, anonymity, deniability, accountability and reputation, and their use of resource trading and management schemes.',\n",
       " 'Cooperative inquiry: developing new technologies for children with children. In todays homes and schools, children are emerging as frequent and experienced users of technology [3, 14]. As this trend continues, it becomes increasingly important to ask if we are fulfilling the technology needs of our children. To answer this question, I have developed a research approach that enables young children to have a voice throughout the technology development process. In this paper, the techniques of  cooperative inquiry  will be described along with a theoretical framework that situates this work in the HCI literature. Two examples of technology resulting from this approach will be presented, along with a brief discussion on the  design-centered learning  of team researchers using cooperative inquiry.',\n",
       " \"The Impact of Social Structure on Economic Outcomes. Social structure, especially in the form of social networks, affects economic outcomes for three main reasons. First, social networks affect the flow and the quality of information. Much information is subtle, nuanced and difficult to verify, so actors do not believe impersonal sources and instead rely on people they know. Second, social networks are an important source of reward and punishment, since these are often magnified in their impact when coming from others personally known. Third, trust, by which I mean the confidence that others will do the ``right'' thing despite a clear balance of incentives to the contrary, emerges, if it does, in the context of a social network. Economists have recently devoted considerable attention to the impact of social structure and networks on the economy; for example, see the economists' chapters in Rauch and Casella (2001) (and the illuminating review essay of this volume by Zuckerman, 2003), as well as Dutta and Jackson (2003) and Calvo ? - Armengol (2004). However, I focus here on sociologists' contributions. Sociologists have developed core principles about the interactions of social structure, informa- tion, ability to punish or reward, and trust that frequently recur in their analyses of political, economic and other institutions. I begin by reviewing some of these principles. Building on these, I then discuss how social structures and social networks can affect economic outcomes like hiring, price, productivity and innovation.\",\n",
       " \"Using model checking to find serious file system errors. This paper shows how to use model checking to find serious errors in file systems. Model checking is a formal verification technique tuned for finding corner-case errors by comprehensively exploring the state spaces defined by a system. File systems have two dynamics that make them attractive for such an approach. First, their errors are some of the most serious, since they can destroy persistent data and lead to unrecoverable corruption. Second, traditional testing needs an impractical, exponential number of test cases to check that the system will recover if it crashes at any point during execution. Model checking employs a variety of state-reducing techniques that allow it to explore such vast state spaces efficiently. <BR> <BR> We built a system, FiSC, for model checking file systems. We applied it to three widely-used, heavily-tested file systems: ext3&nbsp;[<CITE>13</CITE></A>], JFS&nbsp;[<CITE>21</CITE></A>], and ReiserFS&nbsp;[<CITE>27</CITE></A>]. We found serious bugs in all of them, 32 in total. Most have led to patches within a day of diagnosis. For each file system, FiSC found demonstrable events leading to the unrecoverable destruction of metadata and entire directories, including the file system root directory ``<TT>/</TT>''.<BR>     <P>\",\n",
       " 'Lexical Frequency Profiles: A Monte Carlo Analysis. This paper reports a set of Monte Carlo simulations designed to evaluate the main claims made by Laufer and Nation about the Lexical Frequency Profile (LFP). Laufer and Nation claim that the LFP is a sensitive and reliable tool for assessing productive vocabulary in L2 speakers, and they suggest it might have a serious role to play in diagnostic evaluations of learners. The simulations suggest that LFP is not in fact all that sensitive. It works best when the groups being compared have very disparate vocabulary sizes, and is probably not sensitive enough to pick up modest changes in vocabulary size.',\n",
       " 'Hop-by-hop congestion control over a wireless multi-hop network. This paper focuses on congestion control over multi-hop, wireless networks. In a wireless network, an important constraint that arises is that due to the MAC (media access control) layer. Many wireless MACs use a time-division strategy for channel access, where, at any point in space, the physical channel can be accessed by a single user at each instant of time. We develop a fair hop-by-hop congestion control algorithm with the MAC constraint being imposed in the form of a channel access time constraint, using an optimization based framework. In the absence of delay, we show that this algorithm is globally stable using a Lyapunov function based approach. Next, in the presence of delay, we show that the hop-by-hop control algorithm has the property of spatial spreading. In other words, focused loads at a particular spatial location in the network get \"smoothed\" over space. We derive bounds on the \"peak load\" at a node, both with hop-by-hop control, as well as with end-to-end control, show that significant gains are achieved with the hop-by-hop scheme, and validate the analytical results with simulation.',\n",
       " 'The civic culture : political attitudes and democracy in five nations. opposition to fascism and communism (totalitarianism): obvious concern at this time with fragile democracy distinguishes democratic institutions from political culture... but also that democratic infrastructure only beginning to be understood in West > image of democracy conveyed to new nations is obscure and stresses ideology and legal norms rather than attitude and feeling... other problem of spreading democracy is that drawn toward gleam and power of technological/scientific revolutions: not difficult to see why attracted by polity in which authoritarian bureacuracy becomes devide for human/social engineering... but would also prefer to deal gently with their own traditional cultural if choice were available > civic culture not modern but mixed modernising-traditional: CP Snow distinguishes humanistic and scientific-technological cultures, but Shils argues that civic culture enables them to interact without destroying each other e.g. Britain: aristocratic Whigs could enter into coalition with nonconformists merchants and industrialists > parliamentary representation in which traditional and modern in dialogue with each other, aristocratic with rational, etc.',\n",
       " \"The Theory of Planned Behavior. Research dealing with various aspects of the theory of planned behavior ( Ajzen, 1985 and Ajzen, 1987 ) is reviewed, and some unresolved issues are discussed. In broad terms, the theory is found to be well supported by empirical evidence. Intentions to perform behaviors of different kinds can be predicted with high accuracy from attitudes toward the behavior, subjective norms, and perceived behavioral control; and these intentions, together with perceptions of behavioral control, account for considerable variance in actual behavior. Attitudes, subjective norms, and perceived behavioral control are shown to be related to appropriate sets of salient behavioral, normative, and control beliefs about the behavior, but the exact nature of these relations is still uncertain. Expectancy-value formulations are found to be only partly successful in dealing with these relations. Optimal rescaling of expectancy and value measures is offered as a means of dealing with measurement limitations. Finally, inclusion of past behavior in the prediction equation is shown to provide a means of testing the theory's sufficiency, another issue that remains unresolved. The limited available evidence concerning this question shows that the theory is predicting behavior quite well in comparison to the ceiling imposed by behavioral reliability.\",\n",
       " 'Framing Processes and Social Movements: An Overview and Assessment. The recent proliferation of scholarship on collective action frames and framing processes in relation to social movements indicates that framing processes have come to be regarded, alongside resource mobilization and political opportunity processes, as a central dynamic in understanding the character and course of social movements. This review examines the analytic utility of the framing literature for understanding social movement dynamics. We first review how collective action frames have been conceptualized, including their characteristic and variable features. We then examine the literature related to framing dynamics and processes. Next we review the literature regarding various contextual factors that constrain and facilitate framing processes. We conclude with an elaboration of the consequences of framing processes for other movement processes and outcomes. We seek throughout to provide clarification of the linkages between framing concepts/processes and other conceptual and theoretical formulations relevant to social movements, such as schemas and ideology. Copyright © 2000 by Annual Reviews. All rights reserved.',\n",
       " 'Time Is of the Essence: Event History Models in Political Science. Many questions of interest to political scientists may be answered with event history analysis, which studies the duration and timing of events. We discuss the statistical analysis of event history data--data giving the number, timing, and sequence of changes in a variable of interest. These methods are illustrated by examining three substantive political science problems: overt military interventions, challenger deterrence, and congressional career paths; many other applications are possible. Our article is intended to provide a better understanding of the growing number of applications that currently exist in political science and to encourage greater use of these models by showing why event history models are useful in political science research and explaining how one specifies and interprets these models.',\n",
       " \"Public opinion toward immigration reform: The role of economic motivations. This paper tests hypotheses concerning the effects of economic factors on public opinion toward immigration policy Using the 1992 and 1994 National Election Study surveys, probit models are employed to test diverse conceptualizations of the effects of economic adversity and anxiety on opposition to immigration The results indicate that personal economic circumstances play little role in opinion formation, but beliefs about the state of the national economy, anxiety over taxes, and generalized feelings about Hispanics and Asians, the major immigrant groups, are significant determinants of restrictionist sentiment. This restricted role of economic motives rooted in one's personal circumstances held true across ethnic groups, among residents in communities with different numbers of foreign-born, and in both 1992 and 1994.\",\n",
       " 'Visualizing data. Enormous quantities of data go unused or underused today, simply because people can\\'t visualize the quantities and relationships in it. Using a downloadable programming environment developed by the author, Visualizing Data demonstrates methods for representing data accurately on the Web and elsewhere, complete with user interaction, animation, and more. How do the 3.1 billion A, C, G and T letters of the human genome compare to those of a chimp or a mouse? What do the paths that millions of visitors take through a web site look like? With Visualizing Data, you learn how to answer complex questions like these with thoroughly interactive displays. We\\'re not talking about cookie-cutter charts and graphs. This book teaches you how to design entire interfaces around large, complex data sets with the help of a powerful new design and prototyping tool called \"Processing\". Used by many researchers and companies to convey specific data in a clear and understandable manner, the Processing beta is available free. With this tool and Visualizing Data as a guide, you\\'ll learn basic visualization principles, how to choose the right kind of display for your purposes, and how to provide interactive features that will bring users to your site over and over. This book teaches you:        The seven stages of visualizing data -- acquire, parse, filter, mine, represent, refine, and interact   How all data problems begin with a question and end with a narrative construct that provides a clear answer without extraneous details   Several example projects with the code to make them work   Positive and negative points of each representation discussed. The focus is on customization so that each one best suits what you want to convey about your data set  The book does not provide ready-made \"visualizations\" that can be plugged into any data set. Instead, with chapters divided by types of data rather than types of display, you\\'ll learn how each visualization conveys the unique properties of the data it represents -- why the data was collected, what\\'s interesting about it, and what stories it can tell. Visualizing Data teaches you how to answer questions, not simply display information.',\n",
       " \"A Practical Guide to Splines. This book (seventh printing) is based on the author's experience with calculations involving polynomial splines. It presents those parts of the theory which are especially useful in calculations and stresses the representation of splines as linear combinations of B-splines.  After two chapters summarizing polynomial approximation, a rigorous discussion of elementary spline theory is given involving linear, cubic and parabolic splines. The computational handling of piecewise polynomial functions (of one variable) of arbitrary order is the subject of chapters 7 and 8, while chapters 9, 10, and 11 are devoted to B-splines. The distances from splines with fixed and with variable knots is discussed in chapter 12. The remaining five chapters concern specific approximation methods, interpolation, smoothing and least-squares approximation, the solution of an ordinary differential equation by collocation, curve fitting, and surface fitting.\",\n",
       " 'Analysis of Longitudinal Data. { The new edition of this important text has been completely revised and expanded to become the most up-to-date and thorough professional reference text in this fast-moving and important area of biostatistics. Two new chapters have been added on fully parametric models for discrete repeated<br>measures data and on statistical models for time-dependent predictors where there may be feedback between the predictor and response variables. It also contains the many useful features of the previous edition such as, design issues, exploratory methods of analysis, linear models for continuous<br>data, and models and methods for handling data and missing values. }',\n",
       " \"Bayesian Data Analysis. Bayesian Data Analysis describes how to conceptualize, perform, and critique statistical analyses from a Bayesian perspective. Using examples largely from the authors' own experiences, the book focuses on modern computational tools and obtains inferences using computer simulations. Its unique features include thorough discussions of the methods for checking Bayesian models and the role of the design of data collection in influencing Bayesian statistical analysis.Bayesian Data Analysis offers the practicing statistician singular guidance on all aspects of the subject.\",\n",
       " 'Multilevel Statistical Models. {This new edition of this classic incorporates the most recent thinking on methodology and software, as well as the latest literature on multilevel statistical models. Topics covered by multilevel models have increased in recent years, and the methods are widely applied in the social sciences<br>as well as in areas such as epidemiology, geography, education, surveys, and medicine. This third edition includes chapters on meta analysis, factor analysis and structural equation models, and has expanded sections on MCMC methods.}',\n",
       " 'Econometric Analysis. **** _Econometric Analysisi, 6/e_ serves as a bridge between an introduction to the field of econometrics and the professional literature for\\xa0\\xa0social scientists and other professionals in the field\\xa0of social sciences, focusing on applied econometrics and theoretical background. This book provides a broad survey of the field of econometrics that allows the reader to move from here to practice in one or more specialized areas. At the same time, the reader will gain an appreciation of the common foundation of all the fields presented and use the tools they employ. **** This book gives space to a wide range of topics including basic econometrics, Classical, Bayesian, GMM, and Maximum likelihood, and gives special emphasis to new topics such a time series and panels. ****For social scientists and other professionals in the field who want a thorough introduction to applied econometrics that will prepare them for advanced study and practice in the field.',\n",
       " 'Time Series Analysis. {The last decade has brought dramatic changes in the way that researchers analyze economic and financial time series. This book synthesizes these recent advances and makes them accessible to first-year graduate students. James Hamilton provides the first adequate text-book treatments of important innovations such as vector autoregressions, generalized method of moments, the economic and statistical consequences of unit roots, time-varying variances, and nonlinear time series models. In addition, he presents basic tools for analyzing dynamic systems (including linear representations, autocovariance generating functions, spectral analysis, and the Kalman filter) in a way that integrates economic theory with the practical difficulties of analyzing and interpreting real-world data. <i>Time Series Analysis</i> fills an important need for a textbook that integrates economic theory, econometrics, and new results.<P>The book is intended to provide students and researchers with a self-contained survey of time series analysis. It starts from first principles and should be readily accessible to any beginning graduate student, while it is also intended to serve as a reference book for researchers.}',\n",
       " \"The Elements of Statistical Learning: Data Mining, Inference and Prediction. * The many topics include neural networks, support vector machines, classification trees and boosting - the first comprehensive treatment of this topic in any book * Includes more than 200 pages of four-color graphics During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in {R/S-PLUS} and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including {CART,} {MARS,} projection pursuit and gradient boosting.\",\n",
       " 'Exit, voice, and loyalty : responses to decline in firms, organizations, and states. An innovator in contemporary thought on economic and political development looks here at decline rather than growth. Albert O. Hirschman makes a basic distinction between alternative ways of reacting to deterioration in business firms and, in general, to dissatisfaction with organizations: one-exit-is for the member to quit the organization or for the customer to switch to the competing product, and the other-voice-is for members or customers to agitate and exert influence for change \"from within.\"  The efficiency of the competitive mechanism, with its total reliance on exit, is questioned for certain important situations. As exit often undercuts voice while being unable to counteract decline, loyalty is seen in the function of retarding exit and of permitting voice to play its proper role.  The interplay of the three concepts turns out to illuminate a wide range of economic, social, and political phenomena. As the author states in the preface, \"having found my own unifying way of looking at issues as diverse as competition and the two-party system, divorce and the American character, black power and the failure of \\'unhappy\\' top officials to resign over Vietnam, I decided to let myself go a little.\"',\n",
       " 'Statistics and Causal Inference. Problems involving causal inference have dogged at the heels of statistics since its earliest days. Correlation does not imply causation, and yet causal conclusions drawn from a carefully designed experiment are often valid. What can a statistical model say about causation? This question is addressed by using a particular model for causal inference (Holland and Rubin 1983; Rubin 1974) to critique the discussions of other writers on causation and causal inference. These include selected philosophers, medical researchers, statisticians, econometricians, and proponents of causal modeling.',\n",
       " 'Logistic Regression in Rare Events Data. We study rare events data, binary dependent variables with dozens to thousands of times fewer ones (events, such as wars, vetoes, cases of political activism, or epidemiological infections) than zeros (\"nonevents\"). In many literatures, these variables have proven difficult to explain and predict, a problem that seems to have at least two sources. First, popular statistical procedures, such as logistic regression, can sharply underestimate the probability of rare events. We recommend corrections that outperform existing methods and change the estimates of absolute and relative risks by as much as some estimated effects reported in the literature. Second, commonly used data collection strategies are grossly inefficient for rare events data. The fear of collecting data with too few events has led to data collections with huge numbers of observations but relatively few, and poorly measured, explanatory variables, such as in international conflict data with more than a quarter-million dyads, only a few of which are at war. As it turns out, more efficient sampling designs exist for making valid inferences, such as sampling all available events (e.g., wars) and a tiny fraction of nonevents (peace). This enables scholars to save as much as 99% of their (nonfixed) data collection costs or to collect much more meaningful explanatory variables. We provide methods that link these two results, enabling both types of corrections to work simultaneously, and software that implements the methods developed.',\n",
       " 'Designing Social Inquiry: Scientific Inference in Qualitative Research. Chapter 3 decriptive inference vs.causal inference no precise usage of the concept: too wary of \"only correlation\" or immediately taking every correlation as a causation we should draw causal inference where they seem appropriate - and provide the reader with most honest estimate of uncertainty Defining causality - building countercactuals: Carefully design it: Control variables stay, everything corellating with change in DV cannot be controlled... - causual effect: incident effect - counterfactual effect -> but this can never be measured in real life!! - in a concrete situation: random causal effect, due to random events - mean causal effect -> \"the causal effect is the difference between the systematic component of observations made when the explanatory variable takes one value and the systematic component of comparable observations when the explanatory variable takes on another value\" Alternative concepts: All are encompassed - Causal mechanisms: Process focused. Chain of suggested causal effects. But: causal mechanism is not causality - Multiple Causality: many exp vars, view obs. -> problem of equifinality - Lieberson: Symmetric & Asymmetric Causality: When the IV appears: Effect, when it then disappears: Still effect? Assumptions required for Estimating Effects: Ways out from Fundamental Problem of Causal Inference - unit homogeneity: Rerun treatment in similar units. Similar in DV or IV! - Conditional interdependence: Effect of DV (!) on IV Rules for Constructing Causal Theories 1. Construct falsifiable Theories: Good, but in SocSci: not deterministic, but rather prob. ideas of science 2. Build internally consistent theories: Right balance betwee simplification and reality 3. Select DVs carefully: - Dependent Vars - Select to have change in DVs - Choose the DV we wisht to explain 4. Maximize concreteness -> allows for falsification 5. State theories in as encompassing ways as feasible -> put as much from the world in it as possible!',\n",
       " 'Generalized Linear Models. {The success of the first edition of Generalized Linear Models led to the updated Second Edition, which continues to provide a definitive unified, treatment of methods for the analysis of diverse types of data. Today, it remains popular for its clarity, richness of content and direct relevance to agricultural, biological, health, engineering, and other applications.The authors focus on examining the way a response variable depends on a combination of explanatory variables, treatment, and classification variables. They give particular emphasis to the important case where the dependence occurs through some unknown, linear combination of the explanatory variables.The Second Edition includes topics added to the core of the first edition, including conditional and marginal likelihood methods, estimating equations, and models for dispersion effects and components of dispersion. The discussion of other topics-log-linear and related models, log odds-ratio regression models, multinomial response models, inverse linear and related models, quasi-likelihood functions, and model checking-was expanded and incorporates significant revisions.Comprehension of the material requires simply a knowledge of matrix theory and the basic ideas of probability theory, but for the most part, the book is self-contained. Therefore, with its worked examples, plentiful exercises, and topics of direct use to researchers in many disciplines, Generalized Linear Models serves as ideal text, self-study guide, and reference.}',\n",
       " 'The Logic of Collective Action: Public Goods and the Theory of Groups. <p> This book develops an original theory of group and organizational behavior that cuts across disciplinary lines and illustrates the theory with empirical and historical studies of particular organizations. Applying economic analysis to the subjects of the political scientist, sociologist, and economist, Mr. Olson examines the extent to which the individuals that share a common interest find it in their individual interest to bear the costs of the organizational effort. </p><p> The theory shows that most organizations produce what the economist calls \"public goods\"--goods or services that are available to every member, whether or not he has borne any of the costs of providing them. Economists have long understood that defense, law and order were public goods that could not be marketed to individuals, and that taxation was necessary. They have not, however, taken account of the fact that private as well as governmental organizations produce public goods. </p><p> The services the labor union provides for the worker it represents, or the benefits a lobby obtains for the group it represents, are public goods: they automatically go to every individual in the group, whether or not he helped bear the costs. It follows that, just as governments require compulsory taxation, many large private organizations require special (and sometimes coercive) devices to obtain the resources they need. </p><p> This is not true of smaller organizations for, as this book shows, small and large organizations support themselves in entirely different ways. The theory indicates that, though small groups can act to further their interest much more easily than large ones, they will tend to devote too few resources to the satisfaction of their common interests, and that there is a surprising tendency for the \"lesser\" members of the small group to exploit the \"greater\" members by making them bear a disproportionate share of the burden of any group action. </p><p> All of the theory in the book is in Chapter 1; the remaining chapters contain empirical and historical evidence of the theory\\'s relevance to labor unions, pressure groups, corporations, and Marxian class action. </p>',\n",
       " 'Qualitative Evaluation and Research Methods. \"What a pleasant surprise to read a book on qualitative research methods that not only does the subject justice but is also readable. . . . This book is for specialists and is suited for all nurses pursuing a course in research methods or who are considering undertaking qualitative research. It represents a good value for money.\" --Nursing Times \"The updated, 1990 version contains valuable material on rapid reconnaissance techniques, focus group interviews, and qualitative synthesis. It is well- written, easy to understand, and full of useful guidelines for conducting interviews and group work observations. Qualitative research\\'s theories are applicable to training professionals, managers, and anyone responsible for making decisions. Special features include an updated bibliography, samples of interview guides, examples of open-ended interviews, and suggested code books for computerized interviews.\" --American Society for Training & Development \"The new edition has been completely revised and updated, includes new material on rapid reconnaissance techniques, future applications, focus group interviews and qualitative synthesis, and new chapters on the theoretical foundations of qualitative inquiry; enhancing the quality and credibility of qualitative analysis; and particularly appropriate qualitative applications. . . . Michael Quinn Patton\\'s style is direct, engaging, and often humorous. . . . A very useful discussion of the different themes of qualitative inquiry and of the different theoretical traditions and orientations which have been associated with the approach. . . . Provides a much-enhanced treatment of qualitative evaluation and research methods compared to that of the first edition. It should be indispensable reading for the serious qualitative evaluator or researcher, will certainly instruct and possibly entertain those with a general interest in social research or evaluation techniques.\" --International Journal of Information Management \"Michael Quinn Patton\\'s discussion of these two major types of empirical inquiry is admirably objective and readable. For those trained in quantitative inquiry but not in qualitative inquiry, this first section may be the most important part of the book. . . .Besides its readable, common-sense discussion of qualitative data collection, **Qualitative Evaluation and Research Methods** has other features which also recommend it. An experienced researcher and evaluator himself, Patton tells detailed, sometimes humorous narratives of investigations he has conducted. This vicarious experience makes the theoretical discussion more understandable and clearly useful for the reader. Even more important, Patton provides a long bibliography--seventeen pages in eight-point type. This extensive bibliography is a good starting place for novices who want to know more about the issues involved in qualitative data collection. . . . **Qualitative Evaluation and Research Methods** is a useful introduction for graduate students, faculty, or program directors who are interested in learning about qualitative data collection. It offers a way in for graduate students and faculty who want to begin qualitative research projects, and it describes flexible, context-sensitive techniques for program evaluation. The survival of the new professional programs in technical communication depends on our ability to establish a scholarly base rooted in composition studies and to assess the strengths and weaknesses of our course offerings. Qualitative Evaluation and Research Methods offers help in both areas.\" --Journal of Technical Writing and Communication \"The chapters on \\'Qualitative interviewing\\' and \\'Qualitative analysis and interpretation\\' (chapters 7 and 8) were particularly clear and concise. Important points were highlighted.\" --British Journal of Psychology \"Offers a fresh nonconventional approach to qualitative research, emphasizing the use of a research report as a basis for action.\" --NYHEDSBREV \"**Qualitative Evaluation and Research Methods** is written in a readable style for students, researchers, and practitioners, and the text is filled with examples used to clarify concepts. . . . It is comprehensive in covering the essential concepts of qualitative methodology, including problem identification, participant observation, interviewing, document analysis, triangulation, data analysis, and reporting. Additionally, the text reflects a thorough examination of the literature on qualitative inquiry. . . . A useful guide.\" --Educational Studies Once again setting the standard for the field, the second edition of **Qualitative Evaluation and Research Methods **reflects the tremendous explosion of interest in qualitative methods over the past decade. Completely revised and updated, the second edition again emphasizes strategies for generating vivid, useful, and credible qualitative information for decision making. New to this edition are chapters on: - the theoretical foundations of qualitative inquiry; - enhancing the quality and credibility of qualitative analysis; and - particularly appropriate qualitative applications. New material has also been added on rapid reconnaissance techniques, future applications, focus group interviews, and qualitative synthesis. In addition, Michael Quinn Patton has thoroughly revised the literature reviews and citations. A most useful and practical resource, **Qualitative Evaluation and Research Methods** will interest all evaluators and methodologists. Scholars and students in the fields of evaluation studies, education, psychology, sociology, public administration, and management will benefit from the provocative insights offered in this pathbreaking volume.',\n",
       " 'Increasing Returns, Path Dependence, and the Study of Politics. It is increasingly common for social scientists to describe political processes as \"path dependent.\" The concept, however, is often employed without careful elaboration. This article conceptualizes path dependence as a social process grounded in a dynamic of \"increasing returns.\" Reviewing recent literature in economics and suggesting extensions to the world of politics, the article demonstrates that increasing returns processes are likely to be prevalent, and that good analytical foundations exist for exploring their, causes and consequences. The investigation of increasing returns can provide a more rigorous framework for developing some of the key claims of recent scholarship in historical institutionalism: Specific patterns of timing and sequence matter; a wide range of social outcomes may be possible; large consequences may result from relatively small or contingent events; particular courses of action, once introduced, can be almost impossible to reverse; and consequently, political development is punctuated by critical moments or junctures that shape the basic contours of social life.',\n",
       " 'Mixed-Effects Models in S and S-PLUS. This book provides an overview of the theory and application of linear and nonlinear mixed-effects models in the analysis of grouped data, such as longitudinal data, repeated measures, and multilevel data. A unified model-building strategy for both linear and nonlinear models is presented and applied to the analysis of over 20 real datasets from a wide variety of areas, including pharmacokinetics, agriculture, and manufacturing. A strong emphasis is placed on the use of graphical displays at the various phases of the model-building process, starting with exploratory plots of the data and concluding with diagnostic plots to assess the adequacy of a fitted model. Over 170 figures areincluded in the book.  The NLME package for analyzing mixed-effects models in R and S-PLUS, developed by the authors, provides the underlying software for implementing the methods presented in the text, being described and illustrated in detail throughout the book.  The balanced mix of real data examples, modeling software, and theory makes this book a useful reference for practitioners using mixed-effects models in their data analyses. It can also be used as a text for a one-semester graduate-level applied course in mixed-effects models. Researchers in statistical computing will also find this book appealing for its presentation of novel and efficient computational methods for fitting linear and nonlinear mixed-effects models.  José C. Pinheiro is a Senior Biometrical Fellow at Novartis Pharmaceuticals, having worked at Bell Labs during the time this book was produced. He has published extensively in mixed-effects models, dose finding methods in clinical development, and other areas of biostatistics.  Douglas M. Bates is Professor of Statistics at the University of Wisconsin-Madison. He is the author, with Donald G. Watts, of Nonlinear Regression Analysis and Its Applications, a Fellow of the American Statistical Association, and a former chair of the Statistical Computing Section.',\n",
       " 'The Psychology of Survey Response. {Drawing on classic and modern research from cognitive psychology, social  psychology, and survey methodology, this book examines the psychological  roots of survey data, how survey responses are formulated, and how  seemingly unimportant features of the survey can affect the answers  obtained. Topics include the comprehension of survey questions, the recall of relevant facts and beliefs, estimation and inferential processes people use to answer survey questions, the sources of the apparent instability of public opinion, the difficulties in getting responses into the required  format, and distortions introduced into surveys by deliberate  misreporting.  }',\n",
       " 'Bowling Alone: The collapse and revival of American community. {Few people outside certain scholarly circles had heard the name Robert D. Putnam before 1995. But then this self-described \"obscure academic\" hit a nerve with a journal article called \"Bowling Alone.\" Suddenly he found himself invited to Camp David, his picture in <I>People</I> magazine, and his thesis at the center of a raging debate. In a nutshell, he argued that civil society was breaking down as Americans became more disconnected from their families, neighbors, communities, and the republic itself. The organizations that gave life to democracy were fraying. Bowling became his driving metaphor. Years ago, he wrote, thousands of people belonged to bowling leagues. Today, however, they\\'re more likely to bowl alone:  <blockquote>Television, two-career families, suburban sprawl, generational changes in values--these and other changes in American society have meant that fewer and fewer of us find that the League of Women Voters, or the United Way, or the Shriners, or the monthly bridge club, or even a Sunday picnic with friends fits the way we have come to live. Our growing social-capital deficit threatens educational performance, safe neighborhoods, equitable tax collection, democratic responsiveness, everyday honesty, and even our health and happiness.</blockquote>  The conclusions reached in the book <I>Bowling Alone</I> rest on a mountain of data gathered by Putnam and a team of researchers since his original essay appeared. Its breadth of information is astounding--yes, he really has statistics showing people are less likely to take Sunday picnics nowadays. Dozens of charts and graphs track everything from trends in PTA participation to the number of times Americans say they give \"the finger\" to other drivers each year. If nothing else, <I>Bowling Alone</I> is a fascinating collection of factoids. Yet it does seem to provide an explanation for why \"we tell pollsters that we wish we lived in a more civil, more trustworthy, more collectively caring community.\" What\\'s more, writes Putnam, \"Americans are right that the bonds of our communities have withered, and we are right to fear that this transformation has very real costs.\" Putnam takes a stab at suggesting how things might change, but the book\\'s real strength is in its diagnosis rather than its proposed solutions. <I>Bowling Alone</I> won\\'t make Putnam any less controversial, but it may come to be known as a path-breaking work of scholarship, one whose influence has a long reach into the 21st century. <I>--John J. Miller</I> } {<P> Once we bowled in leagues, usually after work; but no longer. This seemingly small phenomenon symbolizes a significant social change that Robert Putnam has identified and describes in this brilliant volume, <I>Bowling Alone.</I>  <P> Drawing on vast new data from the Roper Social and Political Trends and the DDB Needham Life Style -- surveys that report in detail on Americans\\' changing behavior over the past twenty-five years -- Putnam shows how we have become increasingly disconnected from family, friends, neighbors, and social structures, whether the PTA, church, recreation clubs, political parties, or bowling leagues. Our shrinking access to the \"social capital\" that is the reward of communal activity and community sharing is a serious threat to our civic and personal health.  <P> Putnam\\'s groundbreaking work shows how social bonds are the most powerful predictor of life satisfaction. For example, he reports that getting married is the equivalent of quadrupling your income and attending a club meeting regularly is the equivalent of doubling your income. The loss of social capital is felt in critical ways: Communities with less social capital have lower educational performance and more teen pregnancy, child suicide, low birth weight, and prenatal mortality. Social capital is also a strong predictor of crime rates and other measures of neighborhood quality of life, as it is of our health: In quantitative terms, if you both smoke and belong to no groups, it\\'s a close call as to which is the riskier behavior.  <P> A hundred years ago, at the turn of the last century, America\\'s stock of social capital was at an ebb, reduced by urbanization, industrialization, and vast immigration that uprooted Americans from their friends, social institutions, and families, a situation similar to today\\'s. Faced with this challenge, the country righted itself. Within a few decades, a range of organizations was created, from the Red Cross, Boy Scouts, and YWCA to Hadassah and the Knights of Columbus and the Urban League. With these and many more cooperative societies we rebuilt our social capital.  <P> We can learn from the experience of those decades, Putnam writes, as we work to rebuild our eroded social capital. It won\\'t happen without the concerted creativity and energy of Americans nationwide. <P> Like defining works from the past that have endured -- such as <I>The Lonely Crowd</I> and <I>The Affluent Society</I> -- and like C. Wright Mills, Richard Hofstadter, Betty Friedan, David Riesman, Jane Jacobs, Rachel Carson, and Theodore Roszak, Putnam has identified a central crisis at the heart of our society and suggests what we can do.}',\n",
       " \"Mathematical Statistics and Data Analysis. This is the first text in a generation to re-examine the purpose of the mathematical statistics course. The book's approach interweaves traditional topics with data analysis and reflects the use of the computer with close ties to the practice of statistics. The author stresses analysis of data, examines real problems with real data, and motivates the theory. The book's descriptive statistics, graphical displays, and realistic applications stand in strong contrast to traditional texts that are set in abstract settings.\",\n",
       " \"R: A language and environment for statistical computing. R is a language and environment for statistical computing and graphics. It is a GNU project which is similar to the S language and environment which was developed at Bell Laboratories (formerly AT&T, now Lucent Technologies) by John Chambers and colleagues. R can be considered as a different implementation of S. There are some important differences, but much code written for S runs unaltered under R.     R provides a wide variety of statistical (linear and nonlinear modelling, classical statistical tests, time-series analysis, classification, clustering, ...) and graphical techniques, and is highly extensible. The S language is often the vehicle of choice for research in statistical methodology, and R provides an Open Source route to participation in that activity.     One of R's strengths is the ease with which well-designed publication-quality plots can be produced, including mathematical symbols and formulae where needed. Great care has been taken over the defaults for the minor design choices in graphics, but the user retains full control.     R is available as Free Software under the terms of the Free Software Foundation's GNU General Public License in source code form. It compiles and runs on a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux), Windows and MacOS.\",\n",
       " 'Observational Studies. Observational studies are important methodologies to evaluate exposures and risk factors that are not amenable to experimental trials. They offer the advantage of being more generalizable to our patients, as these studies may have more liberal inclusion criteria than the typical randomized trial. Their disadvantage is their susceptibility to biases and their inability to control for unknown factors that may impact on the outcome of interest. Establishing causality of an association noted in observational studies is an intricate process that requires careful assessment. Clinicians and researchers should be familiar with observational studies so they may better evaluate a proposed causal relationship and the quality of reports claiming such relationships. Only then can they determine if the findings are valid and applicable to their patient population.',\n",
       " \"Introduction to Probability Models. {Ross's classic bestseller, <B>Introduction to Probability Models,</B> has been used extensively by professionals and as the primary text for a first undergraduate course in applied probability. It provides an introduction to elementary probability theory and stochastic processes, and shows how probability theory can be applied to the study of phenomena in fields such as engineering, computer science, management science, the physical and social sciences, and operations research. With the addition of several new sections relating to actuaries, this text is highly recommended by the Society of Actuaries.<br><br>A new section (3.7) on COMPOUND RANDOM VARIABLES, that can be used to establish a recursive formula for computing  probability mass functions for a variety of common  compounding  distributions.<br><br>A new section  (4.11) on HIDDDEN MARKOV CHAINS, including the forward and backward approaches for computing the joint probability mass function of the signals, as well as the Viterbi algorithm for determining the most likely sequence of states.<br><br>Simplified Approach for Analyzing Nonhomogeneous Poisson processes<br><br>Additional results on  queues relating to the<br> (a)  conditional distribution of the number found by an M/M/1 arrival who spends a time t in the system,;<br>(b) inspection paradox for M/M/1 queues<br>(c) M/G/1 queue with server breakdown<br><br><br>Many new examples and exercises.}\",\n",
       " 'Time Series Analysis and Its Applications. Time Series Analysis and Its Applications presents a balanced and comprehensive treatment of both time and frequency domain methods with accompanying theory. Numerous examples using non trivial data illustrate solutions to problems such as evaluating pain perception experiments using magnetic resonance imaging or monitoring a nuclear test ban treaty. The book is designed to be useful as a text for graduate-level students in the physical, biological, and social sciences and as a graduate-level text in statistics. Some parts may also serve as an undergraduate introductory course.',\n",
       " \"Democracy in America. {Abridged,with an Introduction by Patrick RenshawDemocracy in America is a  classic of political philosophy. Hailed by John Stuart Mill and Horace  Greely as the finest book ever written on the nature of democracy, it  continuues to be an influential text on both sides of the Atlantic, above  all in the emerging democracies of Eastern Europe.De Tocqueville examines  the structures, institutions and operation of democracyt, and shows how  Europe can learn from American successs and failures. His central theme is the advancement of the rule of the people, but he also predicts that  slavery will bring about the 'most horrible of civil wars', foresees that  the USA and Russia will be the Superpowers of the twentieth century, and  is 150 years ahead of his time in his views on the position and importance of women.  }\",\n",
       " 'Contact order, transition state placement and the refolding rates of single domain proteins.. Theoretical studies have suggested relationships between the size, stability and topology of a protein fold and the rate and mechanisms by which it is achieved. The recent characterization of the refolding of a number of simple, single domain proteins has provided a means of testing these assertions. Our investigations have revealed statistically significant correlations between the average sequence separation between contacting residues in the native state and the rate and transition state placement of folding for a non-homologous set of simple, single domain proteins. These indicate that proteins featuring primarily sequence-local contacts tend to fold more rapidly and exhibit less compact folding transition states than those characterized by more non-local interactions. No significant relationship is apparent between protein length and folding rates, but a weak correlation is observed between length and the fraction of solvent-exposed surface area buried in the transition state. Anticipated strong relationships between equilibrium folding free energy and folding kinetics, or between chemical denaturant and temperature dependence-derived measures of transition state placement, are not apparent. The observed correlations are consistent with a model of protein folding in which the size and stability of the polypeptide segments organized in the transition state are largely independent of protein length, but are related to the topological complexity of the native state. The correlation between topological complexity and folding rates may reflect chain entropy contributions to the folding barrier.',\n",
       " 'Bacterial Persistence as a Phenotypic Switch. A fraction of a genetically homogeneous microbial population may survive exposure to stress such as antibiotic treatment. Unlike resistant mutants, cells regrown from such persistent bacteria remain sensitive to the antibiotic. We investigated the persistence of single cells of Escherichia coli with the use of microfluidic devices. Persistence was linked to preexisting heterogeneity in bacterial populations because phenotypic switching occurred between normally growing cells and persister cells having reduced growth rates. Quantitative measurements led to a simple mathematical description of the persistence switch. Inherent heterogeneity of bacterial populations may be important in adaptation to fluctuating environments and in the persistence of bacterial infections. 10.1126/science.1099390',\n",
       " 'Human Capital: A Theoretical and Empirical Analysis with Special Reference to Education. {<div><i>Human Capital</i> is Becker\\'s classic study of how investment in an individual\\'s education and training is similar to business investments in equipment. Recipient of the 1992 Nobel Prize in Economic Science, Gary S. Becker is a pioneer of applying economic analysis to human behavior in such areas as discrimination, marriage, family relations, and education. Becker\\'s research on human capital was considered by the Nobel committee to be his most noteworthy contribution to economics.<br><br>This expanded edition includes four new chapters, covering recent ideas about human capital, fertility and economic growth, the division of labor, economic considerations within the family, and inequality in earnings.<br><br>\"Critics have charged that Mr. Becker\\'s style of thinking reduces humans to economic entities. Nothing could be further from the truth. Mr. Becker gives people credit for having the power to reason and seek out their own best destiny.\"--<i>Wall Street Journal</i><br><br></div>}',\n",
       " 'Statistical Inference. {This book builds theoretical statistics from the first principles of probability theory. Starting from the basics of probability, the authors develop the theory of statistical inference using techniques, definitions, and concepts that are statistical and are natural extensions and consequences of previous concepts. Intended for first-year graduate students, this book can be used for students majoring in statistics who have a solid mathematics background. It can also be used in a way that stresses the more practical uses of statistical theory, being more concerned with understanding basic statistical concepts and deriving reasonable statistical procedures for a variety of situations, and less concerned with formal optimality investigations.}',\n",
       " 'Practical Nonparametric Statistics. {This highly-regarded text serves as a quick reference book which offers clear, concise instructions on how and when to use the most popular nonparametric procedures. This edition features some procedures that have withstood the test of time and are now used by many practitioners, such as the Fisher Exact Test for two-by-two contingency tables, the Mantel-Haenszel Test for combining several contingency tables, the Kaplan-Meier estimates of the survival curve, the Jonckheere-Terpstra Test and the Page Test for ordered alternatives, and a discussion of the bootstrap method.}',\n",
       " \"Lectures on Macroeconomics. {<I>Lectures on Macroeconomics </I>provides the first comprehensive description and evaluation of macroeconomic theory in many years. While the authors' perspective is broad, they clearly state their assessment of what is important and what is not as they present the essence of macroeconomic theory today.<br /> <br /> The main purpose of <I>Lectures on Macroeconomics</I> is to characterize and explain fluctuations in output, unemployment and movement in prices. The most important fact of modern economic history is persistent long term growth, but as the book makes clear, this growth is far from steady. The authors analyze and explore these fluctuations.<br /> <br /> Topics include consumption and investment; the Overlapping Generations Model; money; multiple equilibria, bubbles, and stability; the role of nominal rigidities; competitive equilibrium business cycles, nominal rigidities and economic fluctuations, goods, labor and credit markets; and monetary and fiscal policy issues. Each of chapters 2 through 9 discusses models appropriate to the topic. Chapter 10 then draws on the previous chapters, asks which models are the workhorses of macroeconomics, and sets the models out in convenient form. A concluding chapter analyzes the goals of economic policy, monetary policy, fiscal policy, and dynamic inconsistency.<br /> <br /> Written as a text for graduate students with some background in macroeconomics, statistics, and econometrics, <I>Lectures on Macroeconomics </I>also presents topics in a self contained way that makes it a suitable reference for professional economists.<br /> <br /> Olivier Jean Blanchard and Stanley Fischer are both Professors of Economics at MIT.}\",\n",
       " 'Analysis of Panel Data. {Panel data models have become increasingly popular among applied researchers due to their heightened capacity for capturing the complexity of human behavior, as compared to cross-sectional or time series data models.  This second edition represents a substantial revision of the highly successful first edition (1986). Recent advances in panel data research are presented in an accessible manner and are carefully integrated with the older material.  The thorough discussion of theory and the judicious use of empirical examples make this book useful to graduate students and advanced researchers in economics, business, sociology and political science.} {This book reviews the basic econometric methods that have been used to analyze panel data--data collected by observing a number of individuals over time. In analyzing this data, difficulties can arise due to unobserved individuals or time characteristics.  The focus of the book is twofold: correcting for these difficulties, and providing methods for improvement of the efficiency of the estimates.  The author draws together work on panel data from a number of different perspectives, including the econometric literature on specification analysis, the time series literature on dynamic models, and discrete choice literature.}',\n",
       " 'Choices, Values and Frames. Discusses the cognitive and the psychophysical determinants of choice in risky and riskless contexts. The psychophysics of value induce risk aversion in the domain of gains and risk seeking in the domain of losses. The psychophysics of chance induce overweighting of sure things and of improbable events, relative to events of moderate probability. Decision problems can be described or framed in multiple ways that give rise to different preferences, contrary to the invariance criterion of rational choice. The process of mental accounting, in which people organize the outcomes of transactions, explains some anomalies of consumer behavior. In particular, the acceptability of an option can depend on whether a negative outcome is evaluated as a cost or as an uncompensated loss. The relationships between decision values and experience values and between hedonic experience and objective states are discussed. (27 ref)',\n",
       " 'The Theory of Incentives: The Principal-agent Model. {Economics has much to do with incentives--not least, incentives to work hard, to produce quality products, to study, to invest, and to save. Although Adam Smith amply confirmed this more than two hundred years ago in his analysis of sharecropping contracts, only in recent decades has a theory begun to emerge to place the topic at the heart of economic thinking. In this book, Jean-Jacques Laffont and David Martimort present the most thorough yet accessible introduction to incentives theory to date. Central to this theory is a simple question as pivotal to modern-day management as it is to economics research: What makes people act in a particular way in an economic or business situation? In seeking an answer, the authors provide the methodological tools to design institutions that can ensure good incentives for economic agents.<p>This book focuses on the principal-agent model, the \"simple\" situation where a principal, or company, delegates a task to a single agent through a contract--the essence of management and contract theory. How does the owner or manager of a firm align the objectives of its various members to maximize profits? Following a brief historical overview showing how the problem of incentives has come to the fore in the past two centuries, the authors devote the bulk of their work to exploring principal-agent models and various extensions thereof in light of three types of information problems: adverse selection, moral hazard, and non-verifiability. Offering an unprecedented look at a subject vital to industrial organization, labor economics, and behavioral economics, this book is set to become the definitive resource for students, researchers, and others who might find themselves pondering what contracts, and the incentives they embody, are really all about.}',\n",
       " 'Microeconomic Theory. Many instructors of microeconomic theory have been waiting for a text that provides balanced and in-depth analysis of the essentials of microeconomics. Masterfully combining the results of years of teaching microeconomics at Harvard University, Andreu Mas-Colell, Michael Whinston, and Jerry Green have filled that conspicuous vacancy with their groundbreaking text, Microeconomic Theory. <P>The authors set out to create a solid organizational foundation upon which to build the effective teaching tool for microeconomic theory. The result presents unprecedented depth of coverage in all the essential topics, while allowing professors to \"tailor-make\" their course to suit personal priorities and style. Topics such as noncooperative game theory, information economics, mechanism design, and general equilibrium under uncertainty receive the attention that reflects their stature within the discipline. The authors devote an entire section to game theory alone, making it \"free-standing\" to allow instructors to return to it throughout the course when convenient. Discussion is clear, accessible, and engaging, enabling the student to gradually acquire confidence as well as proficiency. Extensive exercises within each chapter help students to hone their skills, while the text\\'s appendix of terms, fully cross-referenced throughout the previous five sections, offers an accessible guide to the subject matter\\'s terminology. Teachers of microeconomics need no longer rely upon scattered lecture notes to supplement their textbooks. Deftly written by three of the field\\'s most influential scholars, Microeconomic Theory brings the readability, comprehensiveness, and versatility to the first-year graduate classroom that has long been missing.',\n",
       " 'Public Choice III. {This book represents a considerable revision and expansion of Public Choice II (1989). As in the previous editions, all of the major topics of public choice are covered.  These include: why the state exists, voting rules, federalism, the theory of clubs, two-party and multiparty electoral systems, rent seeking, bureaucracy, interest groups, dictatorship, the size of government, voter participation, and political business cycles.  Normative issues in public choice are also examined.  The book is suitable for upper level courses in economics dealing with politics, and political science courses emphasizing rational actor models.} {This book represents a considerable revision and expansion of Public Choice II (1989). As in the previous editions, all of the major topics of public choice are covered.  These include: why the state exists, voting rules, federalism, the theory of clubs, two-party and multiparty electoral systems, rent seeking, bureaucracy, interest groups, dictatorship, the size of government, voter participation, and political business cycles.  Normative issues in public choice are also examined.  The book is suitable for upper level courses in economics dealing with politics, and political science courses emphasizing rational actor models.}',\n",
       " 'A Course in Game Theory. \"I recommend this book highly, it is beautifully done...\" -- Robert Aumann, Hebrew University <P>A Course in Game Theory presents the main ideas of game theory at a level suitable for graduate students and advanced undergraduates, emphasizing the theory\\'s foundations and interpretations of its basic concepts. The authors provide precise definitions and full proofs of results, sacrificing generalities and limiting the scope of the material in order to do so. The text is organized in four parts: strategic games, extensive games with perfect information, extensive games with imperfect information, and coalitional games. It includes over 100 exercises. More about this book',\n",
       " 'Nonparametric Econometrics. {This book systematically and thoroughly covers a vast literature on the nonparametric and semiparametric statistics and econometrics that has evolved over the past five decades. Within this framework, this is the first book to discuss the principles of the nonparametric approach to the topics covered in a first year graduate course in econometrics, e.g., regression function, heteroskedasticity, simultaneous equations models, logit-probit and censored models. Professors Pagan and Ullah provide intuitive explanations of difficult concepts, heuristic developments of theory, and empirical examples emphasizing the usefulness of modern nonparametric approach. The book should provide a new perspective on teaching and research in applied subjects in general and econometrics and statistics in particular.}',\n",
       " \"Information Rules: A Strategic Guide to The Network Economy. Chapter 1 of <I>Information Rules</I> begins with a description of the change brought on by technology at the close of the century--but the century described is not this one, it's the late 1800s. One hundred years ago, it was an emerging telephone and electrical network that was transforming business. Today it's the Internet. The point? While the circumstances of a particular era may be unique, the underlying principles that describe the exchange of goods in a free-market economy are the same. And the authors, Carl Shapiro and Hal Varian, should know. Shapiro is Professor of Business Strategy at the Haas School of Business at UC Berkeley and has also served as chief economist at the Antitrust Division of the Justice Department. Varian is the Dean of the School of Information Management and Systems at UC Berkeley. Together they offer a deep knowledge of how economic systems work coupled with first-hand experience of today's network economy. They write: <blockquote> Sure, today's business world is different in a myriad of ways from that of a century ago. But many of today's managers are so focused on the trees of technological change that they fail to see the forest: the underlying economic forces that determine success and failure. </blockquote> Shapiro and Varian go to great lengths to purge this book of the technobabble and forecasting of an electronic woo-woo land that's typical in books of this genre. Instead, with their feet on the ground, they consider how to market and distribute goods in the network economy, citing examples from industries as diverse as airlines, software, entertainment, and communications. The authors cover issues such as pricing, intellectual property, versioning, lock-in, compatibility, and standards. Clearly written and presented, <I>Information Rules</I> belongs on the bookshelf of anyone who has an interest in today's network economy--entrepreneurs, managers, investors, students. If there was ever a textbook written on how to do business in the information age, this book is it. Highly recommended. <I>--Harry C. Edwards</I>  In a marketplace that depends so thoroughly on cutting-edge information technology, can classic economic principles still offer any real strategic value? Yes! say Carl Shapiro and Hal Varian. In Information Rules, they reveal that many conventional economic concepts can provide the insight and understanding necessary to succeed in the information age. Shapiro and Varian argue that if managers seriously want to develop effective strategies for competing in the new economy, they must understand the fundamental economics of information technology. Whether information takes the form of software code or recorded music, is published in a book or magazine, or even posted on a website, managers must know how to evaluate the consequences of pricing, protecting, and planning new versions of information products, services, and systems. The first book to distill the economics of information and networks into practical business strategies, Information Rules is a guide to the winning moves that can help business leaders-from writers, lawyers, and finance professionals to executives in the entertainment, publishing, and hardware and software industries--navigate successfully through the information economy.  In Information Rules, authors Shapiro and Varian reveal that many classic economic concepts can provide the insight and understanding necessary to succeed in the information age. They argue that if managers seriously want to develop effective strategies for competing in the new economy, they must understand the fundamental economics of information technology. Whether information takes the form of software code or recorded music, is published in a book or magazine, or even posted on a website, managers must know how to evaluate the consequences of pricing, protecting, and planning new versions of information products, services, and systems. The first book to distill the economics of information and networks into practical business strategies, Information Rules is a guide to the winning moves that can help business leaders navigate successfully through the tough decisions of the information economy.\",\n",
       " 'The Elements of Style. You know the authors\\' names. You recognize the title. You\\'ve probably used this book yourself. And now The Elements of Style--the most widely read and employed English style manual--is available in a specially bound 50th Anniversary Edition that offers the title\\'s vast audience an opportunity to own a more durable and elegantly bound edition of this time-tested classic. Offering the same content as the Fourth Edition, revised in 1999, the new casebound 50th Anniversary Edition includes a brief overview of the book\\'s illustrious history. Used extensively by individual writers as well as high school and college students of writing, it has conveyed the principles of English style to millions of readers. This new deluxe edition makes the perfect gift for writers of any age and ability level. Fifty Years of Acclaim for The Elements of Style, by William Strunk Jr. and E.B. White \"I first read Elements of Style during the summer before I went off to Exeter, and I still direct my students at Harvard to their definition about the difference between \\'that\\' and \\'which.\\' It is the Bible for good, clear writing.\" -- Henry Louis Gates Jr.\"For writers of all kinds and sizes the world begins and ends with Strunk and White\\'s Elements of Style. Only something to actually write about trumps the list of what is required to put words together in some kind of coherent way. I treasure its presence in my life and salute its fifty years of glory and accomplishment.\" -- Jim Lehrer \"The Elements of Style remains an unwavering beacon of light in these grammatically troubled times. I would be lost without it.\" -- Ann Patchett \"To the extent I know how to write clearly at all, I probably taught myself while I was teaching others -- seventh graders, in Flint, Michigan, in 1967. I taught them with a copy of Strunk & White lying in full view on my desk, sort of in the way the Gideons leave Bibles in cheap hotel rooms, as a way of saying to the hapless inhabitant: \\'In case your reckless ways should strand you here, there\\'s help.\\' S&W doesn\\'t really teach you how to write, it just tantalizingly reminds you that there\\'s an orderly way to go about it, that clarity\\'s ever your ideal, but -- really -- it\\'s all going to be up to you.\" -- Richard Ford \"The Elements of Style never seems to go out of date.Its counsel is sound and funny, wise and unpretentious. And while its precepts are a foundation of direct communication, Strunk and White do not insist on a way of writing beyond clear expression. The rest is up to the imagination, the intelligence within.\" -- David Remnick, editor of The New Yorker \"It\\'s the toughness--the irreverence and implicit laughter--that attracted me to the little book when I was seventeen. I fell in love with Strunk & White\\'s loathing for cant and bloviation, the ruthless cutting of crap, jargon, and extra words. For me, that skeptical directness included a tacit permission by The Elements of Style to break its rules on occasion: an alloy of generosity in the blade, a grace I still admire and still learn from.\" -- Robert Pinsky \"In the quest for clarity, one can have no better guides than Strunk and White. For me, their book has been invaluable and remains essential.\" -- Dan Rather \"Eschew surplusage! A perfect book.\"--Jonathan Lethem \"Not until I started teaching writing and I reread The Elements of Style did I realize that most everything I would be teaching young writers, and everything I would be learning myself as a writer, was contained between the covers of this slim, elegant, wise little book. \" -- Julia Alvarez \"Strunk and White seared their way into my brain long ago, and I benefit from them daily.\" -- Steven J. Dubner, co-author of Freakonomics \"Since high school, I have kept a copy of this book handy. That should be unnecessary. I should, by now, have fully internalized The Elements of Style. But sometimes I get entangled in a paragraph that refuses to be \\'clear, brief, bold.\\' I dip back into The Elements of Style and am refreshed. After Scott Simon interviewed me on NPR about whether the word \\'e-mail\\' needs a hyphen (yes, it does), some listeners, including friends of mine, wondered why I had answered in the affirmative when asked, in passing, \\'Are you a drunken white man?\\' Those listeners misheard. \\'Strunk and White man\\' was what Scott said.\" -- Roy Blount Jr.\"Strunk & White--writing\\'s good- natured law firm--still contains enough sparkling good sense to clean up the whole bloviating blogosphere.\" -- Thomas Mallon \"I used Strunk -- that\\'s what we called it, Strunk -- as a student at Berkeley fifty years ago. I didn\\'t know that it was new, and that we were the first generation to be educated in The Elements of Style. I got a firm foundation in the English language, learned to write basically, and could depict the realistic world. Then I was able to become an impressionist and expressionist.\" -- Maxine Hong Kingston \"Strunk and White\\'s gigantic little book must be the most readable advice on writing ever written. Side by side with Roget, Shakespeare, the Bible, and a dictionary, it\\'s an essential for every writer\\'s shelf.\" -- X.J. Kennedy \"With what joy I welcome the fiftieth anniversary of The Elements of Style. I am greatly indebted to this book for the invaluable help it has given me all these years.\" -- Horton Foote \"Elegant, droll, and perfectly proportioned, and like your favorite aunt, strict but affectionate. And, like your favorite aunt, full of optimism: You can, and will, be a better writer!There has never been a better, briefer, or more loved book about the art and craft of communicating.\" -- Susan Orlean \"This book is an essential tool. It has been of great use to me and is probably responsible for my best writing. I owe my success to Strunk and White; only the mistakes are mine.\" -- Ben Affleck, in O, the Oprah Magazine \"This book is a wonderful example of teaching by example. Not only does it recommend clear and concise writing, it demonstrates it. Written in the style of a friend offering help, it is a godsend to anyone wanting to put words on paper. Thank you, Messieurs Strunk and White. And Happy Anniversary, Elements of Style.\" -- S.E. Hinton \"When I began to have ...I wouldn\\'t say arguments but conversations in my mind with Strunk and White about a few of their rules and principles, I knew I was coming into my own. If only they were still here to talk things over! No doubt their side of the exchange would be kindly put, well-informed, and wise. They\\'d probably help me with my side of it. What more could one want from writers reaching out to help other writers?\"-- Barbara Wallraff, language columnist for The Atlantic \"I don\\'t believe there is a serious writer alive who doesn\\'t have a worn copy of \\'Strunk & White\\'on his or her bookshelf.\" -- Mignon Fogarty, author of Grammar Girl\\'s Quick and Dirty Tips for Better Writing \"This little book has inspired hundreds of thousands of people to write better -- partly by precept and partly by example. It continues to influence more writers than any other. It\\'s a force for good in the world.\" -- Bryan A. Garner, author of Garner\\'s Modern American Usage \"I can think of no better guide to good writing, and I always think of this little classic with a warm heart. More importantly, I revisit its pages often. It\\'s the one essential book on writing.\" -- Jay Parini, author of Why Poetry Matters \"Clarity and simplicity have always been the goals, and this book shows the way. It has always been a lighthouse in the dark and stormy night of student prose, of all of our prose.\" -- Ron Carlson \"The only rules you are ever going to get from me are all in Strunk and White.\" --Ursula K.Le Guin, from Steering the Craft \"[The Elements of Style is] a book to which I return from time to time, the way I periodically reread Shakespeare. I always discover something new, settle a question that has been puzzling me, or learn a principle of usage that I have been pretending to know, a pretense that has resulted in inconsistency and in the sort of errors from which I can only pray some saintly copy editor will save me.\" -- Francine Prose, from ReadingLike A Writer \"!still a little book, small enough and important enough to carry in your pocket, as I carry mine. \" -- Charles Osgood \"Almost every writer has a Strunk and White story. One journalism professor spends the first two weeks of school forcing his students to memorize the book. A top editor at a major paper buys copies at yard sales to distribute to her writers and interns. It has even caused love affairs...Could its greatness be any more clear?\" -- Jesse Sheidlower, American Editor of the OxfordEnglish Dictionary, on NPR \"If the English language is one of the finest homes ever devised for the human spirit, Elements is the best guided house tour we\\'ve got.\" --David Gelernter, The Wall Street Journal \"!Should be the daily companion of anyone who writes for a living and, for that matter, anyone who writes at all.\" --Jonathan Yardley,Greensboro (N.C.) Daily News \"No book in shorter space, with fewer words, will help any writer more than this persistent little volume.\" -- Herbert A. Kenny, The Boston Globe \"Buy it, study it, enjoy it. It\\'s as timeless as a book can be in our age of volubility.\" -- Charles Poore, The New York Times \"White is one of the best stylists and most lucid minds in this country. What he says and his way of saying it are equally rewarding.\" -- Edmund Fuller, The Wall Street Journal \"If you have any young friends who aspire to become writers, the second greatest favor you can do them is to present them with copies of The Elements of Style. The first greatest, of course, is to shoot them now, while they\\'re happy.\" -- Dorothy Parker, Esquire',\n",
       " 'Integrated transcriptional profiling and linkage analysis for identification of genes underlying disease. Integration of genome-wide expression profiling with linkage analysis is a new approach to identifying genes underlying complex traits. We applied this approach to the regulation of gene expression in the BXH/HXB panel of rat recombinant inbred strains, one of the largest available rodent recombinant inbred panels and a leading resource for genetic analysis of the highly prevalent metabolic syndrome. In two tissues important to the pathogenesis of the metabolic syndrome, we mapped cis- and trans-regulatory control elements for expression of thousands of genes across the genome. Many of the most highly linked expression quantitative trait loci are regulated in cis, are inherited essentially as monogenic traits and are good candidate genes for previously mapped physiological quantitative trait loci in the rat. By comparative mapping we generated a data set of 73 candidate genes for hypertension that merit testing in human populations. Mining of this publicly available data set is expected to lead to new insights into the genes and regulatory pathways underlying the extensive range of metabolic and cardiovascular disease phenotypes that segregate in these recombinant inbred strains.',\n",
       " 'Complex trait analysis of gene expression uncovers polygenic and pleiotropic networks that modulate nervous system function. Patterns of gene expression in the central nervous system are highly variable and heritable. This genetic variation among normal individuals leads to considerable structural, functional and behavioral differences. We devised a general approach to dissect genetic networks systematically across biological scale, from base pairs to behavior, using a reference population of recombinant inbred strains. We profiled gene expression using Affymetrix oligonucleotide arrays in the BXD recombinant inbred strains, for which we have extensive SNP and haplotype data. We integrated a complementary database comprising 25 years of legacy phenotypic data on these strains. Covariance among gene expression and pharmacological and behavioral traits is often highly significant, corroborates known functional relations and is often generated by common quantitative trait loci. We found that a small number of major-effect quantitative trait loci jointly modulated large sets of transcripts and classical neural phenotypes in patterns specific to each tissue. We developed new analytic and graph theoretical approaches to study shared genetic modulation of networks of traits using gene sets involved in neural synapse function as an example. We built these tools into an open web resource called WebQTL that can be used to test a broad array of hypotheses.',\n",
       " 'Water conduction through the hydrophobic channel of a carbon nanotube.. Confinement of matter on the nanometre scale can induce phase transitions not seen in bulk systems(1). In the case of water, so-called drying transitions occur on this scale(2-5) as a result of strong hydrogen-bonding between water molecules, which can cause the liquid to recede from nonpolar surfaces to form a vapour layer separating the bulk phase from the surface(6). Here we report molecular dynamics simulations showing spontaneous and continuous filling of a nonpolar carbon nanotube with a one-dimensionally ordered chain of water molecules. Although the molecules forming the chain are in chemical and thermal equilibrium with the surrounding bath, we observe pulse-like transmission of water through the nanotube. These transmission bursts result from the tight hydrogen-bonding network inside the tube, which ensures that density fluctuations in the surrounding bath lead to concerted and rapid motion along the tube axis(7-9). We also rnd that a minute reduction in the attraction between the tube wall and water dramatically affects pore hydration, leading to sharp, two-state transitions between empty and filled states on a nanosecond timescale. These observations suggest that carbon nanotubes, with their rigid nonpolar structures(10,11), might be exploited as unique molecular channels for water and protons, with the channel occupancy and conductivity tunable by changes in the local channel polarity and solvent conditions.',\n",
       " 'Diffusion Kernels on Statistical Manifolds. A family of kernels for statistical learning is introduced that exploits the geometric structure of statistical models. The kernels are based on the heat equation on the Riemannian manifold defined by the Fisher information metric associated with a statistical family, and generalize the Gaussian kernel of Euclidean space. As an important special case, kernels based on the geometry of multinomial families are derived, leading to kernel-based learning algorithms that apply naturally to discrete data. Bounds on covering numbers and Rademacher averages for the kernels are proved using bounds on the eigenvalues of the Laplacian on Riemannian manifolds. Experimental results are presented for document classification, for which the use of multinomial geometry is natural and well motivated, and improvements are obtained over the standard use of Gaussian or linear kernels, which have been the standard for text classification.',\n",
       " 'The Genome of the Basidiomycetous Yeast and Human Pathogen Cryptococcus neoformans. Cryptococcus neoformans is a basidiomycetous yeast ubiquitous in the environment, a model for fungal pathogenesis, and an opportunistic human pathogen of global importance. We have sequenced its [~]20-megabase genome, which contains [~]6500 intron-rich gene structures and encodes a transcriptome abundant in alternatively spliced and antisense messages. The genome is rich in transposons, many of which cluster at candidate centromeric regions. The presence of these transposons may drive karyotype instability and phenotypic variation. C. neoformans encodes unique genes that may contribute to its unusual virulence properties, and comparison of two phenotypically distinct strains reveals variation in gene content in addition to sequence polymorphisms between the genomes. 10.1126/science.1103773',\n",
       " 'Genome Biology .  Full text   RNA interference is not involved in natural antisense mediated regulation of gene expression in mammals Antisense transcription, yielding both coding and non-coding RNA, is a widespread phenomenon in mammals. The mechanism by which natural antisense transcripts (NAT) may regulate gene expression are largely unknown. The aim of the present study was to explore the mechanism of reciprocal sense-antisense (S-AS) regulation by studying the effects of a coding and non-coding NAT on corresponding gene expression, and to investigate the possible involvement of endogenous RNA interference (RNAi) in S-AS interactions.  Results  We have examined the mechanism of S-AS RNA base pairing, using thymidylate synthase and hypoxia inducible factor-1α as primary examples of endogenous genes with coding and non-coding NAT partners, respectively. Here we provide direct evidence against S-AS RNA duplex formation in the cytoplasm of human cells and subsequent activation of RNAi.',\n",
       " 'Wait-free synchronization. A  wait-free  implementation of a concurrent data object is one that guarantees that any process can complete any operation in a finite number of steps, regardless of the execution speeds of the other processes. The problem of constructing a wait-free implementation of one data object from another lies at the heart of much recent work in concurrent algorithms, concurrent data structures, and multiprocessor architectures. First, we introduce a simple and general technique, based on reduction to a concensus protocol, for proving statements of the form, &ldquo;there is no wait-free implementation of X by Y.&rdquo; We derive a hierarchy of objects such that no object at one level has a wait-free implementation in terms of objects at lower levels. In particular, we show that    atomic read/write registers, which have been the focus of much recent attention, are at the bottom of the hierarchy: thay cannot be used to construct wait-free implementations of many simple and familiar data types. Moreover, classical synchronization primitives such as test&amp;set  and  fetch&amp;add , while more powerful than  read  and  write , are also computationally weak, as are the standard message-passing primitives. Second, nevertheless, we show that there do exist simple universal objects from which one can construct a wait-free implementation of any sequential object.',\n",
       " 'Why cryptosystems fail. Designers of cryptographic systems are at a disadvantage to most other engineers, in that information on how their systems fail is hard to get: their major users have traditionally been government agencies, which are very secretive about their mistakes. In this article, we present the results of a survey of the failure modes of retail banking systems, which constitute the next largest application of cryptology. It turns out that the threat model commonly used by cryptosystem designers was wrong: most frauds were not caused by cryptanalysis or other technical attacks, but by implementation errors and management failures. This suggests that a paradigm shift is overdue in computer security; we look at some of the alternatives, and see some signs that this shift may be getting under way.',\n",
       " 'Region-based memory management in cyclone. Cyclone is a type-safe programming language derived from C. The primary design goal of Cyclone is to let programmers control data representation and memory management without sacrificing type-safety. In this paper, we focus on the region-based memory management of Cyclone and its static typing discipline. The design incorporates several advancements, including support for region subtyping and a coherent integration with stack allocation and a garbage collector. To support separate compilation, Cyclone requires programmers to write some explicit region annotations, but a combination of default annotations, local type inference, and a novel treatment of region effects reduces this burden. As a result, we integrate C idioms in a region-based framework. In our experience, porting legacy C to Cyclone has required altering about 8% of the code; of the changes, only 6% (of the 8%) were region annotations.',\n",
       " 'A Gossip-Style Failure Detection Service. Failure Detection is valuable for system management, replication, load balancing, and other distributed services. To date, Failure Detection Services scale badly in the number of members that are being monitored. This paper describes a new protocol based on gossiping that does scale well and provides timely detection. We analyze the protocol, and then extend it to discover and leverage the underlying network topology for much improved resource utilization. We then combine it with another protocol, based on broadcast, that is used to handle partition failures.',\n",
       " 'Bridging Physical and Virtual Worlds with Electronic Tags. The role of computers in the modern office has tended to split our activities between virtual interactions in the realm of the computer and physical interactions with real objects that have been part of the traditional office infrastructure. This paper discusses a variety of scenarios we have implemented in which the physical world can be invisibly and seamlessly augmented with electronic tags in order to connect physical objects with virtual representations or computational functionality. We...',\n",
       " 'Group communication specifications: a comprehensive study. View-oriented group communication is an important and widely used building block for many distributed applications. Much current research has been dedicated to specifying the semantics and services of view-oriented group communication systems (GCSs). However, the guarantees of different GCSs are formulated using varying terminologies and modeling techniques, and the specifications vary in their rigor. This makes it difficult to analyze and compare the different systems. This survey provides a comprehensive set of clear and rigorous specifications, which may be combined to represent the guarantees of most existing GCSs. In the light of these specifications, over 30 published GCS specifications are surveyed. Thus, the specifications serve as a unifying framework for the classification, analysis, and comparison of group communication systems. The survey also discusses over a dozen different applications of group communication systems, shedding light on the usefulness of the presented specifications. This survey is aimed at both system builders and theoretical researchers. The specification framework presented in this article will help builders of group communication systems understand and specify their service semantics; the extensive survey will allow them to compare their service to others. Application builders will find a guide here to the services provided by a large variety of GCSs, which could help them choose the GCS appropriate for their needs. The formal framework may provide a basis for interesting theoretical work, for example, analyzing relative strengths of different properties and the costs of implementing them.',\n",
       " 'The Entire Regularization Path for the Support Vector Machine. The support vector machine (SVM) is a widely used tool for classification. Many efficient implementations exist for fitting a two-class SVM model. The user has to supply values for the tuning parameters: the regularization cost parameter, and the kernel parameters. It seems a common practice is to use a default value for the cost parameter, often leading to the least restrictive model. In this paper we argue that the choice of the cost parameter can be critical. We then derive an algorithm that can fit the entire path of SVM solutions for every value of the cost parameter, with essentially the same computational cost as fitting one SVM model. We illustrate our algorithm on some examples, and use our representation to give further insight into the range of SVM solutions.',\n",
       " 'Criterion functions for document clustering: Experiments and analysis. In recent years, we have witnessed a tremendous growth in the volume of text documents available on the Internet,  digital libraries, news sources, and company-wide intranets. This has led to an increased interest in developing  methods that can help users to effectively navigate, summarize, and organize this information with the ultimate  goal of helping them to find what they are looking for. Fast and high-quality document clustering algorithms play an  important role towards this goal as they have been shown to provide both an intuitive navigation/browsing mechanism  by organizing large amounts of information into a small number of meaningful clusters as well as to greatly improve  the retrieval performance either via cluster-driven dimensionality reduction, term-weighting, or query expansion. This  ever-increasing importance of document clustering and the expanded range of its applications led to the development  of a number of new and novel algorithms with different complexity-quality trade-offs. Among them, a class of  clustering algorithms that have relatively low computational requirements are those that treat the clustering problem  as an optimization process which seeks to maximize or minimize a particular clustering criterion function defined  over the entire clustering solution.',\n",
       " 'Deconstructing the relationship between genetics and race.  The success of many strategies for finding genetic variants that underlie complex traits depends on how genetic variation is distributed among human populations. This realization has intensified the investigation of genetic differences among groups, which are often defined by commonly used racial labels. Some scientists argue that race is an adequate proxy of ancestry, whereas others claim that race belies how genetic variation is apportioned. Resolving this controversy depends on understanding the complicated relationship between race, ancestry and the demographic history of humans. Recent discoveries are helping us to deconstruct this relationship, and provide better guidance to scientists and policy makers.',\n",
       " 'Collaborative Authoring on the Web: A Genre Analysis of Online Encyclopedias. This paper presents the results of a genre analysis of two web-based collaborative authoring environments, Wikipedia and Everything2, both of which are intended as repositories of encyclopedic knowledge and are open to contributions from the public. Using corpus linguistic methods and factor analysis of word counts for features of formality and informality, we show that the greater the degree of post-production editorial control afforded by the system, the more formal and standardized the language of the collaboratively-authored documents becomes, analogous to that found in traditional print encyclopedias. Paradoxically, users who faithfully appropriate such systems create homogeneous entries, at odds with the goal of open-access authoring environments to create diverse content. The findings shed light on how users, acting through mechanisms provided by the system, can shape (or not) features of content in particular ways. We conclude by identifying sub-genres of web-based collaborative authoring environments based on their technical affordances.',\n",
       " 'Conversations in the Blogosphere: An Analysis \"From the Bottom Up\". The \"blogosphere\" has been claimed to be a densely in- terconnected conversation, with bloggers linking to other bloggers, referring to them in their entries, and posting comments on each other\\'s blogs. Most such characteriza- tions have privileged a subset of popular blogs, known as the \\'A-list.\\' This study empirically investigates the extent to which, and in what patterns, blogs are interconnected, taking as its point of departure randomly-selected blogs. Quantitative social network analysis, visualization of link patterns, and qualitative analysis of references and comments in pairs of reciprocally-linked blogs show that A-list blogs are overrepresented and central in the network, although other groupings of blogs are more densely interconnected. At the same time, a majority of blogs link sparsely or not at all to other blogs in the sam- ple, suggesting that the blogosphere is partially intercon- nected and sporadically conversational.',\n",
       " 'Bridging the Gap: A Genre Analysis of Weblogs. Weblogs (blogs) — frequently modified web pages in which dated entries are listed in reverse chronological sequence — are the latest genre of Internet communication to attain widespread popularity, yet their characteristics have not been systematically described. This paper presents the results of a content analysis of 203 randomly-selected weblogs,  comparing the empirically observable features of the corpus with popular claims about the nature of weblogs, and finding them to differ in a number of respects. Notably, blog authors, journalists and scholars alike exaggerate the extent to which blogs are interlinked, interactive,  and oriented towards external events, and under-estimate the importance of blogs as individualistic, intimate forms of self-expression. Based on the profile generated by the empirical analysis, we consider the likely antecedents of the blog genre, situate it with respect to the dominant forms of digital communication on the Internet today, and advance predictions about its long-term impacts.',\n",
       " 'The Consequences of Literacy. The importance of writing as a means of communication in a society formerly without it, or where writing has been confined to particular groups, is enormous. It objectifies speech, provides language with a material correlative, and in this material form speech can be transmitted over space and preserved over time. In this book the contributors discuss cultures at different levels of sophistication and literacy and examine the importance of writing on the development of these societies. All the articles except the first were specially written for this book and the extensive introduction unites and synthesizes the material.',\n",
       " 'Information extraction. The automatic extraction of information from unstructured sources has opened up new avenues for querying, organizing, and analyzing data by drawing upon the clean semantics of structured databases and the abundance of unstructured data. The field of information extraction has its genesis in the natural language processing community where the primary impetus came from competitions centered around the recognition of named entities like people names and organization from news articles. As society became more data oriented with easy online access to both structured and unstructured data, new applications of structure extraction came around. Now, there is interest in converting our personal desktops to structured databases, the knowledge in scientific publications to structured records, and harnessing the Internet for structured fact finding queries. Consequently, there are many different communities of researchers bringing in techniques from machine learning, databases, information retrieval, and computational linguistics for various aspects of the information extraction problem.  This review is a survey of information extraction research of over two decades from these diverse communities. We create a taxonomy of the field along various dimensions derived from the nature of the extraction task, the techniques used for extraction, the variety of input resources exploited, and the type of output produced. We elaborate on rule-based and statistical methods for entity and relationship extraction. In each case we highlight the different kinds of models for capturing the diversity of clues driving the recognition process and the algorithms for training and efficiently deploying the models. We survey techniques for optimizing the various steps in an information extraction pipeline, adapting to dynamic data, integrating with existing entities and handling uncertainty in the extraction process.',\n",
       " 'Using mixture models for collaborative filtering. A collaborative filtering system at an e-commerce site or similar service uses data about aggregate user behavior to make recommendations tailored to specific user interests. We develop recommendation algorithms with provable performance guarantees in a probabilistic mixture model for collaborative filtering proposed by Hoffman and Puzicha. We identify certain novel parameters of mixture models that are closely connected with the best achievable performance of a recommendation algorithm; we show that for any system in which these parameters are bounded, it is possible to give recommendations whose quality converges to optimal as the amount of data grows. All our bounds depend on a new measure of independence that can be viewed as an $L_1$-analogue of the smallest singular value of a matrix. Using this, we introduce a technique based on generalized pseudoinverse matrices and linear programming for handling sets of high-dimensional vectors. We also show that standard approaches based on $L_2$ spectral methods are not strong enough to yield comparable results, thereby suggesting some inherent limitations of spectral analysis.',\n",
       " 'The Semantics of Predicate Logic as a Programming Language. Sentences in first-order predicate logic can be usefully interpreted as programs In this paper the operational and fixpomt semantics of predicate logic programs are defined, and the connections with the proof theory and model theory of logic are investigated It is concluded that operational semantics is a part of proof theory and that fixpolnt semantics is a special case of model-theoret:c semantics KEY WORDS AND PHRASES predicate logic as a programming language, semantics of programming languages, resolution theorem proving, operaUonal versus denotatlonal semantics, SL-resoluuon, flxpomt characteriza-tion',\n",
       " 'Call-by-name, call-by-value, and the $\\\\lambda$-calculus. This paper examines the old question of the relationship between ISWIM and the λ-calculus, using the distinction between call-by-value and call-by-name. It is held that the relationship should be mediated by a standardisation theorem. Since this leads to difficulties, a new λ-calculus is introduced whose standardisation theorem gives a good correspondence with ISWIM as given by the SECD machine, but without the  letrec  feature. Next a call-by-name variant of ISWIM is introduced which is in an analogous correspondence withthe usual λ-calculus. The relation between call-by-value and call-by-name is then studied by giving simulations of each language by the other and interpretations of each calculus in the other. These are obtained as another application of the continuation technique. Some emphasis is placed throughout on the notion of operational equality (or contextual equality). If terms can be proved equal in a calculus they are operationally equal in the corresponding language. Unfortunately, operational equality is not preserved by either of the simulations.',\n",
       " \"Design Patterns: Elements of Reusable Object-Oriented Software. _Design Patterns_ is based on the idea that there are only so many design problems in computer programming. This book identifies some common program- design problems--such as adapting the interface of one object to that of another object or notifying an object of a change in another object's state-- and explains the best ways (not always the obvious ways) that the authors know to solve them. The idea is that you can use the authors' sophisticated design ideas to solve problems that you often waste time solving over and over again in your own programming.  The authors have come up with some ingenious ways to solve some common vexations among object-oriented programmers. Want to build a page-layout program that embeds inline images among characters of various sizes? How about building a program that converts files of one format to another? Chances are, some programmer already has thought of a better solution than you will and the recipes you need are here. Solutions are presented in generalised diagrams of data and logic structures. The idea is that you can take the concepts presented here and adapt them--in whatever language you use--to your individual situation. You may have to read some of the chapters several times before you fully understand them, but when you find a solution in this book, it will make your job easier and your results more elegant. --_Jake Bond_\",\n",
       " \"Smalltalk-80: The Language and Its Implementation. From the Preface (See Front Matter for full Preface) Advances in the design and production of computer hardware have brought many more people into direct contact with computers. Similar advances in the design and production of computer software are required in order that this increased contact be as rewarding as possible. The Smalltalk-80 system is a result of a decade of research into creating computer software that is appropriate for producing highly functional and interactive contact with personal computer systems. This book is the first detailed account of the Smalltalk-80 system. It is divided into four major parts: Part One -- an overview of the concepts and syntax of the programming language. Part Two -- an annotated and illustrated specification of the system's functionality. Part Three -- an example of the design and implementation of a moderate-size application. Part Four -- a specification of the Smalltalk-80 virtual machine.\",\n",
       " 'The Art of the Metaobject Protocol. This book details the meta-object protocol, the framework on which the Common Lisp object system (CLOS) is based. The philosophy behind the meta-object protocol is that different applications may require different kinds of object models, and so the object model itself should be subject to program control. <I>The Art of the Meta-Object Protocol</I> provides a wonderful working example of how Lisp can be extended and how it can evolve to incorporate new language constructs. First, the book describes how CLOS is actually implemented by working through a subset. Then it goes on to develop the meta- object protocol in great detail. <I>The Art of the Meta-Object Protocol</I> is useful for the advanced CLOS user as well as for anyone interested in object-oriented programming and language design.',\n",
       " 'Geographic routing without location information. For many years, scalable routing for wireless communication systems was a compelling but elusive goal. Recently, several routing algorithms that exploit geographic information (e.g. {GPSR)} have been proposed to achieve this goal. These algorithms refer to nodes by their location, not address, and use those coordinates to route greedily, when possible, towards the destination. However, there are many situations where location information is not available at the nodes, and so geographic methods cannot be used. In this paper we define a scalable coordinate-based routing algorithm that does not rely on location information, and thus can be used in a wide variety of ad hoc and sensornet environments.',\n",
       " \"A cookbook for using the model-view controller user interface paradigm in Smalltalk-80. This essay describes the Model-View-Controller (MVC) programming paradigm and methodology used in the Smalltalk-80TM programming system. MVC programming is the application of a three-way factoring, whereby objects of different classes take over the operations related to the application domain, the display of the application's state, and the user interaction with the model and the view. We present several extended examples of MVC implementations and of the layout of composite application views. The Appendices provide reference materials for the Smalltalk-80 programmer wishing to understand and use MVC better within the Smalltalk-80 system.\",\n",
       " 'The use of phrases and structured queries in information retrieval. Both phrases and Boolean queries have a long history in information retrieval, particularly in commercial systems. In previous work, Boolean queries have been used as a source of phrases for a statistical retrieval model. This work, like the majority of research on phrases, resulted in little improvement in retrieval effectiveness. In this paper, we describe an approach where phrases identified in natural language queries are used to build structured queries for a probabilistic retrieval model. Our results show that using phrases in this way can improve performance, and that phrases that are automatically extracted from a natural language query perform nearly as well as manually selected phrases.',\n",
       " 'Combining the language model and inference network approaches to retrieval. The inference network retrieval model, as implemented in the InQuery search engine, allows for richly structured queries. However, it incorporates a form of ad hoc tf.idf estimates for word probabilities. Language modeling offers more formal estimation techniques. In this paper we combine the language modeling and inference network approaches into a single framework. The resulting model allows structured queries to be evaluated using language modeling estimates. We explore the issues involved, such as combining beliefs and smoothing of proximity nodes. Experimental results are presented comparing the query likelihood model, the InQuery system, and our new model. The results reaffirm that high quality structured queries outperform unstructured queries and show that our system consistently achieves higher average precision than InQuery.',\n",
       " 'Object-oriented application frameworks. Computing power and network bandwidth have increased dramatically over the past decade, yet the design and implementation of complex software remain expensive and error-prone. Much of the cost and effort stems from the continuous rediscovery and reinvention of core concepts and components across the software industry. In particular, the growing heterogeneity of hardware architectures and diversity of operating system and communication platforms make it difficult to build correct, portable, efficient, and inexpensive applications from scratch.',\n",
       " 'Being Digital. {As the founder of MIT\\'s Media Lab and a popular columnist for <I>Wired</I>, Nicholas Negroponte has amassed a following of dedicated readers. Negroponte\\'s fans will want to get a copy of <I>Being Digital</I>, which is an edited version of the 18 articles he wrote for <I>Wired</I> about \"being digital.\" <p> Negroponte\\'s text is mostly a history of media technology rather than a set of predictions for future technologies. In the beginning, he describes the evolution of CD-ROMs, multimedia, hypermedia, HDTV (high-definition television), and more. The section on interfaces is informative, offering an up-to-date history on visual interfaces, graphics, virtual reality (VR), holograms, teleconferencing hardware, the mouse and touch-sensitive interfaces, and speech recognition.<p> In the last chapter and the epilogue, Negroponte offers visionary insight on what \"being digital\" means for our future. Negroponte praises computers for their educational value but recognizes certain dangers of technological advances, such as increased software and data piracy and huge shifts in our job market that will require workers to transfer their skills to the digital medium. Overall, <I>Being Digital</I> provides an informative history of the rise of technology and some interesting predictions for its future.} {In lively, mordantly witty prose, Negroponte decodes the mysteries--and debunks the hype--surrounding bandwidth, multimedia, virtual reality, and the Internet, and explains why such touted innovations as the fax and the CD-ROM are likely to go the way of the BetaMax. \"Succinct and readable. . . . If you suffer from digital anxiety . . . here is a book that lays it all out for you.\"--Newsday.<br><br><br><i>From the Trade Paperback edition.</i>}',\n",
       " 'Efficient incremental algorithms for dynamic detection of likely invariants. Dynamic detection of likely invariants is a program analysis that generalizes over observed values to hypothesize program properties. The reported program properties are a set of likely invariants over the program, also known as an operational abstraction. Operational abstractions are useful in testing, verification, bug detection, refactoring, comparing behavior, and many other tasks. Previous techniques for...',\n",
       " 'An instructional model for web-based e-learning education with a blended learning process approach. Web-based e-learning education research and development now focuses on the inclusion of new technological features and the exploration of software standards. However, far less effort is going into finding solutions to psychopedagogical problems in this new educational category. This paper proposes a psychopedagogical instructional model based on content structure, the latest research into information processing psychology and social contructivism, and defines a blended approach to the learning process. Technologically speaking, the instructional model is supported by learning objects, a concept inherited from the object-oriented paradigm.',\n",
       " 'Optimistic Replication. Data replication is a key technology in distributed data sharing systems, enabling higher availability and performance. This paper surveys optimistic replication algorithms that allow replica contents to diverge in the short term, in order to support concurrent work practices and to tolerate failures in low-quality communication links. The importance of such techniques is increasing as collaboration through wide-area and mobile networks becomes popular. Optimistic replication techniques are different from traditional \"pessimistic\" ones. Instead of synchronous replica coordination, an optimistic algorithm propagates changes in the background, discovers conflicts after they happen and reaches agreement on the final contents incrementally. We explore the solution space for optimistic replication algorithms. This paper identifies key challenges facing optimistic replication systems -- ordering operations, detecting and resolving conflicts, propagating changes efficiently, and bounding replica divergence -- and provides a comprehensive survey of techniques developed for addressing these challenges.',\n",
       " 'Active learning using pre-clustering. The paper is concerned with two-class active learning. While the common approach for collecting data in active learning is to select samples close to the classification boundary, better performance can be achieved by taking into account the prior data distribution. The main contribution of the paper is a formal framework that incorporates clustering into active learning. The algorithm first constructs a classifier on the set of the cluster representatives, and then propagates the classification decision to the other samples via a local noise model. The proposed model allows to select the most representative samples as well as to avoid repeatedly labeling samples in the same cluster. During the active learning process, the clustering is adjusted using the coarse-to-fine strategy in order to balance between the advantage of large clusters and the accuracy of the data representation. The results of experiments in image databases show a better performance of our algorithm compared to the current methods.',\n",
       " 'Evaluation of hierarchical clustering algorithms for document datasets. Fast and high-quality document clustering algorithms play an important role in providing intuitive navigation and browsing mechanisms by organizing large amounts of information into a small number of meaningful clusters. In particular, hierarchical clustering solutions provide a view of the data at different levels of granularity, making them ideal for people to visualize and interactively explore large document collections.In this paper we evaluate different partitional and agglomerative approaches for hierarchical clustering. Our experimental evaluation showed that partitional algorithms always lead to better clustering solutions than agglomerative algorithms, which suggests that partitional clustering algorithms are well-suited for clustering large document datasets due to not only their relatively low computational requirements, but also comparable or even better clustering performance. We present a new class of clustering algorithms called  constrained agglomerative algorithms  that combine the features of both partitional and agglomerative algorithms. Our experimental results showed that they consistently lead to better hierarchical solutions than agglomerative or partitional algorithms alone.',\n",
       " 'Multiagent Systems: A Survey from a Machine Learning Perspective. Distributed Artificial Intelligence (DAI) has existed as a subfield of AI for less than two decades. DAI is concerned with systems that consist of multiple independent entities that interact in a domain. Traditionally, DAI has been divided into two sub-disciplines: Distributed Problem Solving (DPS) focuses on the information management aspects of systems with several components working together towards a common goal; Multiagent Systems (MAS) deals with behavior management in collections of several independent entities, or agents. This survey of MAS is intended to serve as an introduction to the field and as an organizational framework. A series of general multiagent scenarios are presented. For each scenario, the issues that arise are described along with a sampling of the techniques that exist to deal with them. The presented techniques are not exhaustive, but they highlight how multiagent systems can be and have been used to build complex systems. When options exist, the techniques presented are biased towards machine learning approaches. Additional opportunities for applying machine learning to MAS are highlighted and robotic soccer is presented as an appropriate test bed for MAS. This survey does not focus exclusively on robotic systems. However, we believe that much of the prior research in non-robotic MAS is relevant to robotic MAS, and we explicitly discuss several robotic MAS, including all of those presented in this issue.',\n",
       " 'A unified framework for model-based clustering. Model-based clustering techniques have been widely used and have shown promising results in many applications involving complex data. This paper presents a unified framework for probabilistic model-based clustering based on a bipartite graph view of data and models that highlights the commonalities and differences among existing model-based clustering algorithms. In this view, clusters are represented as probabilistic models in a model space that is conceptually separate from the data space. For partitional clustering, the view is conceptually similar to the Expectation-Maximization (EM) algorithm. For hierarchical clustering, the graph-based view helps to visualize critical/important distinctions between similarity-based approaches and model-based approaches. The framework also suggests several useful variations of existing clustering algorithms. Two new variations—balanced model-based clustering and hybrid model-based clustering—are discussed and empirically evaluated on a variety of data types.',\n",
       " 'Transductive learning via spectral graph partitioning. We present a new method for transductive learning, which can be seen as a transductive version of the k nearest-neighbor classifier. Unlike for many other transductive learning methods, the training problem has a meaningful relaxation that can be solved globally optimally using spectral methods. We propose an algorithm that robustly achieves good generalization performance and that can be trained efficiently. A key advantage of the algorithm is that it does not require additional heuristics to avoid unbalanced splits. Furthermore, we show a connection to transductive Support Vector Machines, and that an effective Co-Training algorithm arises as a special case. 1.',\n",
       " 'The faculty of language: what\\'s special about it?. We examine the question of which aspects of language are uniquely human and uniquely linguistic in light of recent suggestions by Hauser, Chomsky, and Fitch that the only such aspect is syntactic recursion, the rest of language being either specific to humans but not to language (e.g. words and concepts) or not specific to humans (e.g. speech perception). We find the hypothesis problematic. It ignores the many aspects of grammar that are not recursive, such as phonology, morphology, case, agreement, and many properties of words. It is inconsistent with the anatomy and neural control of the human vocal tract. And it is weakened by experiments suggesting that speech perception cannot be reduced to primate audition, that word learning cannot be reduced to fact learning, and that at least one gene involved in speech and language was evolutionarily selected in the human lineage but is not specific to recursion. The recursion-only claim, we suggest, is motivated by Chomsky\\'s recent approach to syntax, the Minimalist Program, which de-emphasizes the same aspects of language. The approach, however, is sufficiently problematic that it cannot be used to support claims about evolution. We contest related arguments that language is not an adaptation, namely that it is \"perfect,\" non-redundant, unusable in any partial form, and badly designed for communication. The hypothesis that language is a complex adaptation for communication which evolved piecemeal avoids all these problems.',\n",
       " \"Stuff I've seen: a system for personal information retrieval and re-use. Most information retrieval technologies are designed to facilitate information discovery. However, much knowledge work involves finding and re-using previously seen information. We describe the design and evaluation of a system, called Stuff I've Seen (SIS), that facilitates information re-use. This is accomplished in two ways. First, the system provides a unified index of information that a person has seen, whether it was seen as email, web page, document, appointment, etc. Second, because the information has been seen before, rich contextual cues can be used in the search interface. The system has been used internally by more than 230 employees. We report on both qualitative and quantitative aspects of system use. Initial findings show that time and people are important retrieval cues. Users find information more easily using SIS, and use other search tools less frequently after installation. Unix commands [15], library book borrowing [7], and human memory [3].\",\n",
       " 'The relationship between sequence and interaction divergence in proteins.. There is currently a gap in knowledge between complexes of known three-dimensional structure and those known from other experimental methods such as affinity purifications or the two-hybrid system. This gap can sometimes be bridged by methods that extrapolate interaction information from one complex structure to homologues of the interacting proteins. To do this, it is important to know if and when proteins of the same type (e.g. family, superfamily or fold) interact in the same way. Here, we study interactions of known structure to address this question. We found all instances within the structural classification of proteins database of the same domain pairs interacting in different complexes, and then compared them with a simple measure (interaction RMSD). When plotted against sequence similarity we find that close homologues (30–40% or higher sequence identity) almost invariably interact the same way. Conversely, similarity only in fold (i.e. without additional evidence for a common ancestor) is only rarely associated with a similarity in interaction. The results suggest that there is a twilight zone of sequence similarity where it is not possible to say whether or not domains will interact similarly. We also discuss the rare instances of fold similarities interacting the same way, and those where obviously homologous proteins interact differently.',\n",
       " 'Recoverable one-dimensional encoding of protein three-dimensional structures.. Protein one-dimensional (1D) structures such as secondary structure and contact number provide intuitive pictures to understand how the native three-dimensional (3D) structure of a protein is encoded in the amino acid sequence. However, it has not been clear whether a given set of 1D structures contains sufficient information for recovering the underlying 3D structure. Here we show that the 3D structure of a protein can be recovered from a set of three types of 1D structures, namely, secondary structure, contact number and residue-wise contact order which is introduced here for the first time. Using simulated annealing molecular dynamics simulations, the structures satisfying the given native 1D structural restraints were sought for 16 proteins of various structural classes and of sizes ranging from 56 to 146 residues. By selecting the structures best satisfying the restraints, all the proteins showed a coordinate RMS deviation of less than 4A from the native structure, and for most of them, the deviation was even less than 2A. The present result opens a new possibility to protein structure prediction and our understanding of the sequence-structure relationship.',\n",
       " 'Splitstream: High-bandwidth multicast in cooperative environments. In tree-based multicast systems, a relatively small number of interior nodes carry the load of forwarding multicast messages. This works well when the interior nodes are highly-available, dedicated infrastructure routers but it poses a problem for application-level multicast in peer-to-peer systems. SplitStream addresses this problem by striping the content across a forest of interior-node-disjoint multicast trees that distributes the forwarding load among all participating peers. For example, it is possible to construct efficient SplitStream forests in which each peer contributes only as much forwarding bandwidth as it receives. Furthermore, with appropriate content encodings, SplitStream is highly robust to failures because a node failure causes the loss of a single stripe on average. We present the design and implementation of SplitStream and show experimental results obtained on an Internet testbed and via large-scale network simulation. The results show that SplitStream distributes the forwarding load among all peers and can accommodate peers with different bandwidth capacities while imposing low overhead for forest construction and maintenance.',\n",
       " 'Tor: The Second-Generation Onion Router. We present Tor, a circuit-based low-latency anonymous communication service. This second-generation Onion Routing system addresses limitations in the original design by adding perfect forward secrecy, congestion control, directory servers, integrity checking, configurable exit policies, and a practical design for location-hidden services via rendezvous points. Tor works on the real-world Internet, requires no special privileges or kernel modifications, requires little synchronization or coordination between nodes, and provides a reasonable tradeoff between anonymity, usability, and efficiency. We briefly describe our experiences with an international network of more than 30 nodes. We close with a list of open problems in anonymous communication.',\n",
       " \"The Symbolic Species: The Co-Evolution of Language and the Brain. {This revolutionary book provides fresh answers to long-standing questions of human origins and consciousness. Drawing on his breakthrough research in comparative neuroscience, Terrence Deacon offers a wealth of insights into the significance of symbolic thinking: from the co-evolutionary exchange between language and brains over two million years of hominid evolution to the ethical repercussions that followed man's newfound access to other people's thoughts and emotions. Informing these insights is a new understanding of how Darwinian processes underlie the brain's development and function as well as its evolution. In contrast to much contemporary neuroscience that treats the brain as no more or less than a computer, Deacon provides a new clarity of vision into the mechanism of mind. It injects a renewed sense of adventure into the experience of being human.}\",\n",
       " \"Advanced Topics in Types and Programming Languages. {The study of type systems for programming languages now touches many areas of computer science, from language design and implementation to software engineering, network security, databases, and analysis of concurrent and distributed systems. This book offers accessible introductions to key ideas in the field, with contributions by experts on each topic.<br /> <br /> The topics covered include precise type analyses, which extend simple type systems to give them a better grip on the run time behavior of systems; type systems for low-level languages; applications of types to reasoning about computer programs; type theory as a framework for the design of sophisticated module systems; and advanced techniques in ML-style type inference.<br /> <br /> <i>Advanced Topics in Types and Programming Languages</i> builds on Benjamin Pierce's <i>Types and Programming Languages</i> (MIT Press, 2002); most of the chapters should be accessible to readers familiar with basic notations and techniques of operational semantics and type systems -- the material covered in the first half of the earlier book.<br /> <br /> <i>Advanced Topics in Types and Programming Languages</i> can be used in the classroom and as a resource for professionals. Most chapters include exercises, ranging in difficulty from quick comprehension checks to challenging extensions, many with solutions.}\",\n",
       " \"Cognitive Psychology: A Student's Handbook. {This is a thorough revision and updating of the extremely successful third edition. As in previous editions, the following three perspectives are considered in depth: experimental cognitive psychology; cognitive science, with its focus on cognitive modelling; and cognitive neuropsychology with its focus on cognition following brain damage. In addition, and new to this edition, is detailed discussion of the cognitive neuroscience perspective, which uses advanced brain-scanning techniques to clarify the functioning of the human brain. There is detailed coverage of the dynamic impact of these four perspectives on the main areas of cognitive psychology, including perception, attention, memory, knowledge representation, categorisation, language, problem-solving, reasoning, and judgement.<br> The aim is to provide comprehensive coverage that is up-to-date, authoritative, and accessible. All existing chapters have been extensively revised and re-organised. Some of the topics receiving much greater coverage in this edition are: brain structures in perception, visual attention, implicit learning, brain structures in memory, prospective memory, exemplar theories of categorisation, language comprehension, connectionist models in perception, neuroscience studies of thinking, judgement, and decision making.<br> Cognitive Psychology: A Students Handbook will be essential reading for undergraduate students of psychology. It will also be of interest to students taking related courses in computer science, education, linguistics, physiology, and medicine.}\",\n",
       " 'Metaphors We Live By. {<div>The now-classic <i>Metaphors We Live By</i> changed our understanding of metaphor and its role in language and the mind. Metaphor, the authors explain, is a fundamental mechanism of mind, one that allows us to use what we know about our physical and social experience to provide understanding of countless other subjects. Because such metaphors structure our most basic understandings of our experience, they are \"metaphors we live by\"--metaphors that can shape our perceptions and actions without our ever noticing them.<br><br>In this updated edition of Lakoff and Johnson\\'s influential book, the authors supply an afterword surveying how their theory of metaphor has developed within the cognitive sciences to become central to the contemporary understanding of how we think and how we express our thoughts in language.<br><br></div>}',\n",
       " 'A guide to LATEX: Document preparation for beginners and advanced users. {A completely revised edition of this accessible guide to LATEX document preparation, bringing it up to date with the latest releases and Web ad PC based developments.  A Guide to LATEX covers the basics as well as advanced LATEX topics and contains numerous practical examples and handy tips for avoiding problems.  It covers the latest LATEX extensions and has been completely updated to cover latest releases and upgrades.   <P>The book explains the LATEX macro package for the TEX text formatting program, presenting a complete description for beginners, going on to more advanced and specialized features.  Files for LATEX processing contain the actual text plus markup and programming command, al as ASCII text, something tat makes them portable to every computer system.  The LATEX/TEX program processes these files to produce high-quality typeset results, especially for complicated mathematics.  <P>LATEX offers the user all the features of any text processing system: automatic section formatting, numbering of sections, figures, tables and equations, table of contents, lists of figures and tables, cross-referencing to the numbers, bibliography, keyword index, colour, inclusion of illustrations.  All of these are demonstrated to the reader via examples and exercises through a structure that takes him or her from the simplest beginnings to the more complicated refinements.}',\n",
       " 'E-mail writing as a cross-cultural learning experience. This study looks into the cultural dimension involved in the e-mail correspondence between university EFL students in Taiwan and pre-service bilingual/ESL teachers in the USA. E-mail entries and end-of-project reports were analyzed to yield insights into the cross-cultural communication process. The data analysis focused on the types of cultural information transmitted and effects of cultural assumptions and values on communication effectiveness. The findings revealed perceived fundamental characteristics of both Chinese and American cultures by the two groups of participants. It was also found that curiosity toward the other culture was a motivating factor for on-going correspondence, but cultural presumptions were sometimes a hindrance for communication; positive interpretations of cultural differences and empathy were key factors contributing to the removal of communication obstacles. Although there is no substitute for actual experiences of immersing into the target culture, cross-cultural e-mail correspondence sensitized the participants to cultural differences and served as a learning experience for better cross-cultural understanding.',\n",
       " \"Linked: How Everything Is Connected to Everything Else and What It Means. How is the human brain like the AIDS epidemic? Ask physicist Albert-László Barabási and he'll explain them both in terms of networks of individual nodes connected via complex but understandable relationships. _Linked: The New Science of Networks_ is his bright, accessible guide to the fundamentals underlying neurology, epidemiology, Internet traffic, and many other fields united by complexity.  Barabási's gift for concrete, nonmathematical explanations and penchant for eccentric humor would make the book thoroughly enjoyable even if the content weren't engaging. But the results of Barabási's research into the behavior of networks are deeply compelling. Not all networks are created equal, he says, and he shows how even fairly robust systems like the Internet could be crippled by taking out a few super-connected nodes, or hubs. His mathematical descriptions of this behavior are helping doctors, programmers, and security professionals design systems better suited to their needs. _Linked_ presents the next step in complexity theory--from understanding chaos to practical applications. _--Rob Lightner_\",\n",
       " \"Identity and Search in Social Networks. Social networks have the surprising property of being ``searchable'': Ordinary people are capable of directing messages through their network of acquaintances to reach a specific but distant target person in only a few steps. We present a model that offers an explanation of social network searchability in terms of recognizable personal identities: sets of characteristics measured along a number of social dimensions. Our model defines a class of searchable networks and a method for searching them that may be applicable to many network search problems, including the location of data files in peer-to-peer networks, pages on the World Wide Web, and information in distributed databases.\",\n",
       " 'Automatic Creation of an Autonomous Agent: Genetic Evolution of a Neural-Network Driven Robot. The paper describes the results of the evolutionary development of a real, neural-network driven mobile robot. The evolutionary approach to the development of neural controllers for autonomous agents has been successfully used by many researchers, but most-if not all- studies have been carried out with computer simulations. Instead, in this research the whole evolutionary process takes places entirely on a real robot without human intervention. Although the experiments described here tackle a...',\n",
       " 'Independent Component Analysis. {A comprehensive introduction to ICA for students and practitioners<br>   Independent Component Analysis (ICA) is one of the most exciting new topics in fields such as neural networks, advanced statistics, and signal processing. This is the first book to provide a comprehensive introduction to this new technique complete with the fundamental mathematical background needed to understand and utilize it. It offers a general overview of the basics of ICA, important solutions and algorithms, and in-depth coverage of new applications in image processing, telecommunications, audio signal processing, and more.<br>   Independent Component Analysis is divided into four sections that cover:<br>   * General mathematical concepts utilized in the book<br>   * The basic ICA model and its solution<br>   * Various extensions of the basic ICA model<br>   * Real-world applications for ICA models<br>   Authors Hyvarinen, Karhunen, and Oja are well known for their contributions to the development of ICA and here cover all the relevant theory, new algorithms, and applications in various fields. Researchers, students, and practitioners from a variety of disciplines will find this accessible volume both helpful and informative.}',\n",
       " 'Foundations of Statistical Natural Language Processing. {\"Statistical natural-language processing is, in my estimation, one of the most fast-moving and exciting areas of computer science these days. Anyone who wants to learn this field would be well advised to get this book. For that matter, the same goes for anyone who is already in the field. I know that it is going to be one of the most well-thumbed books on my bookshelf.\" -- Eugene Charniak, Department of Computer Science, Brown University  <P>Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.  <P>More on this book}',\n",
       " \"Bayesian Data Analysis, Second Edition (Chapman & Hall/CRC Texts in Statistical Science). {Incorporating new and updated information, this second edition of THE bestselling text in Bayesian data analysis continues to emphasize practice over theory, describing how to conceptualize, perform, and critique statistical analyses from a Bayesian perspective. Its world-class authors provide guidance on all aspects of Bayesian data analysis and include examples of real statistical analyses, based on their own research, that demonstrate how to solve complicated problems. Changes in the new edition include: &#183;Stronger focus on MCMC&#183;Revision of the computational advice in Part III&#183;New chapters on nonlinear models and decision analysis&#183;Several additional applied examples from the authors' recent research&#183;Additional chapters on current models for Bayesian data analysis such as nonlinear models, generalized linear mixed models, and more&#183;Reorganization of chapters 6 and 7 on model checking and data collectionBayesian computation is currently at a stage where there are many reasonable ways to compute any given posterior distribution. However, the best approach is not always clear ahead of time. Reflecting this, the new edition offers a more pluralistic presentation, giving advice on performing computations from many perspectives while making clear the importance of being aware that there are different ways to implement any given iterative simulation computation. The new approach, additional examples, and updated information make Bayesian Data Analysis an excellent introductory text and a reference that working scientists will use throughout their professional life.}\",\n",
       " \"Learning collaborative information filters. Predicting items a user would like on the basis of other users’ ratings for these items has become a well-established strategy adopted by many rec- ommendation services on the Internet. Although this can be seen as a classification problem, algo- rithms proposed thus far do not draw on results from the machine learning literature. We propose a representation for collaborative filtering tasks that allows the application of virtually any ma- chine learning algorithm. We identify the short- comings of current collaborative filtering tech- niques and propose the use of learning algo- rithms paired with feature extraction techniques that specifically address the limitations of previ- ous approaches. Our best-performing algorithm is based on the singular value decomposition of an initial matrix of user ratings, exploiting latent structure that essentially eliminates the need for users to rate common items in order to become predictors for one another's preferences. We evaluate the proposed algorithm on a large data- base of user ratings for motion pictures and find that our approach significantly outperforms cur- rent collaborative filtering algorithms.\",\n",
       " \"On-line new event detection and tracking. We define and describe the related problems of new event detection and event tracking within a stream of broadcast news stories. We focus on a strict on-line setting---i.e., the system must make decisions about one story before looking at any subsequent stories. Our approach to detection uses a single pass clustering algorithm and a novel thresholding model that incorporates the properties of events as a major component. Our approach to tracking is similar to typical information filtering methods. We discuss the value of ``surprising'' features that have unusual occurrence characteristics, and briefly explore on-line adaptive filtering to handle evolving events in the news. New event detection and event tracking are part of the Topic Detection and Tracking (TDT) initiative.\",\n",
       " 'Information literacy: Essential skills for the information age. This monograph traces the history and development of the term \"information literacy.\" It examines the economic necessity of being information literate, and explores the research related to the concept. Included are reports on the National Educational Goals (1991) and on the report of the Secretary\\'s Commission on Achieving Necessary Skills (SCANS, 1991). Also examined are recent revisions in national subject matter standards that imply a recognition of the process skills included in information literacy. The book outlines the impact information literacy has on K-12 and higher education, and provides examples of information literacy in various contexts. Appendices include: Information Literacy Standards for Student Learning (prepared by the American Association of School Librarians and the Association for Educational Communications and Technology); definitions of SCANS components; a chronology of the development of information literacy; correlation of information literacy skills with selected National Subject Matter Standards; Dalbotten\\'s Correlation of Inquiry Skills to National Content Standards; and an explanation of rubrics and their application in standards education. Contains an extensive annotated ERIC (Educational Resources Information Center) bibliography and information about ERIC.',\n",
       " 'A New Kind of Science. {Physics and computer science genius Stephen Wolfram, whose Mathematica computer language launched a multimillion-dollar company, now sets his sights on a more daunting goal: understanding the universe. Wolfram lets the world see his work in <I>A New Kind of Science</I>, a gorgeous, 1,280-page tome more than a decade in the making. With patience, insight, and self-confidence to spare, Wolfram outlines a fundamental new way of modeling complex systems.<p>  On the frontier of complexity science since he was a boy, Wolfram is a  champion of cellular automata--256 \"programs\" governed by simple  nonmathematical rules. He points out that even the most complex  equations fail to accurately model biological systems, but the simplest  cellular automata can produce results straight out of nature--tree  branches, stream eddies, and leopard spots, for instance. The graphics  in <I>A New Kind of Science</I> show striking resemblance to the  patterns we see in nature every day.<p>  Wolfram wrote the book in a distinct style meant to make it easy to read,   even for nontechies; a basic familiarity with logic is helpful but  not essential. Readers will find themselves swept away by the elegant  simplicity of Wolfram\\'s ideas and the accidental artistry of the  cellular automaton models. Whether or not Wolfram\\'s revolution  ultimately gives us the keys to the universe, his new science is  absolutely awe-inspiring. <I>--Therese Littleton</I>} {This long-awaited work from one of the world\\'s most respected scientists presents a series of dramatic discoveries never before made public. Starting from a collection of simple computer experiments---illustrated in the book by striking computer graphics---Wolfram shows how their unexpected results force a whole new way of looking at the operation of our universe.  <P>Wolfram uses his approach to tackle a remarkable array of fundamental problems in science: from the origin of the Second Law of thermodynamics, to the development of complexity in biology, the computational limitations of mathematics, the possibility of a truly fundamental theory of physics, and the interplay between free will and determinism.  <P>Written with exceptional clarity, and illustrated by more than a thousand original pictures, this seminal book allows scientists and non-scientists alike to participate in what promises to be a major intellectual revolution.}',\n",
       " 'Improving literature based discovery support by genetic knowledge integration.. We present an interactive literature based biomedical discovery support system (BITOLA). The goal of the system is to discover new, potentially meaningful relations between a given starting concept of interest and other concepts, by mining the bibliographic database Medline. To make the system more suitable for disease candidate gene discovery and to decrease the number of candidate relations, we integrate background knowledge about the chromosomal location of the starting disease as well as the chromosomal location of the candidate genes from resources such as LocusLink, HUGO and OMIM. The BITOLA system can be also used as an alternative way of searching the Medline database. The system is available at http://www.mf.uni-lj.si/bitola/.',\n",
       " 'Using literature-based discovery to identify disease candidate genes.. We present BITOLA, an interactive literature-based biomedical discovery support system. The goal of this system is to discover new, potentially meaningful relations between a given starting concept of interest and other concepts, by mining the bibliographic database MEDLINE. To make the system more suitable for disease candidate gene discovery and to decrease the number of candidate relations, we integrate background knowledge about the chromosomal location of the starting disease as well as the chromosomal location of the candidate genes from resources such as LocusLink and Human Genome Organization (HUGO). BITOLA can also be used as an alternative way of searching the MEDLINE database. The system is available at http://www.mf.uni-lj.si/bitola/.',\n",
       " 'Statistical Modeling: The Two Cultures. Abstract. There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated bya given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical communityhas been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theoryand practice, has developed rapidlyin fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move awayfrom exclusive dependence on data models and adopt a more diverse set of tools. 1.',\n",
       " 'Combining biological networks to predict genetic interactions.. Genetic interactions define overlapping functions and compensatory pathways. In particular, synthetic sick or lethal (SSL) genetic interactions are important for understanding how an organism tolerates random mutation, i.e., genetic robustness. Comprehensive identification of SSL relationships remains far from complete in any organism, because mapping these networks is highly labor intensive. The ability to predict SSL interactions, however, could efficiently guide further SSL discovery. Toward this end, we predicted pairs of SSL genes in Saccharomyces cerevisiae by using probabilistic decision trees to integrate multiple types of data, including localization, mRNA expression, physical interaction, protein function, and characteristics of network topology. Experimental evidence demonstrated the reliability of this strategy, which, when extended to human SSL interactions, may prove valuable in discovering drug targets for cancer therapy and in identifying genes responsible for multigenic diseases.',\n",
       " 'Modeling Brain Function : The World of Attractor Neural Networks. {Exploring one of the most exciting and potentially rewarding areas of scientific research, the study of the principles and mechanisms underlying brain function, this book introduces and explains the techniques brought from physics to the study of neural networks and the insights they have stimulated.  Substantial progress in understanding memory, the learning process, and self-organization by studying the properties of models of neural networks have resulted in discoveries of important parallels between the properties of statistical, nonlinear cooperative systems in physics and neural networks.     The author presents a coherent and clear, nontechnical view of all the basic ideas and results.  More technical aspects are restricted to special sections and appendices in each chapter.}',\n",
       " 'Electric Fields of the Brain: The Neurophysics of EEG. Electroencephalography (EEG) is practiced by neurologists, cognitive neuroscientists, and others interested in functional brain imaging. Whether for clinical or experimental purposes, all studies share a common purpose-to relate scalp potentials to the underlying neurophysiology. Electrical potentials on the scalp exhibit spatial and temporal patterns that depend on the nature and location of the sources and the way that currents and fields spread through tissue. Because these dynamic patterns are correlated with behavior and cognition, EEG provides a \"window on the mind,\" correlating physiology and psychology. This classic and widely acclaimed text, originally published in 1981, filled the large gap between EEG and the physical sciences. It has now been brought completely up to date and will again serve as an invaluable resource for understanding the principles of electric fields in living tissue and for using hard science to study human consciousness and cognition. No comparable volume exists for it is no easy task to explain the problems of EEG in clear language, with mathematics presented mainly in appendices. Among the many topics covered by the Second Edition are micro and meso (intermediate scale) synaptic sources, electrode placement, choice of reference, volume conduction, power and coherence measures, projection of scalp potentials to dura surface, dynamic signatures of conscious experience, neural networks immersed in global fields of synaptic action, and physiological bases for brain source dynamics. The Second Edition is an invaluable resource for neurologists, neuroscientists (especially cognitive neuroscientists), biomedical engineers, and their students and trainees. It will also appeal to physicists, mathematicians, computer scientists, psychiatrists, and industrial engineers interested in EEG.',\n",
       " 'A simple rule-based part of speech tagger. Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods. In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another. Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rulebased tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below. 1.',\n",
       " 'Learning Bayesian Networks. We examine Bayesian methods for learning Bayesian networks from a combination of prior knowledge and statistical data. In particular, we develop simple methods for generating priors for Bayesian-network parameters. Our work is a generalization of previous work that has concentrated on Bayesian networks containing only discrete variables and (to a lesser extent) on Gaussian networks. We introduce three assumptions that are abstractions of previously made assumptions: likelihood equivalence,...',\n",
       " \"Systematic discovery of regulatory motifs in human promoters and 3' UTRs by comparison of several mammals.. Comprehensive identification of all functional elements encoded in the human genome is a fundamental need in biomedical research. Here, we present a comparative analysis of the human, mouse, rat and dog genomes to create a systematic catalogue of common regulatory motifs in promoters and 3' untranslated regions (3' UTRs). The promoter analysis yields 174 candidate motifs, including most previously known transcription-factor binding sites and 105 new motifs. The 3'-UTR analysis yields 106 motifs likely to be involved in post-transcriptional regulation. Nearly one-half are associated with microRNAs (miRNAs), leading to the discovery of many new miRNA genes and their likely target genes. Our results suggest that previous estimates of the number of human miRNA genes were low, and that miRNAs regulate at least 20\\\\% of human genes. The overall results provide a systematic view of gene regulation in the human, which will be refined as additional mammalian genomes become available.\",\n",
       " 'Coase\\'s Penguin, or Linux and the Nature of the Firm. For decades our understanding of economic production has been that individuals order their productive activities in one of two ways: either as employees in firms, following the directions of managers, or as individuals in markets, following price signals. This dichotomy was first identified in the early work of Nobel laureate Ronald Coase, and was developed most explicitly in the work of neo-institutional economist Oliver Williamson. In the past three or four years, public attention has focused on a fifteen-year-old social-economic phenomenon in the software development world. This phenomenon, called free software or open source software, involves thousands or even tens of thousands of programmers contributing to large and small scale project, where the central organizing principle is that the software remains free of most constraints on copying and use common to proprietary materials. No one \"owns\" the software in the traditional sense of being able to command how it is used or developed, or to control its disposition. The result is the emergence of a vibrant, innovative and productive collaboration, whose participants are not organized in firms and do not choose their projects in response to price signals.  In this paper I explain that while free software is highly visible, it is in fact only one example of a much broader social-economic phenomenon. I suggest that we are seeing is the broad and deep emergence of a new, third mode of production in the digitally networked environment. I call this mode \"commons-based peer-production,\" to distinguish it from the property- and contract-based models of firms and markets. Its central characteristic is that groups of individuals successfully collaborate on large-scale projects following a diverse cluster of motivational drives and social signals, rather than either market prices or managerial commands.  The paper also explains why this mode has systematic advantages over markets and managerial hierarchies when the object of production is information or culture, and where the capital investment necessary for production-computers and communications capabilities-is widely distributed instead of concentrated. In particular, this mode of production is better than firms and markets for two reasons. First, it is better at identifying and assigning human capital to information and cultural production processes. In this regard, peer-production has an advantage in what I call \"information opportunity cost.\" That is, it loses less information about who the best person for a given job might be than do either of the other two organizational modes. Second, there are substantial increasing returns to allow very larger clusters of potential contributors to interact with very large clusters of information resources in search of new projects and collaboration enterprises. Removing property and contract as the organizing principles of collaboration substantially reduces transaction costs involved in allowing these large clusters of potential contributors to review and select which resources to work on, for which projects, and with which collaborators. This results in allocation gains, that increase more than proportionately with the increase in the number of individuals and resources that are part of the system. The article concludes with an overview of how these models use a variety of technological and social strategies to overcome the collective action problems usually solved in managerial and market-based systems by property and contract.',\n",
       " 'Spiking Neuron Models. {This introduction to spiking neurons can be used in advanced-level courses in computational neuroscience, theoretical biology, neural modeling, biophysics, or neural networks. It focuses on phenomenological approaches rather than detailed models in order to provide the reader with a conceptual framework. The authors formulate the theoretical concepts clearly without many mathematical details. While the book contains standard material for courses in computational neuroscience, neural modeling, or neural networks, it also provides an entry to current research. No prior knowledge beyond undergraduate mathematics is required.}',\n",
       " 'The Success of Open Source. {<p> Much of the innovative programming that powers the Internet, creates operating systems, and produces software is the result of \"open source\" code, that is, code that is freely distributed--as opposed to being kept secret--by those who write it. Leaving source code open has generated some of the most sophisticated developments in computer technology, including, most notably, Linux and Apache, which pose a significant challenge to Microsoft in the marketplace. As Steven Weber discusses, open source\\'s success in a highly competitive industry has subverted many assumptions about how businesses are run, and how intellectual products are created and protected. </p><p> Traditionally, intellectual property law has allowed companies to control knowledge and has guarded the rights of the innovator, at the expense of industry-wide cooperation. In turn, engineers of new software code are richly rewarded; but, as Weber shows, in spite of the conventional wisdom that innovation is driven by the promise of individual and corporate wealth, ensuring the free distribution of code among computer programmers can empower a more effective process for building intellectual products. In the case of Open Source, independent programmers--sometimes hundreds or thousands of them--make unpaid contributions to software that develops organically, through trial and error. </p><p> Weber argues that the success of open source is not a freakish exception to economic principles. The open source community is guided by standards, rules, decisionmaking procedures, and sanctioning mechanisms. Weber explains the political and economic dynamics of this mysterious but important market development.  </p>}',\n",
       " 'The Power of Identity: The Information Age: Economy, Society and Culture, Volume II (The Information Age) 2nd Edition. {The Power of Identity is the second volume of Manuel Castells&#146;s trilogy, The Information Age: Economy, Society, and Culture. It deals with the social, political, and cultural dynamics associated with the technological transformation of our societies and with the globalization of the economy. It analyzes the importance of cultural, religious, and national identities as sources of meaning for people, and the implications of these identities for social movements. It studies grassroots mobilizations against the unfettered globalization of wealth and power, and considers the formation of alternative projects of social organization, as represented by the environmental movement and the women&#146;s movement. It also analyzes the crisis of the nation-state and its transformation into a network state, and the effects on political democracies of the difficulties of international governance and the submission of political representation to the dictates of media politics and the!  politics of scandal.    <P>This substantially expanded second edition updates and elaborates the analysis of these themes, adding new sections on al-Qaeda and global terrorist networks, on the anti-globalization movement, on American unilateralism and the conflicts of global governance, on the crisis of political legitimacy throughout the world, and on the theory of the network state.}',\n",
       " 'The Informational City: Information Technology, Economic Restructuring, and the Urban-Regional Process. {The cities and the regions of the world are being transformed under the combined impact of a restructuring of the capitalist system and a technological revolution.  This is the thesis of this book, now in paperback.  Castells not only brings together an impressive array of evidence to support it but also puts forward a new body of theory to explain it.  He analyzes the interaction between information technology, economic restructuring and socio-spatial change through the empirical observation of contemporary national, urban and regional processes in the capitalist world, with emphasis on the United States.  The author summarizes a very wide range of evidence of urban and regional development, and isolates the causes and consequences of the processes and trends that may be observed.}',\n",
       " 'The Network Society: A Cross-Cultural Perspective. {Manuel Castells - one of the world&#146;s pre-eminent social scientists - has drawn together a stellar group of contributors to explore the patterns and dynamics of the network society in its cultural and institutional diversity. The book analyzes the technological, cultural and institutional transformation of societies around the world in terms of the critical role of electronic communication networks in business, everyday life, public services, social interaction and politics. The contributors demonstrate that the network society is the new form of social organization in the Information age, replacing the Industrial society. The book analyzes processes of technological transformation in interaction with social culture in different cultural and institutional contexts: the United States of America, the United Kingdom, Finland, Russia, China, India, Canada, and Catalonia. The topics examined include business productivity, global financial markets, cultural identity, the uses of the Internet in education and health, the anti-globalization movement, political processes, media and identity, and public policies to guide technological development. Taken together these studies show that the network society adopts very different forms, depending on the cultural and institutional environments in which it evolves. The Network Society, now available in paperback, is an outstanding and original volume of direct interest in academia - particularly in the fields of social sciences, communication studies, and business schools - as well as for policymakers engaged in technological policy and economic development. Business and management experts will also discover much of value to them within this book.}',\n",
       " 'MedKit: a helper toolkit for automatic mining of MEDLINE/PubMed citations. Summary: MEDLINE/PubMed is one of the most important information sources for bioinformatics text mining. However, there remain limitations in working with MEDLINE/PubMed citations. For example, PubMed imposes an upper limit of 10 000 for downloading PMID list or citations; and MEDLINE files are too large for most off-the-shelf XML parsers. We developed a Java package, MedKit, to work-around the limitations, as well as provide other useful functionalities, e.g. random sampling. Its four modules (querier, sampler, fetcher and parser) can work independently, or be pipelined in various combinations. It can be used as a stand-alone GUI application, or integrated into other text-mining systems. Text mining researchers and others may download and use the toolkit free for non-commercial purposes.  Availability: http://metnetdb.gdcb.iastate.edu/medkit  Contact: berleant@iastate.edu 10.1093/bioinformatics/bti087',\n",
       " 'A comprehensive evaluation of multicategory classification methods for microarray gene expression cancer diagnosis.. MOTIVATION: Cancer diagnosis is one of the most important emerging clinical applications of gene expression microarray technology. We are seeking to develop a computer system for powerful and reliable cancer diagnostic model creation based on microarray data. To keep a realistic perspective on clinical applications we focus on multicategory diagnosis. To equip the system with the optimum combination of classifier, gene selection and cross-validation methods, we performed a systematic and comprehensive evaluation of several major algorithms for multicategory classification, several gene selection methods, multiple ensemble classifier methods and two cross-validation designs using 11 datasets spanning 74 diagnostic categories and 41 cancer types and 12 normal tissue types. RESULTS: Multicategory support vector machines (MC-SVMs) are the most effective classifiers in performing accurate cancer diagnosis from gene expression data. The MC-SVM techniques by Crammer and Singer, Weston and Watkins and one-versus-rest were found to be the best methods in this domain. MC-SVMs outperform other popular machine learning algorithms, such as k-nearest neighbors, backpropagation and probabilistic neural networks, often to a remarkable degree. Gene selection techniques can significantly improve the classification performance of both MC-SVMs and other non-SVM learning algorithms. Ensemble classifiers do not generally improve performance of the best non-ensemble models. These results guided the construction of a software system GEMS (Gene Expression Model Selector) that automates high-quality model construction and enforces sound optimization and performance estimation procedures. This is the first such system to be informed by a rigorous comparative analysis of the available algorithms and datasets. AVAILABILITY: The software system GEMS is available for download from http://www.gems-system.org for non-commercial use. CONTACT: alexander.statnikov@vanderbilt.edu.',\n",
       " 'Statistical Learning Theory. {A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more.}',\n",
       " 'Critical Mass: How One Thing Leads to Another. Are there any \"laws of nature\" that influence the ways in which humans behave and organize themselves? In the seventeenth century, tired of the civil war ravaging England, Thomas Hobbes decided that he would work out what kind of government was needed for a stable society. His approach was based not on utopian wishful thinking but rather on Galileo\\'s mechanics to construct a theory of government from first principles. His solution is unappealing to today\\'s society, yet Hobbes had sparked a new way of thinking about human behavior in looking for the \"scientific\" rules of society.   Adam Smith, Immanuel Kant, Auguste Comte, and John Stuart Mill pursued this idea from different political perspectives. Little by little, however, social and political philosophy abandoned a \"scientific\" approach. Today, physics is enjoying a revival in the social, political and economic sciences. Ball shows how much we can understand of human behavior when we cease to try to predict and analyze the behavior of individuals and instead look to the impact of individual decisions-whether in circumstances of cooperation or conflict-can have on our laws, institutions and customs.   Lively and compelling, _Critical Mass_ is the first book to bring these new ideas together and to show how they fit within the broader historical context of a rational search for better ways to live.',\n",
       " 'Media Technology and Society: A History : From the Telegraph to the Internet. {How are media born? How do they change? And how do they change us?  <P> <i>Media Technology and Society</i> offers a comprehensive account of the history of communications technologies, from the printing press to the internet. Brian Winston argues that the development of new media, from the telegraph and the telephone to computers, satellite and virtual reality, is the product of a constant play-off between social necessity and suppression: the unwritten law by which new technologies are introduced into society only insofar as their disruptive potential is limited. Winston\\'s fascinating account examines the role played by individuals such as Alexander Graham Bell, Gugliemo Marconi, John Logie Baird, Boris Rozing and Charles Babbage, and challenges the popular myth of the present-day \"information revolution.\"}',\n",
       " \"On Rereading R. A. Fisher. Fisher's contributions to statistics are surveyed. His background, skills, temperament, and style of thought and writing are sketched. His mathematical and methodological contributions are outlined. More attention is given to the technical concepts he introduced or emphasized, such as consistency, sufficiency, efficiency, information, and maximum likelihood. Still more attention is given to his conception and concepts of probability and inference, including likelihood, the fiducial argument, and hypothesis testing. Fisher is at once very near to and very far from modern statistical thought generally.\",\n",
       " 'DakNet: rethinking connectivity in developing nations. DakNet provides extraordinarily low-cost digital communication, letting remote villages leapfrog past the expense of traditional connectivity solutions and begin development of a full-coverage broadband wireless infrastructure. What is the basis for a progressive, market-driven migration from e-governance to universal broadband connectivity that local users will pay for? DakNet, an ad hoc network that uses wireless technology to provide asynchronous digital connectivity, is evidence that the marriage of wireless and asynchronous service may indeed be the beginning of a road to universal broadband connectivity. DakNet has been successfully deployed in remote parts of both India and Cambodia at a cost two orders of magnitude less than that of traditional landline solutions.',\n",
       " 'Wearable computers as packet transport mechanisms in highly--partitioned ad--hoc networks. The decreasing size and cost of wearable computers and mobile sensors is presenting new challenges and opportunities for deploying networks. Existing network routing protocols provide reliable communication between nodes and allow for mobility and even ad-hoc deployment. They rely, however on the assumption of a dense scattering of nodes and end-to-end connectivity in the network. In this paper we address routing support for ad-hoc, wireless networks under conditions of sporadic connectivity and ever-present network partitions. This work proposes a general framework of agent movement and communication in which mobile computers physically carry packets across network partitions. We then propose algorithms that exploit the relative position of stationary devices and non-randonmess in the movement of mobile agents in the network. The learned structure of the network is used to inform an adaptive routing strategy With a simulation, we evaluate these algorithms and their ability to route packets efficiently through a highly-partitioned network',\n",
       " 'Routing in a delay tolerant network. We formulate the delay-tolerant networking routing problem, where messages are to be moved end-to-end across a connectivity graph that is time-varying but whose dynamics may be known in advance. The problem has the added constraints of finite buffers at each node and the general property that no contemporaneous end-to-end path may ever exist. This situation limits the applicability of traditional routing approaches that tend to treat outages as failures and seek to find an existing end-to-end path. We propose a framework for evaluating routing algorithms in such environments. We then develop several algorithms and use simulations to compare their performance with respect to the amount of knowledge they require about network topology. We find that, as expected, the algorithms using the least knowledge tend to perform poorly. We also find that with limited additional knowledge, far less than complete global knowledge, efficient algorithms can be constructed for routing in such environments. To the best of our knowledge this is the first such investigation of routing issues in DTNs.',\n",
       " 'Probabilistic routing in intermittently connected networks. In this paper, we address the problem of routing in intermittently connected networks. In such networks there is no guarantee that a fully connected path between source and destination exists at any time, rendering traditional routing protocols unable to deliver messages between hosts. There does, however, exist a number of scenarios where connectivity is intermittent, but where the possibility of communication still is desirable. Thus, there is a need for a way to route through networks with these properties. We propose PRoPHET, a probabilistic routing protocol for intermittently connected networks and compare it to the earlier presented Epidemic Routing protocol through simulations. We show that PRoPHET is able to deliver more messages than Epidemic Routing with a lower communication overhead.',\n",
       " 'On-demand multipath routing for mobile ad hoc networks. Mobile ad hoc networks are characterized by multi-hop wireless links, absence of any cellular infrastructure, and frequent host mobility. Design of efficient routing protocols in such networks is a challenging issue. A class of routing protocols called on-demand protocols has recently attracted attention because of their low routing overhead. The on-demand protocols depend on query floods to discover routes whenever a new route is needed. Such floods take up a substantial portion of network bandwidth. We focus on a particular on-demand protocol, called dynamic source routing, and show how intelligent use of multipath techniques can reduce the frequency of query floods. We develop an analytic modeling framework to determine the relative frequency of query floods for various techniques. Results show that while multipath routing is significantly better than single path routing, the performance advantage is small beyond a few paths and for long path lengths. It also shows that providing all intermediate nodes in the primary (shortest) route with alternative paths has a significantly better performance than providing only the source with alternate paths',\n",
       " 'Highly dynamic Destination-Sequenced Distance-Vector routing (DSDV) for mobile computers. An  ad-hoc  network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize ad hoc networks. Our modifications address some of the previous  objections  to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.',\n",
       " 'Ad-hoc on-demand distance vector routing. An ad-hoc network is the cooperative engagement of a collection of mobile nodes without the required intervention of any centralized access point or existing infrastructure. We present Ad-hoc On Demand Distance Vector Routing (AODV), a novel algorithm for the operation of such ad-hoc networks. Each mobile host operates as a specialized router, and routes are obtained as needed (i.e., on-demand) with little or no reliance on periodic advertisements. Our new routing algorithm is quite suitable for a dynamic self starting network, as required by users wishing to utilize ad-hoc networks. AODV provides loop- free routes even while repairing broken links. Because the protocol does not require global periodic routing advertisements, the demand on the overall bandwidth available to the mobile nodes is substantially less than in those protocols that do necessitate such advertisements. Nevertheless we can still maintain most of the advantages of basic distance vector routing mechanisms. We show that our algorithm scales to large populations of mobile nodes wishing to form ad-hoc networks. We also include an evaluation methodology and simulation results to verify the operation of our algorithm',\n",
       " \"Evaluating location predictors with extensive Wi-Fi mobility data. Location is an important feature for many applications, and wireless networks can better serve their clients by anticipating client mobility. As a result, many location predictors have been proposed in the literature, though few have been evaluated with empirical evidence. This paper reports on the results of the first extensive empirical evaluation of location predictors, using a two-year trace of the mobility patterns of over 6,000 users on Dartmouth's campus-wide Wi-Fi wireless network. \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\par We implemented and compared the prediction accuracy of several location predictors drawn from two major families of domain-independent predictors, namely Markov-based and compression-based predictors. We found that low-order Markov predictors performed as well or better than the more complex and more space-consuming compression-based predictors. Predictors of both families fail to make a prediction when the recent context has not been previously seen. To overcome this drawback, we added a simple fallback feature to each predictor and found that it significantly enhanced its accuracy in exchange for modest effort. Thus the Order-2 Markov predictor with fallback was the best predictor we studied, obtaining a median accuracy of about 72% for users with long trace lengths. We also investigated a simplification of the Markov predictors, where the prediction is based not on the most frequently seen context in the past, but the most recent, resulting in significant space and computational savings. We found that Markov predictors with this recency semantics can rival the accuracy of standard Markov predictors in some cases. Finally, we considered several seemingly obvious enhancements, such as smarter tie-breaking and aging of context information, and discovered that they had little effect on accuracy. The paper ends with a discussion and suggestions for further work.\",\n",
       " 'Epidemic Routing for Partially Connected Ad Hoc Networks. Mobile ad hoc routing protocols allow nodes with wireless adaptors to communicate with one another  without any pre-existing network infrastructure. Existing ad hoc routing protocols, while robust to  rapidly changing network topology, assume the presence of a connected path from source to destination.  Given power limitations, the advent of short-range wireless networks, and the wide physical conditions  over which ad hoc networks must be deployed, in some scenarios it is likely that this...',\n",
       " 'The FF planning system: Fast plan generation through heuristic search. We describe and evaluate the algorithmic techniques that are used in the FF planning system. Like the HSP system, FF relies on forward state space search, using a heuristic that estimates goal distances by ignoring delete lists. Unlike HSP&#039;s heuristic, our method does not assume facts to be independent. We introduce a novel search strategy that combines Hill-climbing with systematic search, and we show how other powerful heuristic information can be extracted and used to prune the search space. FF was the most successful automatic planner at the recent AIPS-2000 planning competition. We review the results of the competition, give data for other benchmark domains, and investigate the reasons for the runtime performance of FF compared to HSP.',\n",
       " 'Estimation and inference in econometrics. Offering students a unifying theoretical perspective, this innovative text emphasizes nonlinear techniques of estimation, including nonlinear least squares, nonlinear instrumental variables, maximum likelihood and the generalized method of moments, but nevertheless relies heavily on simple geometrical arguments to develop intuition. One theme of the book is the use of artificial regressions for estimation, inference, and specification testing of nonlinear models, including diagnostic tests for parameter constancy, series correlation, heteroskedasticity and other types of misspecification. Other topics include the linear simultaneous equations model, non-nested hypothesis tests, influential observations and leverage, transformations of the dependent variable, binary response models, models for time-series/cross-section data, multivariate models, seasonality, unit roots and cointegration, and Monte Carlo methods, always with an emphasis on problems that arise in applied work. Explaining throughout how estimates can be obtained and tests can be carried out, the text goes beyond a mere algebraic description to one that can be easily translated into the commands of a standard econometric software package. A comprehensive and coherent guide to the most vital topics in econometrics today, this text is indispensable for all levels of students of econometrics, economics, and statistics on regression and related topics.',\n",
       " 'Bayesian Methods: A Social and Behavioral Sciences Approach. This is the first book to provide a comprehensive but accessible introduction to Bayesian data analysis designed specifically for those in the social and behavioral sciences. Requiring few prerequisites, it first introduces Bayesian statistics and inference, then provides explicit guidance on assessing model quality and model fit, and finally introduces hierarchical models within the Bayesian context, which leads naturally to Markov Chain Monte Carlo techniques and other numerical methods. The author emphasizes practical computing issues, includes specific details for Bayesian model building and testing, and uses the freely available R and BUGS software for examples and exercise problems.',\n",
       " 'This computer responds to user frustration. A human-computer interaction (HCI) agent was designed and built to support users in their ability to recover from negative emotional states, particularly frustration. The agent uses social-affective feedback strategies delivered to the user with text-only interaction. The agent\\'s effectiveness was evaluated against two control conditions: (1) user\\'s emotions were ignored, and (2) users were able to report problems and \"vent\" their feelings and thoughts to the computer. Behavioral results showed that the agent was significantly more effective than the control conditions in helping relieve frustration levels.',\n",
       " \"What's basic about basic emotions?. A widespread assumption in theories of emotion is that there exists a small set of basic emotions. From a biological perspective, this idea is manifested in the belief that there might be neurophysiological and anatomical substrates corresponding to the basic emotions. From a psychological perspective, basic emotions are often held to be the primitive building blocks of other, nonbasic emotions. The content of such claims is examined, and the results suggest that there is no coherent nontrivial notion of basic emotions as the elementary psychological primitives in terms of which other emotions can be explained. Thus, the view that there exist basic emotions out of which all other emotions are built, and in terms of which they can be explained, is questioned, raising the possibility that this position is an article of faith rather than an empirically or theoretically defensible basis for the conduct of emotion research. This suggests that perhaps the notion of basic emotions will not lead to significant progress in the field. An alternative approach to explaining the phenomena that appear to motivate the postulation of basic emotions is presented.\",\n",
       " \"A model of textual affect sensing using real-world knowledge. This paper presents a novel way for assessing the affective qualities of natural language and a scenario for its use. Previous approaches to textual affect sensing have employed keyword spotting, lexical affinity, statistical methods, and hand-crafted models. This paper demonstrates a new approach, using large-scale real-world knowledge about the inherent affective nature of everyday situations (such as ``getting into a car accident'') to classify sentences into ``basic'' emotion categories. This commonsense approach has new robustness implications. Open Mind Commonsense was used as a real world corpus of 400,000 facts about the everyday world. Four linguistic models are combined for robustness as a society of commonsense-based affect recognition. These models cooperate and compete to classify the affect of text. Such a system that analyzes affective qualities sentence by sentence is of practical value when people want to evaluate the text they are writing. As such, the system is tested in an email writing application. The results suggest that the approach is robust enough to enable plausible affective text user interfaces.\",\n",
       " \"The {C}hicago Manual of Style. {<div>In the 1890s, a proofreader at the University of Chicago Press prepared a single sheet of typographic fundamentals intended as a guide for the University community. That sheet grew into a pamphlet, and the pamphlet grew into a book--the first edition of the <i>Manual of Style</i>, published in 1906. Now in its fifteenth edition, <i>The Chicago Manual of Style</i>--the essential reference for authors, editors, proofreaders, indexers, copywriters, designers, and publishers in any field--is more comprehensive and easier to use than ever before. <br><br>Those who work with words know how dramatically publishing has changed in the past decade, with technology now informing and influencing every stage of the writing and publishing process. In creating the fifteenth edition of the <i>Manual</i>, Chicago's renowned editorial staff drew on direct experience of these changes, as well as on the recommendations of the <i>Manual</i>'s first advisory board, composed of a distinguished group of scholars, authors, and professionals from a wide range of publishing and business environments.<br><br>Every aspect of coverage has been examined and brought up to date--from publishing formats to editorial style and method, from documentation of electronic sources to book design and production, and everything in between. In addition to books, the <i>Manual</i> now also treats journals and electronic publications. All chapters are written for the electronic age, with advice on how to prepare and edit manuscripts online, handle copyright and permissions issues raised by technology, use new methods of preparing mathematical copy, and cite electronic and online sources.<br><br>A new chapter covers American English grammar and usage, outlining the grammatical structure of English, showing how to put words and phrases together to achieve clarity, and identifying common errors. The two chapters on documentation have been reorganized and updated: the first now describes the two main systems preferred by Chicago, and the second discusses specific elements and subject matter, with examples of both systems. Coverage of design and manufacturing has been streamlined to reflect what writers and editors need to know about current procedures. And, to make it easier to search for information, each numbered paragraph throughout the <i>Manual</i> is now introduced by a descriptive heading.<br><br>Clear, concise, and replete with commonsense advice, <i>The Chicago Manual of Style</i>, fifteenth edition, offers the wisdom of a hundred years of editorial practice while including a wealth of new topics and updated perspectives. For anyone who works with words, whether on a page or computer screen, this continues to be the one reference book you simply must have.<br><br>What's new in the Fifteenth Edition:<br><br>* Updated material throughout to reflect current style, technology, and professional practice<br><br>* Scope expanded to include journals and electronic publications<br><br>* Comprehensive new chapter on American English grammar and usage by Bryan A. Garner (author of <i>A Dictionary of Modern American Usage</i>)<br><br>* Updated and rewritten chapter on preparing mathematical copy<br><br>* Reorganized and updated chapters on documentation, including guidance on citing electronic sources<br><br>* Streamlined coverage of current design and production processes, with a glossary of key terms<br><br>* Descriptive headings on all numbered paragraphs for ease of reference<br><br>* New diagrams of the editing and production processes for both books and journals, keyed to chapter discussions<br><br>* New, expanded Web site with special tools and features for <i>Manual</i> users. Sign up at www.chicagomanualofstyle.org for information and special discounts on future electronic <i>Manual of Style</i> products.<br><br><br></div>}\",\n",
       " 'Towards Semantic Web Mining. Semantic Web Mining aims at combining the two fast-developing research areas Semantic Web and Web Mining. The idea is to improve, on the one hand, the results of Web Mining by exploiting the new semantic structures in the Web; and to make use of Web Mining, on the other hand, for building up the Semantic Web. This paper gives an overview of where the two areas meet today, and sketches ways of how a closer integration could be profitable.',\n",
       " 'Semantic Matching of Web Services Capabilities. The Web is moving from being a collection of pages toward a collection of services that interoperate through the Internet. The first step toward this interoperation is the location of other services that can help toward the solution of a problem. In this paper we claim that lo- cation of web services should be based on the semantic match between a declarative description of the service being sought, and a description of the service being offered. Furthermore, we claim that this match is outside the representation capabilities of registries such as UDDI and languages such as WSDL. We propose a solution based on DAML-S, a DAML-based language for service description, and we show how service capabilities are presented in the Profile section of a DAML-S description and how a semantic match between advertisements and requests is performed.',\n",
       " '{DAML}-{S}: Web Service Description for the Semantic Web. In this paper we present DAML-S, a DAML+OIL ontology for describing the properties and capabilities of Web Services. Web Services - Web-accessible programs and devices - are garnering a great deal of interest from industry, and standards are emerging for low-level descriptions of Web Services. DAML-S complements this effort by providing Web Service descriptions at the application layer, describing what a service can do, and not just how it does it. In this paper we describe three aspects of our ontology: the service profile, the process model, and the service grounding. The paper focuses on the grounding, which connects our ontology with low-level XML-based descriptions of Web Services.',\n",
       " 'Negotiation and cooperation in multi-agent environments. Automated intelligent agents inhabiting a shared environment must coordinate their activities. Cooperation -- not merely coordination -- may improve the performance of the individual agents or the overall behavior of the system they form. Research in Distributed Artificial Intelligence (DAI) addresses the problem of designing automated intelligent systems which interact effectively. DAI is not the only field to take on the challenge of understanding cooperation and coordination. There are a variety of other multi-entity environments in which the entities coordinate their activity and cooperate. Among them are groups of people, animals, particles, and computers. We argue that in order to address the challenge of building coordinated and collaborated intelligent agents, it is beneficial to combine AI techniques with methods and techniques from a range of multi-entity fields, such as game theory, operations research, physics and philosophy. To support this claim, we describe some of our p...',\n",
       " \"Enzyme-specific profiles for genome annotation: PRIAM.. The advent of fully sequenced genomes opens the ground for the reconstruction of metabolic pathways on the basis of the identification of enzyme-coding genes. Here we describe PRIAM, a method for automated enzyme detection in a fully sequenced genome, based on the classification of enzymes in the ENZYME database. PRIAM relies on sets of position-specific scoring matrices ('profiles') automatically tailored for each ENZYME entry. Automatically generated logical rules define which of these profiles is required in order to infer the presence of the corresponding enzyme in an organism. As an example, PRIAM was applied to identify potential metabolic pathways from the complete genome of the nitrogen-fixing bacterium Sinorhizobium meliloti. The results of this automated method were compared with the original genome annotation and visualised on KEGG graphs in order to facilitate the interpretation of metabolic pathways and to highlight potentially missing enzymes.\",\n",
       " 'A performance comparison of multi-hop wireless ad hoc network routing protocols. An ad hoc network is a collection of wireless mobile nodes dynamically forming a temporary network without the use of any existing network infrastructure or centralized administration. Due to the limited transmission range of wireless network interfaces, multiple network \"hops\" may be needed for one node to exchange data with another across the network. In recent years, a variety of new routing protocols targeted specifically at this environment have been developed, but little performance information on each protocol and no realistic performance comparison between them is available. This paper presents the results of a detailed packet-level simulation comparing four multi-hop wireless ad hoc network routing protocols that cover a range of design choices: DSDV, TORA, DSR, and AODV. We have extended the ns-2 network simulator to accurately model the MAC and physical-layer behavior of the IEEE 802.11 wireless LAN standard, including a realistic wireless transmission channel model, and present the results of simulations of networks of 50 mobile nodes.',\n",
       " 'A review of current routing protocols for ad hoc mobile wireless networks. An ad hoc mobile network is a collection of mobile nodes that are dynamically and arbitrarily located in such a manner that the interconnections between nodes are capable of changing on a continual basis. In order to facilitate communication within the network, a routing protocol is used to discover routes between nodes. The primary goal of such an ad hoc network routing protocol is correct and efficient route establishment between a pair of nodes so that messages may be delivered in a timely manner. Route construction should be done with a minimum of overhead and bandwidth consumption. This article examines routing protocols for ad hoc networks and evaluates these protocols based on a given set of parameters. The article provides an overview of eight different protocols by presenting their characteristics and functionality, and then provides a comparison and discussion of their respective merits and drawbacks',\n",
       " 'A high-throughput path metric for multi-hop wireless routing. This paper presents the  expected transmission count  metric (ETX), which finds high-throughput paths on multi-hop wireless networks. ETX minimizes the expected total number of packet transmissions (including retransmissions) required to successfully deliver a packet to the ultimate destination. The ETX metric incorporates the effects of link loss ratios, asymmetry in the loss ratios between the two directions of each link, and interference among the successive links of a path. In contrast, the minimum hop-count metric chooses arbitrarily among the different paths of the same minimum length, regardless of the often large differences in throughput among those paths, and ignoring the possibility that a longer path might offer higher throughput.This paper describes the design and implementation of ETX as a metric for the DSDV and DSR routing protocols, as well as modifications to DSDV and DSR which allow them to use ETX. Measurements taken from a 29-node 802.11b test-bed demonstrate the poor performance of minimum hop-count, illustrate the causes of that poor performance, and confirm that ETX improves performance. For long paths the throughput improvement is often a factor of two or more, suggesting that ETX will become more useful as networks grow larger and paths become longer.',\n",
       " 'Routing in multi-radio, multi-hop wireless mesh networks. We present a new metric for routing in multi-radio, multi-hop wireless networks. We focus on wireless networks with stationary nodes, such as community wireless networks.The goal of the metric is to choose a high-throughput path between a source and a destination. Our metric assigns weights to individual links based on the Expected Transmission Time (ETT) of a packet over the link. The ETT is a function of the loss rate and the bandwidth of the link. The individual link weights are combined into a path metric called Weighted Cumulative ETT (WCETT) that explicitly accounts for the interference among links that use the same channel. The WCETT metric is incorporated into a routing protocol that we call Multi-Radio Link-Quality Source Routing.We studied the performance of our metric by implementing it in a wireless testbed consisting of 23 nodes, each equipped with two 802.11 wireless cards. We find that in a multi-radio environment, our metric significantly outperforms previously-proposed routing metrics by making judicious use of the second radio.',\n",
       " 'Power-aware routing in mobile ad hoc networks. In this paper we present a case for using new power-aware metrics for determining routes in wireless ad hoc networks. We present five different metrics based on battery power consumption at nodes. We show that using these metrics in a shortest-cost routing algorithm reduces the cost/packet of routing packets by 5-30% over shortest-hop routing (this cost reduction is on top of a 40-70% reduction in energy consumption obtained by using PAMAS, our MAClayer protocol) . Furthermore, using these new...',\n",
       " 'Large-scale copy number polymorphism in the human genome.. The extent to which large duplications and deletions contribute to human genetic variation and diversity is unknown. Here, we show that large-scale copy number polymorphisms (CNPs) (about 100 kilobases and greater) contribute substantially to genomic variation between normal humans. Representational oligonucleotide microarray analysis of 20 individuals revealed a total of 221 copy number differences representing 76 unique CNPs. On average, individuals differed by 11 CNPs, and the average length of a CNP interval was 465 kilobases. We observed copy number variation of 70 different genes within CNP intervals, including genes involved in neurological function, regulation of cell growth, regulation of metabolism, and several genes known to be associated with disease.',\n",
       " 'Detection of large-scale variation in the human genome. We identified 255 loci across the human genome that contain genomic imbalances among unrelated individuals. Twenty-four variants are present in $>$10% of the individuals that we examined. Half of these regions overlap with genes, and many coincide with segmental duplications or gaps in the human genome assembly. This previously unappreciated heterogeneity may underlie certain human phenotypic variation and susceptibility to disease and argues for a more dynamic human genome structure.',\n",
       " \"Practical RDF. <i>Practical RDF</i> explains RDF from the ground up, providing real-world examples and descriptions of how the technology is being used in applications like Mozilla, FOAF, and Chandler, as well as infrastructure you can use to build your own applications. This book cuts to the heart of the W3C's often obscure specifications, giving you tools to apply RDF successfully in your own projects. The first part of the book focuses on the RDF specifications. After an introduction to RDF, the book covers the RDF specification documents themselves, including RDF Semantics and Concepts and Abstract Model specifications, RDF constructs, and the RDF Schema. The second section focuses on programming language support, and the tools and utilities that allow developers to review, edit, parse, store, and manipulate RDF/XML. Subsequent sections focus on RDF's data roots, programming and framework support, and practical implementation and use of RDF and RDF/XML. If you want to know how to apply RDF to information processing, <i>Practical RDF</i> is for you. Whether your interests lie in large-scale information aggregation and analysis or in smaller-scale projects like weblog syndication, this book will provide you with a solid foundation for working with RDF.\",\n",
       " 'Nonlinear Dimensionality Reduction by Locally Linear Embedding. Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighbor- hood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstruc- tions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.',\n",
       " 'Intrinsically unstructured proteins and their functions.. { Many gene sequences in eukaryotic genomes encode entire proteins or large segments of proteins that lack a well-structured three-dimensional fold. Disordered regions can be highly conserved between species in both composition and sequence and, contrary to the traditional view that protein function equates with a stable three-dimensional structure, disordered regions are often functional, in ways that we are only beginning to discover. Many disordered segments fold on binding to their biological targets (coupled folding and binding), whereas others constitute flexible linkers that have a role in the assembly of macromolecular arrays.}',\n",
       " 'A Taxonomy of Visualization Techniques Using the Data State Reference Model. In previous work, researchers have attempted to construct taxonomies of information visualization techniques by examining the data domains that are compatible with these techniques. This is useful because implementers can quickly identify various techniques that can be applied to their domain of interest. However, these taxonomies do not help the implementers understand how to apply and implement these techniques. In this paper, we will extend and then propose a new way to taxonomize information visualization techniques by using the Data State Model [Chi98]. In fact, as the taxonomic analysis in this paper will show, many of the techniques share similar operating steps that can easily be reused. The paper shows that the Data State Model not only helps researchers understand the space of design, but also helps implementers understand how information visualization techniques can be applied more broadly.  Keywords  Information Visualization, Data State Model, Reference Model, Taxonomy, Te...',\n",
       " 'The Central Role of the Propensity Score in Observational Studies for Causal Effects. The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two- dimensional plot.',\n",
       " 'Performance of optical flow techniques. While different optical flow techniques continue to appear, there has been a lack of quantitative evaluation of existing methods. For a common set of real and synthetic image sequences, we report the results of a number of regularly cited optical flow techniques, including instances of differential, matching, energy-based and phase-based methods. Our comparisons are primarily empirical, and concentrate on the accuracy, reliability and density of the velocity measurements; they show that performance can differ significantly among the techniques we implemented. 1 Introduction  Without doubt, a fundamental problem in the processing of image sequences is the measurement of optical flow (or image velocity). The goal is to compute an approximation to the 2-d motion field -- a projection of the 3-d velocities of surface points onto the imaging surface -- from spatiotemporal patterns of image intensity [31, 58]. Once computed, the measurements of image velocity can be used for a wide variety o...',\n",
       " 'Coupled hidden Markov models for complex action recognition. We present algorithms for coupling and training hidden Markov models (HMMs) to model interacting processes, and demonstrate their superiority to conventional HMMs in a vision task classifying two-handed actions. HMMs are perhaps the most successful framework in perceptual computing for modeling and classifying dynamic behaviors, popular because they offer dynamic time warping, a training algorithm and a clear Bayesian semantics. However the Markovian framework makes strong restrictive assumptions about the system generating the signal-that it is a single process having a small number of states and an extremely limited state memory. The single-process model is often inappropriate for vision (and speech) applications, resulting in low ceilings on model performance. Coupled HMMs provide an efficient way to resolve many of these problems, and offer superior training speeds, model likelihoods, and robustness to initial conditions.',\n",
       " 'Comparing images using the Hausdorff distance. The Hausdorff distance measures the extent to which each point of a model set lies near some point of an image set and vice versa. Thus, this distance can be used to determine the degree of resemblance between two objects that are superimposed on one another. Efficient algorithms for computing the Hausdorff distance between all possible relative positions of a binary image and a model are presented. The focus is primarily on the case in which the model is only allowed to translate with respect to the image. The techniques are extended to rigid motion. The Hausdorff distance computation differs from many other shape comparison methods in that no correspondence between the model and the image is derived. The method is quite tolerant of small position errors such as those that occur with edge detectors and other feature extraction methods. It is shown that the method extends naturally to the problem of comparing a portion of a model against an image.',\n",
       " 'A Bayesian Approach to Joint Feature Selection and Classifier Design. This paper adopts a Bayesian approach to simultaneously learn both an optimal nonlinear classifier and a subset of predictor variables (or features) that are most relevant to the classification task. The approach uses heavy-tailed priors to promote sparsity in the utilization of both basis functions and features; these priors act as regularizers for the likelihood function that rewards good classification on the training data. We derive an expectation- maximization (EM) algorithm to efficiently compute a maximum a posteriori (MAP) point estimate of the various parameters. The algorithm is an extension of recent state-of-the-art sparse Bayesian classifiers, which in turn can be seen as Bayesian counterparts of support vector machines. Experimental comparisons using kernel classifiers demonstrate both parsimonious feature selection and excellent classification accuracy on a range of synthetic and benchmark data sets.',\n",
       " 'Simultaneous Feature Selection and Clustering Using Mixture Models. Clustering is a common unsupervised learning technique used to discover group structure in a set of data. While there exist many algorithms for clustering, the important issue of feature selection, that is, what attributes of the data should be used by the clustering algorithms, is rarely touched upon. Feature selection for clustering is difficult because, unlike in supervised learning, there are no class labels for the data and, thus, no obvious criteria to guide the search. Another important problem in clustering is the determination of the number of clusters, which clearly impacts and is influenced by the feature selection issue. In this paper, we propose the concept of feature saliency and introduce an expectation-maximization (EM) algorithm to estimate it, in the context of mixture-based clustering. Due to the introduction of a minimum message length model selection criterion, the saliency of irrelevant features is driven toward zero, which corresponds to performing feature selection. The criterion and algorithm are then extended to simultaneously estimate the feature saliencies and the number of clusters.',\n",
       " 'Tutorial on higher-order statistics (spectra) in signal processing and system theory: theoretical results and some applications. A compendium of recent theoretical results associated with using higher-order statistics in signal processing and system theory is provided, and the utility of applying higher-order statistics to practical problems is demonstrated. Most of the results are given for one-dimensional processes, but some extensions to vector processes and multichannel systems are discussed. The topics covered include cumulant-polyspectra formulas; impulse response formulas; autoregressive (AR) coefficients; relationships between second-order and higher-order statistics for linear systems; double <e1>C</e1>(<e1>q</e1>,<e1>k</e1>) formulas for extracting autoregressive moving average (ARMA) coefficients; bicepstral formulas; multichannel formulas; harmonic processes; estimates of cumulants; and applications to identification of various systems, including the identification of systems from just output measurements, identification of AR systems, identification of moving-average systems, and identification of ARMA systems',\n",
       " \"Face recognition using eigenfaces. An approach to the detection and identification of human faces is presented, and a working, near-real-time face recognition system which tracks a subject's head and then recognizes the person by comparing characteristics of the face to those of known individuals is described. This approach treats face recognition as a two-dimensional recognition problem, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. Face images are projected onto a feature space (`face space') that best encodes the variation among known face images. The face space is defined by the `eigenfaces', which are the eigenvectors of the set of faces; they do not necessarily correspond to isolated features such as eyes, ears, and noses. The framework provides the ability to learn to recognize new faces in an unsupervised manner.\",\n",
       " 'Rapid object detection using a boosted cascade of simple features. This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the \"Integral Image\" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers[5]. The third contribution is a method for combining increasingly more complex classifiers in a \"cascade\" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.',\n",
       " 'Alignment by Maximization of Mutual Information. A new information-theoretic approach is presented for nding the pose of an object in an image. The technique does not require information about the surface properties of the object, besides its shape, and is robust with respect to variations of illumination. In our derivation, few assumptions are made about the nature of the imaging process. As a result the algorithms are quite general and can foreseeably be used in a wide variety of imaging situations. Experiments are presented that demonstrate the approach registering magnetic resonance (MR) images with computed tomography (CT) images, aligning a complex 3D object model to real scenes including clutter and occlusion, tracking a human head in a video sequence and aligning a view-based 2D object model to real images. The method is based on a formulation of the mutual information between the model and the image called EMMA. As applied here the technique is intensity-based, rather than feature-based. It works well in domains where edge or gradient-magnitude based methods have di\\x0eculty, yet it is more robust than traditional correlation. Additionally, it has an e\\x0ecient implementation that is based on stochastic approximation. Finally, we will describe a number of additional real-world applications that can be solved ef- ciently and reliably using EMMA. EMMA can be used in machine learning to nd maximally informative projections of high-dimensional data. EMMA can also be used to detect and correct corruption in magnetic resonance images (MRI).',\n",
       " 'Tracking Multiple Humans in Complex Situations. Tracking multiple humans in complex situations is challenging. The difficulties are tackled with appropriate knowledge in the form of various models in our approach. Human motion is decomposed into its global motion and limb motion. In the first part, we show how multiple human objects are segmented and their global motions are tracked in 3D using ellipsoid human shape models. Experiments show that it successfully applies to the cases where a small number of people move together, have occlusion, and cast shadow or reflection. In the second part, we estimate the modes (e.g., walking, running, standing) of the locomotion and 3D body postures by making inference in a prior locomotion model. Camera model and ground plane assumptions provide geometric constraints in both parts. Robust results are shown on some difficult sequences.',\n",
       " 'On the Surprising Behavior of Distance Metrics in High Dimensional Space. In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a effciency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used L k norm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric L(1 norm) is consistently more preferable than the Euclidean distance metric L(2 norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the L k norm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.',\n",
       " 'Outlier detection for high dimensional data. The outlier detection problem has important applications in the field of fraud detection, network robustness analysis, and intrusion detection. Most such applications are high dimensional domains in which the data can contain hundreds of dimensions. Many recent algorithms use concepts of proximity in order to find outliers based on their relationship to the rest of the data. However, in high dimensional space, the data is sparse and the notion of proximity fails to retain its meaningfulness. In fact, the sparsity of high dimensional data implies that every point is an almost equally good outlier from the perspective of proximity-based definitions. Consequently, for high dimensional data, the notion of finding meaningful outliers becomes substantially more complex and non-obvious. In this paper, we discuss new techniques for outlier detection which find the outliers by studying the behavior of projections from the data set.',\n",
       " 'Automatic subspace clustering of high dimensional data for data mining applications. Data mining applications place special requirements on clustering algorithms including: the ability to find clusters embedded in subspaces of high dimensional data, scalability, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity to the order of input records. We present CLIQUE, a clustering algorithm that satisfies each of these requirements. CLIQUE identifies dense clusters in subspaces of maximum dimensionality. It generates cluster descriptions in the form of DNF expressions that are minimized for ease of comprehension. It produces identical results irrespective of the order in which input records are presented and does not presume any specific mathematical form for data distribution. Through experiments, we show that CLIQUE efficiently finds accurate clusters in large high dimensional datasets.',\n",
       " 'Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays. Oligonucleotide arrays can provide a broad picture of the state of the cell, by monitoring the expression level of thousands of genes at the same time. It is of interest to develop techniques for extracting useful information from the resulting data sets. Here we report the application of a two-way clustering method for analyzing a data set consisting of the expression patterns of different cell types. Gene expression in 40 tumor and 22 normal colon tissue samples was analyzed with an Affymetrix oligonucleotide array complementary to more than 6,500 human genes. An efficient two-way clustering algorithm was applied to both the genes and the tissues, revealing broad coherent patterns that suggest a high degree of organization underlying gene expression in these tissues. Coregulated families of genes clustered together, as demonstrated for the ribosomal proteins. Clustering also separated cancerous from noncancerous tissue and cell lines from in vivo tissues on the basis of subtle distributed patterns of genes even when expression of individual genes varied only slightly between the tissues. Two-way clustering thus may be of use both in classifying genes into functional groups and in classifying tissues based on gene expression.',\n",
       " 'Correlation Clustering. We consider the following clustering problem: we have a complete graph on n vertices (items), where each edge (u, v) is labeled either + or - depending on whether u and v have been deemed to be similar or different. The goal is to produce a partition of the vertices (a clustering) that agrees as much as possible with the edge labels. That is, we want a clustering that maximizes the number of + edges within clusters, plus the number of - edges between clusters (equivalently, minimizes the number of disagreements: the number of - edges inside clusters plus the number of + edges between clusters). This formulation is motivated from a document clustering problem in which one has a pairwise similarity function f learned from past data, and the goal is to partition the current set of documents in a way that correlates with f as much as possible; it can also be viewed as a kind of “agnostic learning” problem.',\n",
       " 'Discovering local structure in gene expression data: the order-preserving submatrix problem. This paper concerns the discovery of patterns in gene expression matrices, in which each element gives the expression level of a given gene in a given experiment. Most existing methods for pattern discovery in such matrices are based on clustering genes by comparing their expression levels in all experiments, or clustering experiments by comparing their expression levels for all genes. Our work goes beyond such global approaches by looking for local patterns that manifest themselves when we focus simultaneously on a subset G of the genes and a subset T of the experiments. Specifically, we look for order-preserving submatrices (OPSMs), in which the expression levels of all genes induce the same linear ordering of the experiments (we show that the OPSM search problem is NP-hard in the worst case). Such a pattern might arise, for example, if the experiments in T represent distinct stages in the progress of a disease or in a cellular process, and the expression levels of all genes in G vary across the stages in the same way.We define a probabilistic model in which an OPSM is hidden within an otherwise random matrix. Guided by this model we develop an efficient algorithm for finding the hidden OPSM in the random matrix. In data generated according to the model the algorithm recovers the hidden OPSM with very high success rate. Application of the methods to breast cancer data seems to reveal significant local patterns.Our algorithm can be used to discover more than one OPSM within the same data set, even when these OPSMs overlap. It can also be adapted to handle relaxations and extensions of the OPSM condition. For example, we may allow the different rows of G x T to induce similar but not identical orderings of the columns, or we may allow the set T to include more than one representative of each stage of a biological process.',\n",
       " 'Clustering Gene Expression Patterns. Recent advances in biotechnology allow researchers to measure expression levels for thousands of genes simultaneously, across different conditions and over time. Analysis of data produced by such experiments offers potential insight into gene function and regulatory mechanisms. A key step in the analysis of gene expression data is the detection of groups of genes that manifest similar expression patterns. The corresponding algorithmic problem is to cluster multicondition gene expression patterns. In this paper we describe a novel clustering algorithm that was developed for analysis of gene expression data. We define an appropriate stochastic error model on the input, and prove that under the conditions of the model, the algorithm recovers the cluster structure with high probability. The running time of the algorithm on an n-gene dataset is O[n2[log(n)]c]. We also present a practical heuristic based on the same algorithmic ideas. The heuristic was implemented and its performance is demonstrated on simulated data and on real gene expression data, with very promising results.',\n",
       " 'Survey Of Clustering Data Mining Techniques. Clustering is a division of data into groups of similar objects. Representing the data by fewer clusters necessarily loses certain fine details, but achieves simplification. It models data by its clusters. Data modeling puts clustering in a historical perspective rooted in mathematics, statistics, and numerical analysis. From a machine learning perspective clusters correspond to hidden patterns, the search for clusters is unsupervised learning, and the resulting system represents a data concept. From a practical perspective clustering plays an outstanding role in data mining applications such as scientific data exploration, information retrieval and text mining, spatial database applications, Web analysis, CRM, marketing, medical diagnostics, computational biology, and many others. Clustering is the subject of active research in several fields such as statistics, pattern recognition, and machine learning. This survey focuses on clustering in data mining. Data mining adds to clustering the complications of very large datasets with very many attributes of different types. This imposes unique computational requirements on relevant clustering algorithms. A variety of algorithms have recently emerged that meet these requirements and were successfully applied to real-life data mining problems. They are subject of the survey.',\n",
       " 'Random projection in dimensionality reduction: applications to image and text data. Random projections have recently emerged as a powerful method for dimensionality reduction. Theoretical results indicate that the method preserves distances quite nicely; however, empirical results are sparse. We present experimental results on using random projection as a dimensionality reduction tool in a number of cases, where the high dimensionality of the data would otherwise lead to burden-some computations. Our application areas are the processing of both noisy and noiseless images, and information retrieval in text documents. We show that projecting the data onto a random lower-dimensional subspace yields results comparable to conventional dimensionality reduction methods such as principal component analysis: the similarity of data vectors is preserved well under random projection. However, using random projections is computationally significantly less expensive than using, e.g., principal component analysis. We also show experimentally that using a sparse random matrix gives additional computational savings in random projection.',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docs is the list of all the text strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '40',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '50',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '58',\n",
       " '59',\n",
       " '60',\n",
       " '61',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '65',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '70',\n",
       " '71',\n",
       " '72',\n",
       " '73',\n",
       " '74',\n",
       " '75',\n",
       " '76',\n",
       " '77',\n",
       " '78',\n",
       " '79',\n",
       " '80',\n",
       " '81',\n",
       " '82',\n",
       " '83',\n",
       " '84',\n",
       " '85',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '89',\n",
       " '90',\n",
       " '91',\n",
       " '92',\n",
       " '93',\n",
       " '94',\n",
       " '95',\n",
       " '96',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " '100',\n",
       " '101',\n",
       " '102',\n",
       " '103',\n",
       " '104',\n",
       " '105',\n",
       " '106',\n",
       " '107',\n",
       " '108',\n",
       " '109',\n",
       " '110',\n",
       " '111',\n",
       " '112',\n",
       " '113',\n",
       " '114',\n",
       " '115',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '120',\n",
       " '121',\n",
       " '122',\n",
       " '123',\n",
       " '124',\n",
       " '125',\n",
       " '126',\n",
       " '127',\n",
       " '128',\n",
       " '129',\n",
       " '130',\n",
       " '131',\n",
       " '132',\n",
       " '133',\n",
       " '134',\n",
       " '135',\n",
       " '136',\n",
       " '137',\n",
       " '138',\n",
       " '139',\n",
       " '140',\n",
       " '141',\n",
       " '142',\n",
       " '143',\n",
       " '144',\n",
       " '145',\n",
       " '146',\n",
       " '147',\n",
       " '148',\n",
       " '149',\n",
       " '150',\n",
       " '151',\n",
       " '152',\n",
       " '153',\n",
       " '154',\n",
       " '155',\n",
       " '156',\n",
       " '157',\n",
       " '158',\n",
       " '159',\n",
       " '160',\n",
       " '161',\n",
       " '162',\n",
       " '163',\n",
       " '164',\n",
       " '165',\n",
       " '166',\n",
       " '167',\n",
       " '168',\n",
       " '169',\n",
       " '170',\n",
       " '171',\n",
       " '172',\n",
       " '173',\n",
       " '174',\n",
       " '175',\n",
       " '176',\n",
       " '177',\n",
       " '178',\n",
       " '179',\n",
       " '180',\n",
       " '181',\n",
       " '182',\n",
       " '183',\n",
       " '184',\n",
       " '185',\n",
       " '186',\n",
       " '187',\n",
       " '188',\n",
       " '189',\n",
       " '190',\n",
       " '191',\n",
       " '192',\n",
       " '193',\n",
       " '194',\n",
       " '195',\n",
       " '196',\n",
       " '197',\n",
       " '198',\n",
       " '199',\n",
       " '200',\n",
       " '201',\n",
       " '202',\n",
       " '203',\n",
       " '204',\n",
       " '205',\n",
       " '206',\n",
       " '207',\n",
       " '208',\n",
       " '209',\n",
       " '210',\n",
       " '211',\n",
       " '212',\n",
       " '213',\n",
       " '214',\n",
       " '215',\n",
       " '216',\n",
       " '217',\n",
       " '218',\n",
       " '219',\n",
       " '220',\n",
       " '221',\n",
       " '222',\n",
       " '223',\n",
       " '224',\n",
       " '225',\n",
       " '226',\n",
       " '227',\n",
       " '228',\n",
       " '229',\n",
       " '230',\n",
       " '231',\n",
       " '232',\n",
       " '233',\n",
       " '234',\n",
       " '235',\n",
       " '236',\n",
       " '237',\n",
       " '238',\n",
       " '239',\n",
       " '240',\n",
       " '241',\n",
       " '242',\n",
       " '243',\n",
       " '244',\n",
       " '245',\n",
       " '246',\n",
       " '247',\n",
       " '248',\n",
       " '249',\n",
       " '250',\n",
       " '251',\n",
       " '252',\n",
       " '253',\n",
       " '254',\n",
       " '255',\n",
       " '256',\n",
       " '257',\n",
       " '258',\n",
       " '259',\n",
       " '260',\n",
       " '261',\n",
       " '262',\n",
       " '263',\n",
       " '264',\n",
       " '265',\n",
       " '266',\n",
       " '267',\n",
       " '268',\n",
       " '269',\n",
       " '270',\n",
       " '271',\n",
       " '272',\n",
       " '273',\n",
       " '274',\n",
       " '275',\n",
       " '276',\n",
       " '277',\n",
       " '278',\n",
       " '279',\n",
       " '280',\n",
       " '281',\n",
       " '282',\n",
       " '283',\n",
       " '284',\n",
       " '285',\n",
       " '286',\n",
       " '287',\n",
       " '288',\n",
       " '289',\n",
       " '290',\n",
       " '291',\n",
       " '292',\n",
       " '293',\n",
       " '294',\n",
       " '295',\n",
       " '296',\n",
       " '297',\n",
       " '298',\n",
       " '299',\n",
       " '300',\n",
       " '301',\n",
       " '302',\n",
       " '303',\n",
       " '304',\n",
       " '305',\n",
       " '306',\n",
       " '307',\n",
       " '308',\n",
       " '309',\n",
       " '310',\n",
       " '311',\n",
       " '312',\n",
       " '313',\n",
       " '314',\n",
       " '315',\n",
       " '316',\n",
       " '317',\n",
       " '318',\n",
       " '319',\n",
       " '320',\n",
       " '321',\n",
       " '322',\n",
       " '323',\n",
       " '324',\n",
       " '325',\n",
       " '326',\n",
       " '327',\n",
       " '328',\n",
       " '329',\n",
       " '330',\n",
       " '331',\n",
       " '332',\n",
       " '333',\n",
       " '334',\n",
       " '335',\n",
       " '336',\n",
       " '337',\n",
       " '338',\n",
       " '339',\n",
       " '340',\n",
       " '341',\n",
       " '342',\n",
       " '343',\n",
       " '344',\n",
       " '345',\n",
       " '346',\n",
       " '347',\n",
       " '348',\n",
       " '349',\n",
       " '350',\n",
       " '351',\n",
       " '352',\n",
       " '353',\n",
       " '354',\n",
       " '355',\n",
       " '356',\n",
       " '357',\n",
       " '358',\n",
       " '359',\n",
       " '360',\n",
       " '361',\n",
       " '362',\n",
       " '363',\n",
       " '364',\n",
       " '365',\n",
       " '366',\n",
       " '367',\n",
       " '368',\n",
       " '369',\n",
       " '370',\n",
       " '371',\n",
       " '372',\n",
       " '373',\n",
       " '374',\n",
       " '375',\n",
       " '376',\n",
       " '377',\n",
       " '378',\n",
       " '379',\n",
       " '380',\n",
       " '381',\n",
       " '382',\n",
       " '383',\n",
       " '384',\n",
       " '385',\n",
       " '386',\n",
       " '387',\n",
       " '388',\n",
       " '389',\n",
       " '390',\n",
       " '391',\n",
       " '392',\n",
       " '393',\n",
       " '394',\n",
       " '395',\n",
       " '396',\n",
       " '397',\n",
       " '398',\n",
       " '399',\n",
       " '400',\n",
       " '401',\n",
       " '402',\n",
       " '403',\n",
       " '404',\n",
       " '405',\n",
       " '406',\n",
       " '407',\n",
       " '408',\n",
       " '409',\n",
       " '410',\n",
       " '411',\n",
       " '412',\n",
       " '413',\n",
       " '414',\n",
       " '415',\n",
       " '416',\n",
       " '417',\n",
       " '418',\n",
       " '419',\n",
       " '420',\n",
       " '421',\n",
       " '422',\n",
       " '423',\n",
       " '424',\n",
       " '425',\n",
       " '426',\n",
       " '427',\n",
       " '428',\n",
       " '429',\n",
       " '430',\n",
       " '431',\n",
       " '432',\n",
       " '433',\n",
       " '434',\n",
       " '435',\n",
       " '436',\n",
       " '437',\n",
       " '438',\n",
       " '439',\n",
       " '440',\n",
       " '441',\n",
       " '442',\n",
       " '443',\n",
       " '444',\n",
       " '445',\n",
       " '446',\n",
       " '447',\n",
       " '448',\n",
       " '449',\n",
       " '450',\n",
       " '451',\n",
       " '452',\n",
       " '453',\n",
       " '454',\n",
       " '455',\n",
       " '456',\n",
       " '457',\n",
       " '458',\n",
       " '459',\n",
       " '460',\n",
       " '461',\n",
       " '462',\n",
       " '463',\n",
       " '464',\n",
       " '465',\n",
       " '466',\n",
       " '467',\n",
       " '468',\n",
       " '469',\n",
       " '470',\n",
       " '471',\n",
       " '472',\n",
       " '473',\n",
       " '474',\n",
       " '475',\n",
       " '476',\n",
       " '477',\n",
       " '478',\n",
       " '479',\n",
       " '480',\n",
       " '481',\n",
       " '482',\n",
       " '483',\n",
       " '484',\n",
       " '485',\n",
       " '486',\n",
       " '487',\n",
       " '488',\n",
       " '489',\n",
       " '490',\n",
       " '491',\n",
       " '492',\n",
       " '493',\n",
       " '494',\n",
       " '495',\n",
       " '496',\n",
       " '497',\n",
       " '498',\n",
       " '499',\n",
       " '500',\n",
       " '501',\n",
       " '502',\n",
       " '503',\n",
       " '504',\n",
       " '505',\n",
       " '506',\n",
       " '507',\n",
       " '508',\n",
       " '509',\n",
       " '510',\n",
       " '511',\n",
       " '512',\n",
       " '513',\n",
       " '514',\n",
       " '515',\n",
       " '516',\n",
       " '517',\n",
       " '518',\n",
       " '519',\n",
       " '520',\n",
       " '521',\n",
       " '522',\n",
       " '523',\n",
       " '524',\n",
       " '525',\n",
       " '526',\n",
       " '527',\n",
       " '528',\n",
       " '529',\n",
       " '530',\n",
       " '531',\n",
       " '532',\n",
       " '533',\n",
       " '534',\n",
       " '535',\n",
       " '536',\n",
       " '537',\n",
       " '538',\n",
       " '539',\n",
       " '540',\n",
       " '541',\n",
       " '542',\n",
       " '543',\n",
       " '544',\n",
       " '545',\n",
       " '546',\n",
       " '547',\n",
       " '548',\n",
       " '549',\n",
       " '550',\n",
       " '551',\n",
       " '552',\n",
       " '553',\n",
       " '554',\n",
       " '555',\n",
       " '556',\n",
       " '557',\n",
       " '558',\n",
       " '559',\n",
       " '560',\n",
       " '561',\n",
       " '562',\n",
       " '563',\n",
       " '564',\n",
       " '565',\n",
       " '566',\n",
       " '567',\n",
       " '568',\n",
       " '569',\n",
       " '570',\n",
       " '571',\n",
       " '572',\n",
       " '573',\n",
       " '574',\n",
       " '575',\n",
       " '576',\n",
       " '577',\n",
       " '578',\n",
       " '579',\n",
       " '580',\n",
       " '581',\n",
       " '582',\n",
       " '583',\n",
       " '584',\n",
       " '585',\n",
       " '586',\n",
       " '587',\n",
       " '588',\n",
       " '589',\n",
       " '590',\n",
       " '591',\n",
       " '592',\n",
       " '593',\n",
       " '594',\n",
       " '595',\n",
       " '596',\n",
       " '597',\n",
       " '598',\n",
       " '599',\n",
       " '600',\n",
       " '601',\n",
       " '602',\n",
       " '603',\n",
       " '604',\n",
       " '605',\n",
       " '606',\n",
       " '607',\n",
       " '608',\n",
       " '609',\n",
       " '610',\n",
       " '611',\n",
       " '612',\n",
       " '613',\n",
       " '614',\n",
       " '615',\n",
       " '616',\n",
       " '617',\n",
       " '618',\n",
       " '619',\n",
       " '620',\n",
       " '621',\n",
       " '622',\n",
       " '623',\n",
       " '624',\n",
       " '625',\n",
       " '626',\n",
       " '627',\n",
       " '628',\n",
       " '629',\n",
       " '630',\n",
       " '631',\n",
       " '632',\n",
       " '633',\n",
       " '634',\n",
       " '635',\n",
       " '636',\n",
       " '637',\n",
       " '638',\n",
       " '639',\n",
       " '640',\n",
       " '641',\n",
       " '642',\n",
       " '643',\n",
       " '644',\n",
       " '645',\n",
       " '646',\n",
       " '647',\n",
       " '648',\n",
       " '649',\n",
       " '650',\n",
       " '651',\n",
       " '652',\n",
       " '653',\n",
       " '654',\n",
       " '655',\n",
       " '656',\n",
       " '657',\n",
       " '658',\n",
       " '659',\n",
       " '660',\n",
       " '661',\n",
       " '662',\n",
       " '663',\n",
       " '664',\n",
       " '665',\n",
       " '666',\n",
       " '667',\n",
       " '668',\n",
       " '669',\n",
       " '670',\n",
       " '671',\n",
       " '672',\n",
       " '673',\n",
       " '674',\n",
       " '675',\n",
       " '676',\n",
       " '677',\n",
       " '678',\n",
       " '679',\n",
       " '680',\n",
       " '681',\n",
       " '682',\n",
       " '683',\n",
       " '684',\n",
       " '685',\n",
       " '686',\n",
       " '687',\n",
       " '688',\n",
       " '689',\n",
       " '690',\n",
       " '691',\n",
       " '692',\n",
       " '693',\n",
       " '694',\n",
       " '695',\n",
       " '696',\n",
       " '697',\n",
       " '698',\n",
       " '699',\n",
       " '700',\n",
       " '701',\n",
       " '702',\n",
       " '703',\n",
       " '704',\n",
       " '705',\n",
       " '706',\n",
       " '707',\n",
       " '708',\n",
       " '709',\n",
       " '710',\n",
       " '711',\n",
       " '712',\n",
       " '713',\n",
       " '714',\n",
       " '715',\n",
       " '716',\n",
       " '717',\n",
       " '718',\n",
       " '719',\n",
       " '720',\n",
       " '721',\n",
       " '722',\n",
       " '723',\n",
       " '724',\n",
       " '725',\n",
       " '726',\n",
       " '727',\n",
       " '728',\n",
       " '729',\n",
       " '730',\n",
       " '731',\n",
       " '732',\n",
       " '733',\n",
       " '734',\n",
       " '735',\n",
       " '736',\n",
       " '737',\n",
       " '738',\n",
       " '739',\n",
       " '740',\n",
       " '741',\n",
       " '742',\n",
       " '743',\n",
       " '744',\n",
       " '745',\n",
       " '746',\n",
       " '747',\n",
       " '748',\n",
       " '749',\n",
       " '750',\n",
       " '751',\n",
       " '752',\n",
       " '753',\n",
       " '754',\n",
       " '755',\n",
       " '756',\n",
       " '757',\n",
       " '758',\n",
       " '759',\n",
       " '760',\n",
       " '761',\n",
       " '762',\n",
       " '763',\n",
       " '764',\n",
       " '765',\n",
       " '766',\n",
       " '767',\n",
       " '768',\n",
       " '769',\n",
       " '770',\n",
       " '771',\n",
       " '772',\n",
       " '773',\n",
       " '774',\n",
       " '775',\n",
       " '776',\n",
       " '777',\n",
       " '778',\n",
       " '779',\n",
       " '780',\n",
       " '781',\n",
       " '782',\n",
       " '783',\n",
       " '784',\n",
       " '785',\n",
       " '786',\n",
       " '787',\n",
       " '788',\n",
       " '789',\n",
       " '790',\n",
       " '791',\n",
       " '792',\n",
       " '793',\n",
       " '794',\n",
       " '795',\n",
       " '796',\n",
       " '797',\n",
       " '798',\n",
       " '799',\n",
       " '800',\n",
       " '801',\n",
       " '802',\n",
       " '803',\n",
       " '804',\n",
       " '805',\n",
       " '806',\n",
       " '807',\n",
       " '808',\n",
       " '809',\n",
       " '810',\n",
       " '811',\n",
       " '812',\n",
       " '813',\n",
       " '814',\n",
       " '815',\n",
       " '816',\n",
       " '817',\n",
       " '818',\n",
       " '819',\n",
       " '820',\n",
       " '821',\n",
       " '822',\n",
       " '823',\n",
       " '824',\n",
       " '825',\n",
       " '826',\n",
       " '827',\n",
       " '828',\n",
       " '829',\n",
       " '830',\n",
       " '831',\n",
       " '832',\n",
       " '833',\n",
       " '834',\n",
       " '835',\n",
       " '836',\n",
       " '837',\n",
       " '838',\n",
       " '839',\n",
       " '840',\n",
       " '841',\n",
       " '842',\n",
       " '843',\n",
       " '844',\n",
       " '845',\n",
       " '846',\n",
       " '847',\n",
       " '848',\n",
       " '849',\n",
       " '850',\n",
       " '851',\n",
       " '852',\n",
       " '853',\n",
       " '854',\n",
       " '855',\n",
       " '856',\n",
       " '857',\n",
       " '858',\n",
       " '859',\n",
       " '860',\n",
       " '861',\n",
       " '862',\n",
       " '863',\n",
       " '864',\n",
       " '865',\n",
       " '866',\n",
       " '867',\n",
       " '868',\n",
       " '869',\n",
       " '870',\n",
       " '871',\n",
       " '872',\n",
       " '873',\n",
       " '874',\n",
       " '875',\n",
       " '876',\n",
       " '877',\n",
       " '878',\n",
       " '879',\n",
       " '880',\n",
       " '881',\n",
       " '882',\n",
       " '883',\n",
       " '884',\n",
       " '885',\n",
       " '886',\n",
       " '887',\n",
       " '888',\n",
       " '889',\n",
       " '890',\n",
       " '891',\n",
       " '892',\n",
       " '893',\n",
       " '894',\n",
       " '895',\n",
       " '896',\n",
       " '897',\n",
       " '898',\n",
       " '899',\n",
       " '900',\n",
       " '901',\n",
       " '902',\n",
       " '903',\n",
       " '904',\n",
       " '905',\n",
       " '906',\n",
       " '907',\n",
       " '908',\n",
       " '909',\n",
       " '910',\n",
       " '911',\n",
       " '912',\n",
       " '913',\n",
       " '914',\n",
       " '915',\n",
       " '916',\n",
       " '917',\n",
       " '918',\n",
       " '919',\n",
       " '920',\n",
       " '921',\n",
       " '922',\n",
       " '923',\n",
       " '924',\n",
       " '925',\n",
       " '926',\n",
       " '927',\n",
       " '928',\n",
       " '929',\n",
       " '930',\n",
       " '931',\n",
       " '932',\n",
       " '933',\n",
       " '934',\n",
       " '935',\n",
       " '936',\n",
       " '937',\n",
       " '938',\n",
       " '939',\n",
       " '940',\n",
       " '941',\n",
       " '942',\n",
       " '943',\n",
       " '944',\n",
       " '945',\n",
       " '946',\n",
       " '947',\n",
       " '948',\n",
       " '949',\n",
       " '950',\n",
       " '951',\n",
       " '952',\n",
       " '953',\n",
       " '954',\n",
       " '955',\n",
       " '956',\n",
       " '957',\n",
       " '958',\n",
       " '959',\n",
       " '960',\n",
       " '961',\n",
       " '962',\n",
       " '963',\n",
       " '964',\n",
       " '965',\n",
       " '966',\n",
       " '967',\n",
       " '968',\n",
       " '969',\n",
       " '970',\n",
       " '971',\n",
       " '972',\n",
       " '973',\n",
       " '974',\n",
       " '975',\n",
       " '976',\n",
       " '977',\n",
       " '978',\n",
       " '979',\n",
       " '980',\n",
       " '981',\n",
       " '982',\n",
       " '983',\n",
       " '984',\n",
       " '985',\n",
       " '986',\n",
       " '987',\n",
       " '988',\n",
       " '989',\n",
       " '990',\n",
       " '991',\n",
       " '992',\n",
       " '993',\n",
       " '994',\n",
       " '995',\n",
       " '996',\n",
       " '997',\n",
       " '998',\n",
       " '999',\n",
       " '1000',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_corpus = torch.load('temp/encoded_corpus.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1556,  0.1022, -0.0095,  ..., -0.2524, -0.2963, -0.0900],\n",
       "        [-0.4095,  0.1304,  0.0308,  ...,  0.4947,  0.2034,  0.0846],\n",
       "        [ 0.4423,  0.1621,  0.4447,  ..., -0.0036, -0.3720,  0.0240],\n",
       "        ...,\n",
       "        [-0.2899, -0.1725,  0.0080,  ...,  0.1382,  0.3344, -0.3326],\n",
       "        [-0.4259, -0.0597, -0.2117,  ...,  0.0700,  0.3439, -0.0537],\n",
       "        [-0.1999,  0.0034, -0.1960,  ..., -0.2166,  0.5256, -0.0830]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16980"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16980, 384])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_corpus.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating_threshold = 0.5\n",
      "exclude_unknowns = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\cornac\\data\\dataset.py:335: UserWarning: 18 duplicated observations are removed!\n",
      "  warnings.warn(\"%d duplicated observations are removed!\" % dup_count)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Training data:\n",
      "Number of users = 5551\n",
      "Number of items = 16949\n",
      "Number of ratings = 168396\n",
      "Max rating = 1.0\n",
      "Min rating = 1.0\n",
      "Global mean = 1.0\n",
      "---\n",
      "Test data:\n",
      "Number of users = 5551\n",
      "Number of items = 16949\n",
      "Number of ratings = 42053\n",
      "Number of unknown users = 0\n",
      "Number of unknown items = 0\n",
      "---\n",
      "Total users = 5551\n",
      "Total items = 16949\n",
      "\n",
      "[DMRL] Training started!\n",
      "Using device cpu for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\cornac\\data\\dataset.py:335: UserWarning: 5 duplicated observations are removed!\n",
      "  warnings.warn(\"%d duplicated observations are removed!\" % dup_count)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16384x100 and 150x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m prec_30 \u001b[38;5;241m=\u001b[39m cornac\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mPrecision(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Put everything together into an experiment and run it\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[43mcornac\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExperiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mratio_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdmrl_recommender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprec_30\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_300\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\cornac\\experiment\\experiment.py:142\u001b[0m, in \u001b[0;36mExperiment.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m         model\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[1;32m--> 142\u001b[0m     test_result, val_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_based\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_based\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mappend(test_result)\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\cornac\\eval_methods\\base_method.py:734\u001b[0m, in \u001b[0;36mBaseMethod.evaluate\u001b[1;34m(self, model, metrics, user_based, show_validation)\u001b[0m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] Training started!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m    733\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 734\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    735\u001b[0m train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m    737\u001b[0m \u001b[38;5;66;03m##############\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;66;03m# EVALUATION #\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;66;03m##############\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\cornac\\models\\dmrl\\recom_dmrl.py:140\u001b[0m, in \u001b[0;36mDMRL.fit\u001b[1;34m(self, train_set, val_set)\u001b[0m\n\u001b[0;32m    137\u001b[0m Recommender\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m, train_set, val_set)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_dmrl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\cornac\\models\\dmrl\\recom_dmrl.py:339\u001b[0m, in \u001b[0;36mDMRL._fit_dmrl\u001b[1;34m(self, train_set, val_set)\u001b[0m\n\u001b[0;32m    336\u001b[0m     item_image_embeddings \u001b[38;5;241m=\u001b[39m item_image_embeddings\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m embedding_factor_lists, rating_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_text_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_image_embeddings\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# preds = self.model(u_batch, i_batch, text)\u001b[39;00m\n\u001b[0;32m    343\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(embedding_factor_lists, rating_scores)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\cornac\\models\\dmrl\\dmrl.py:213\u001b[0m, in \u001b[0;36mDMRLModel.forward\u001b[1;34m(self, batch, text, image)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_factors):\n\u001b[0;32m    204\u001b[0m     concatted_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m    205\u001b[0m         [\n\u001b[0;32m    206\u001b[0m             user_embedding_factors[i],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    212\u001b[0m     )\n\u001b[1;32m--> 213\u001b[0m     attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcatted_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m     r_ui \u001b[38;5;241m=\u001b[39m attention[:, :, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSoftplus()(\n\u001b[0;32m    218\u001b[0m         torch\u001b[38;5;241m.\u001b[39msum(\n\u001b[0;32m    219\u001b[0m             user_embedding_factors[i] \u001b[38;5;241m*\u001b[39m item_embedding_factors[i], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    220\u001b[0m         )\n\u001b[0;32m    221\u001b[0m     )\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# log rating\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\CS608_RecommenderSystems\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16384x100 and 150x2)"
     ]
    }
   ],
   "source": [
    "\"\"\"Try without text modality\"\"\"\n",
    "\n",
    "# The necessary data can be loaded as follows\n",
    "# docs, item_ids = citeulike.load_text()\n",
    "feedback = citeulike.load_feedback(reader=Reader(item_set=item_ids))\n",
    "\n",
    "# item_text_modality = TextModality(\n",
    "#     corpus=docs,\n",
    "#     ids=item_ids,\n",
    "# )\n",
    "\n",
    "# Define an evaluation method to split feedback into train and test sets\n",
    "ratio_split = RatioSplit(\n",
    "    data=feedback,\n",
    "    test_size=0.2,\n",
    "    exclude_unknowns=True,\n",
    "    verbose=True,\n",
    "    seed=123,\n",
    "    rating_threshold=0.5,\n",
    "    # item_text=item_text_modality,\n",
    ")\n",
    "\n",
    "# Instantiate DMRL recommender\n",
    "dmrl_recommender = cornac.models.dmrl.DMRL(\n",
    "    batch_size=4096,\n",
    "    epochs=20,\n",
    "    log_metrics=False,\n",
    "    learning_rate=0.01,\n",
    "    num_factors=2,\n",
    "    decay_r=0.5,\n",
    "    decay_c=0.01,\n",
    "    num_neg=3,\n",
    "    embedding_dim=100,\n",
    ")\n",
    "\n",
    "# Use Recall@300 for evaluations\n",
    "rec_300 = cornac.metrics.Recall(k=300)\n",
    "prec_30 = cornac.metrics.Precision(k=30)\n",
    "\n",
    "# Put everything together into an experiment and run it\n",
    "cornac.Experiment(\n",
    "    eval_method=ratio_split, models=[dmrl_recommender], metrics=[prec_30, rec_300]\n",
    ").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS608_RecommenderSystems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
