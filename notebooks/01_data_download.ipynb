{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1. Download kaggle datasets\n",
    "2. Scrape following from MAL websites:\n",
    "    - Synopsis\n",
    "    - Background\n",
    "    - Voice actors (TBC)\n",
    "    - Image links\n",
    "\n",
    "Data preprocessing intended for next ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download kaggle datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "ds_path = \"../data/01_raw\"\n",
    "\n",
    "kaggle.api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/CooperUnion/anime-recommendations-database\n",
      "Dataset URL: https://www.kaggle.com/datasets/crazygump/myanimelist-scrappind-a-decade-of-anime\n"
     ]
    }
   ],
   "source": [
    "ds1_kagglename = \"CooperUnion/anime-recommendations-database\"\n",
    "ds2_kagglename = \"crazygump/myanimelist-scrappind-a-decade-of-anime\"\n",
    "\n",
    "kaggle.api.dataset_download_files(ds1_kagglename, path=ds_path, unzip=True)\n",
    "kaggle.api.dataset_download_files(ds2_kagglename, path=ds_path, unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scrape data from MAL Websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# soup = BeautifulSoup(html_doc, 'html.parser')\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# soup = BeautifulSoup(html_doc, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraper development (To be run outside this notebook)\n",
    "\n",
    "# import scrapy \n",
    "# import csv\n",
    "# import os\n",
    "# import logging\n",
    "\n",
    "# class MALSpider(scrapy.Spider):\n",
    "#     def __init__(self):\n",
    "#         anime_df = pd.read_csv(\"../data/rating.csv\")\n",
    "#         self.MAL_id_list = anime_df[\"anime_id\"]\n",
    "#         self.target_cols_list = [\"anime_id\", \"synopsis\", \"background\", \"image_url\"]\n",
    "\n",
    "#     name = \"MAL_spider\"\n",
    "#     base_url = \"https://myanimelist.net/anime/\"\n",
    "#     output_directory = \"../data/01_raw/\"\n",
    "#     os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "#     def start_requests(self):\n",
    "\n",
    "#         print(f\"Starting scraping operation\\n\")\n",
    "\n",
    "#         for MAL_id in self.MAL_id_list:\n",
    "\n",
    "#             url = self.base_url + f\"/{MAL_id}\"\n",
    "#             output_file = self.output_directory + \"/anime_scrapy.csv\"\n",
    "\n",
    "#             self.log(f\"URL: {url}\", level=logging.INFO)\n",
    "            \n",
    "#             with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "#                 writer = csv.writer(file, delimiter=\",\")\n",
    "#                 writer.writerow(self.target_cols_list)\n",
    "\n",
    "#             yield scrapy.Request(url=url, callback=self.parse, meta={\"output_file\":output_file})\n",
    "\n",
    "#     def parse(self, response):\n",
    "#         pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_details_scraper.py\n",
    "\n",
    "import pandas as pd\n",
    "import scrapy\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "from scrapy import signals\n",
    "import csv\n",
    "\n",
    "\n",
    "class csv_dialect(csv.Dialect):\n",
    "    delimiter = ','\n",
    "    quotechar = '\"'\n",
    "    doublequote = True\n",
    "    skipinitialspace = False\n",
    "    lineterminator = '\\n'\n",
    "    quoting = csv.QUOTE_ALL\n",
    "\n",
    "\n",
    "class IMDbSpider(scrapy.Spider):\n",
    "    name = 'scraping_test2'\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    output_directory = '/content/drive/MyDrive/IMDB Project/Scraping/scraped_data/ScraPy_Code_2.2_data'\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    data = []\n",
    "\n",
    "    # Try to read in the already scraped titles\n",
    "    try:\n",
    "        df_scraped = pd.read_csv('/content/drive/MyDrive/IMDB Project/Scraping/scraped_data/ScraPy_Code_2.2_data/more_details.csv', header=None)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        # If the file is empty, create an empty DataFrame\n",
    "        df_scraped = pd.DataFrame()\n",
    "\n",
    "    scraped_ids = df_scraped[0].tolist() if not df_scraped.empty else []\n",
    "\n",
    "    # Read in the start URLs and remove already scraped titles\n",
    "    df = pd.read_csv('/content/drive/MyDrive/IMDB Project/Scraping/scraped_data/ScraPy_Code_1_data/all_movie_ids_final.csv')\n",
    "    df['details_url'] = 'https://www.imdb.com' + df['details_url']\n",
    "    df['id'] = df['details_url'].str.extract(r'(tt\\d+)')\n",
    "    df = df[~df['id'].isin(scraped_ids)]\n",
    "    start_urls = df['details_url'].tolist()\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # Initialize a counter\n",
    "    url_count = 0\n",
    "\n",
    "    def handle_error(self, failure):\n",
    "        self.log(failure)\n",
    "\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "            imdb_id = url.split('/')[-2]\n",
    "            output_file = os.path.join(self.output_directory, 'more_details_2.csv')\n",
    "            yield scrapy.Request(url, headers={'User-Agent': self.user_agent}, meta={'imdb_id': imdb_id}, errback=self.handle_error, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        imdb_id = response.meta['imdb_id']\n",
    "\n",
    "\n",
    "        # Increment the counter\n",
    "        self.url_count += 1\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate the elapsed time and the average time per item\n",
    "        elapsed_time = datetime.datetime.now() - self.start_time\n",
    "        avg_time_per_item = elapsed_time / self.url_count\n",
    "\n",
    "        # Estimate the remaining time\n",
    "        remaining_items = len(self.start_urls) - self.url_count\n",
    "        estimated_remaining_time = avg_time_per_item * remaining_items\n",
    "\n",
    "        self.log(f'***********************TIME ESTIMATION ********************************* \\n\\n Processing {imdb_id} ({self.url_count}/{len(self.start_urls)}), estimated remaining time: {estimated_remaining_time}\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Initialize the meta dictionary with the imdb_id\n",
    "        meta = {\n",
    "            'imdb_id': imdb_id\n",
    "        }\n",
    "    #START OF PARSING CODE HERE \n",
    "    #Scraping Main Details\n",
    "        title = response.css('h1.sc-afe43def-0 span.sc-afe43def-1::text').get()\n",
    "        director = response.css('li[data-testid=\"title-pc-principal-credit\"] span:contains(\"Director\") ~ div ul li a::text').get()\n",
    "        writers = response.css('li[data-testid=\"title-pc-principal-credit\"] span:contains(\"Writers\") ~ div ul li a::text').getall()\n",
    "        stars = response.css('li[data-testid=\"title-pc-principal-credit\"] a:contains(\"Stars\") ~ div ul li a::text').getall()\n",
    "        user_reviews = response.css('ul[data-testid=\"reviewContent-all-reviews\"] a:contains(\"User reviews\") span.score::text').get()\n",
    "        critic_reviews = response.css('ul[data-testid=\"reviewContent-all-reviews\"] a:contains(\"Critic reviews\") span.score::text').get()\n",
    "        metascore = response.css('ul[data-testid=\"reviewContent-all-reviews\"] a:contains(\"Metascore\") span.score-meta::text').get()\n",
    "\n",
    "    # Scraping Technical Specs\n",
    "        tech_specs = response.css('div[data-testid=\"title-techspecs-section\"]')\n",
    "        runtime = tech_specs.css('li[data-testid=\"title-techspec_runtime\"] div.ipc-metadata-list-item__content-container::text').getall()\n",
    "        runtime = \" \".join(runtime)  # Joining the scraped parts to form the complete runtime text\n",
    "        sound_mix = tech_specs.css('li[data-testid=\"title-techspec_soundmix\"] a::text').getall()\n",
    "        # Scraping aspect ratio\n",
    "        aspect_ratio = tech_specs.css('span.ipc-metadata-list-item__list-content-item::text').get()\n",
    "    # Scraping Box Office Information\n",
    "        budget = response.css('li[data-testid=\"title-boxoffice-budget\"] span.ipc-metadata-list-item__list-content-item::text').get()\n",
    "        gross_us_canada = response.css('li[data-testid=\"title-boxoffice-grossdomestic\"] span.ipc-metadata-list-item__list-content-item::text').get()\n",
    "        \n",
    "        # \n",
    "        opening_weekend_data = response.css('li[data-testid=\"title-boxoffice-openingweekenddomestic\"] span.ipc-metadata-list-item__list-content-item::text').getall()\n",
    "        if opening_weekend_data:\n",
    "            opening_weekend_amount = opening_weekend_data[0]\n",
    "            opening_weekend_date = opening_weekend_data[1] if len(opening_weekend_data) > 1 else None\n",
    "        else:\n",
    "            opening_weekend_amount = None\n",
    "            opening_weekend_date = None\n",
    "        # \n",
    "        \n",
    "        \n",
    "        \n",
    "        # opening_weekend_amount = response.css('li[data-testid=\"title-boxoffice-openingweekenddomestic\"] span.ipc-metadata-list-item__list-content-item::text').getall()[0]\n",
    "        # opening_weekend_date = response.css('li[data-testid=\"title-boxoffice-openingweekenddomestic\"] span.ipc-metadata-list-item__list-content-item::text').getall()[1]\n",
    "        opening_weekend_us_canada = f\"{opening_weekend_amount}, {opening_weekend_date}\"\n",
    "        gross_worldwide = response.css('li[data-testid=\"title-boxoffice-cumulativeworldwidegross\"] span.ipc-metadata-list-item__list-content-item::text').get()\n",
    "\n",
    "    # Scraping Details Section\n",
    "        release_date = response.css('li[data-testid=\"title-details-releasedate\"] a.ipc-metadata-list-item__list-content-item--link::text').get()\n",
    "        countries_of_origin = response.css('li[data-testid=\"title-details-origin\"] a.ipc-metadata-list-item__list-content-item--link::text').getall()\n",
    "        official_sites = response.css('li[data-testid=\"details-officialsites\"] a.ipc-metadata-list-item__list-content-item--link::attr(href)').getall()\n",
    "        languages = response.css('li[data-testid=\"title-details-languages\"] a.ipc-metadata-list-item__list-content-item--link::text').getall()\n",
    "        also_known_as = response.css('li[data-testid=\"title-details-akas\"] span.ipc-metadata-list-item__list-content-item::text').get()\n",
    "        filming_locations = response.css('li[data-testid=\"title-details-filminglocations\"] a.ipc-metadata-list-item__list-content-item--link::text').get()\n",
    "        production_companies = response.css('li[data-testid=\"title-details-companies\"] a.ipc-metadata-list-item__list-content-item--link::text').getall()\n",
    "\n",
    "\n",
    "        # Update the meta dictionary with new data\n",
    "        meta.update({\n",
    "            'title': title,\n",
    "            'runtime': runtime,\n",
    "            'sound_mix': sound_mix,\n",
    "            'aspect_ratio': aspect_ratio,\n",
    "            'budget': budget,\n",
    "            'gross_us_canada': gross_us_canada,\n",
    "            'opening_weekend_us_canada': opening_weekend_us_canada,\n",
    "            'gross_worldwide': gross_worldwide,\n",
    "            'writers': writers,\n",
    "            'release_date': release_date,\n",
    "            'countries_of_origin': countries_of_origin,\n",
    "            'official_sites': official_sites,\n",
    "            'director': director,\n",
    "            'writers': writers,\n",
    "            'stars': stars,\n",
    "            'user_reviews': user_reviews,\n",
    "            'critic_reviews': critic_reviews,\n",
    "            'metascore': metascore,\n",
    "            'languages': languages,\n",
    "            'also_known_as': also_known_as,\n",
    "            'filming_locations': filming_locations,\n",
    "            'production_companies': production_companies\n",
    "        })\n",
    "\n",
    "\n",
    "        print(f'Title: {title}')\n",
    "        print(f'Director: {director}')\n",
    "        \n",
    "        yield response.follow(f'https://www.imdb.com/title/{imdb_id}/plotsummary', self.parse_plot_summary, meta=meta, errback=self.handle_error)\n",
    "\n",
    "    def parse_plot_summary(self, response):\n",
    "        imdb_id = response.meta['imdb_id']\n",
    "\n",
    "        # Scraping plot summaries\n",
    "        plot_summaries = response.css('div[data-testid=\"sub-section-summaries\"] div.ipc-html-content-inner-div::text').getall()\n",
    "        # Scraping synopsis\n",
    "        synopsis = response.css('ul.meta-data-list-full div.ipc-html-content-inner-div::text').get()\n",
    "\n",
    "\n",
    "\n",
    "        # Update the meta dictionary with new data\n",
    "        response.meta.update({\n",
    "            'plot_summaries': plot_summaries,\n",
    "            'synopsis': synopsis\n",
    "        })\n",
    "\n",
    "\n",
    "        yield response.follow(f'https://www.imdb.com/title/{imdb_id}/reviews?ref_=tt_urv', self.parse_user_reviews, meta=response.meta, errback=self.handle_error)\n",
    "\n",
    "\n",
    "    def parse_user_reviews(self, response):\n",
    "        imdb_id = response.meta['imdb_id']\n",
    "        review_blocks = response.css('.review-container')\n",
    "\n",
    "        if not review_blocks:\n",
    "            print(\"No review blocks found.\")\n",
    "            print(\"Going to Technical Specs\")\n",
    "            yield response.follow(f'https://www.imdb.com/title/{imdb_id}/technical/?ref_=tt_spec_sm', \n",
    "                                self.parse_technical_specs, \n",
    "                                meta=response.meta, \n",
    "                                errback=self.handle_error)\n",
    "\n",
    "        # Get reviews_data from response.meta, if it doesn't exist initialize it as an empty list\n",
    "        reviews_data_str = response.meta.get('reviews_data', '[]')\n",
    "        # Convert the string back to a list\n",
    "        reviews_data_list = eval(reviews_data_str)\n",
    "        reviewer_ratings = response.meta.get('reviewer_ratings', [])\n",
    "\n",
    "        for block in review_blocks:\n",
    "            review = block.css('.text.show-more__control::text').get()\n",
    "            reviewer = block.css('.display-name-link a::text').get()\n",
    "            rating = block.css('.ipl-ratings-bar span.rating-other-user-rating span::text').get()\n",
    "\n",
    "            print(f\"Reviewer: {reviewer}, Rating: {rating}\")\n",
    "\n",
    "            review_data = {  # dictionary to hold individual review data\n",
    "                'review': review,\n",
    "                'reviewer': reviewer,\n",
    "                'rating': rating\n",
    "            }\n",
    "\n",
    "            reviews_data_list.append(str(review_data))  # append string representation of review_data dictionary to reviews_data_list\n",
    "\n",
    "            reviewer_ratings.append({\n",
    "                'reviewer': reviewer,\n",
    "                'rating': rating\n",
    "            })\n",
    "\n",
    "        # Convert the list back to a string to store it in response.meta\n",
    "        reviews_data_str = str(reviews_data_list)\n",
    "\n",
    "        response.meta.update({\n",
    "            'reviews_data': reviews_data_str,  # Update with the string representation of the list\n",
    "            'reviewer_ratings': reviewer_ratings\n",
    "        })\n",
    "\n",
    "        key = response.css('.load-more-data::attr(data-key)').get()\n",
    "\n",
    "        # ... rest of your code\n",
    "\n",
    "\n",
    "        print(f\"Key: {key}\")\n",
    "\n",
    "        if key:\n",
    "            yield scrapy.Request(\n",
    "                url = f'https://www.imdb.com/title/{imdb_id}/reviews/_ajax?ref_=undefined&paginationKey='+ key,\n",
    "                callback=self.parse_user_reviews, \n",
    "                meta=response.meta,  \n",
    "            )\n",
    "        else:\n",
    "            print(\"No key found.\")\n",
    "            yield response.follow(f'https://www.imdb.com/title/{imdb_id}/technical/?ref_=tt_spec_sm', \n",
    "                                self.parse_technical_specs, \n",
    "                                meta=response.meta, \n",
    "                                errback=self.handle_error)\n",
    "\n",
    "    def parse_technical_specs(self, response):\n",
    "        imdb_id = response.meta['imdb_id']\n",
    "\n",
    "        # Scraping Technical Specs\n",
    "        runtime = response.css('li#runtime span.ipc-metadata-list-item__list-content-item::text').getall()\n",
    "        sound_mix = response.css('li#soundmixes a.ipc-metadata-list-item__list-content-item--link::text').getall()\n",
    "        color = response.css('li#colorations a.ipc-metadata-list-item__list-content-item--link::text').getall()\n",
    "        aspect_ratio = response.css('li#aspectratio span.ipc-metadata-list-item__list-content-item::text').getall()\n",
    "        camera = response.css('li#cameras span.ipc-metadata-list-item__list-content-item::text').getall()\n",
    "        laboratory = response.css('li#laboratory span.ipc-metadata-list-item__list-content-item::text').getall()\n",
    "        film_length = response.css('li#filmLength span.ipc-metadata-list-item__list-content-item::text').getall()\n",
    "        negative_format = response.css('li#negativeFormat span.ipc-metadata-list-item__list-content-item::text').getall()\n",
    "        cinematographic_process = response.css('li#process span.ipc-metadata-list-item__list-content-item::text').getall()\n",
    "        printed_film_format = response.css('li#printedFormat span.ipc-metadata-list-item__list-content-item::text').getall()\n",
    "\n",
    "        \n",
    "\n",
    "        # Update the meta dictionary with new data\n",
    "        response.meta.update({\n",
    "            'runtime': runtime,\n",
    "            'sound_mix': sound_mix,\n",
    "            'color': color,\n",
    "            'aspect_ratio': aspect_ratio,\n",
    "            'camera': camera,\n",
    "            'laboratory': laboratory,\n",
    "            'film_length': film_length,\n",
    "            'negative_format': negative_format,\n",
    "            'cinematographic_process': cinematographic_process,\n",
    "            'printed_film_format': printed_film_format\n",
    "        })\n",
    "\n",
    "        yield response.follow(f'https://www.imdb.com/title/{imdb_id}/externalreviews?ref_=tt_ov_rt', self.parse_external_reviews, meta=response.meta, errback=self.handle_error)\n",
    "\n",
    "    def parse_external_reviews(self, response):\n",
    "        imdb_id = response.meta['imdb_id']\n",
    "\n",
    "        # Locate all review site blocks\n",
    "        review_site_blocks = response.css('.ipc-metadata-list__item.ipc-metadata-list-item--link')\n",
    "\n",
    "        # Lists to store review site names and URLs\n",
    "        review_site_names = []\n",
    "        review_site_urls = []\n",
    "\n",
    "        # For each block, extract the reviewer site name and URL\n",
    "        for block in review_site_blocks:\n",
    "            review_site_name = block.css('a.ipc-metadata-list-item__label--link::text').get()\n",
    "            review_site_url = block.css('a.ipc-metadata-list-item__label--link::attr(href)').get()\n",
    "\n",
    "            # Add reviewer site name to the list\n",
    "            review_site_names.append(review_site_name)\n",
    "\n",
    "            # Add reviewer site URL to the list\n",
    "            review_site_urls.append(review_site_url)\n",
    "\n",
    "        # Update the meta dictionary with new data\n",
    "        response.meta.update({\n",
    "            'review_site_names': review_site_names,\n",
    "            'review_site_urls': review_site_urls\n",
    "        })\n",
    "\n",
    "\n",
    "        # Convert the meta dictionary to a DataFrame\n",
    "        meta_df = pd.DataFrame([response.meta])\n",
    "\n",
    "        # Write to CSV file\n",
    "        output_file = os.path.join(self.output_directory, '/content/drive/MyDrive/IMDB Project/Scraping/scraped_data/ScraPy_Code_2.2_data/more_details.csv')\n",
    "        print(f\"Writing data to {output_file}\")  # Print the file path\n",
    "        print(meta_df)  # Print the data that is being written\n",
    "        meta_df.to_csv(output_file, mode='a', header=False, index=False, encoding='utf-8')\n",
    "\n",
    "        # Yielding the scraped data\n",
    "        yield response.meta\n",
    "\n",
    "    def handle_error(self, failure): \n",
    "        # Log all failures\n",
    "        self.log(failure)\n",
    "        # Yield the meta data\n",
    "        yield failure.request.meta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs608_p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
