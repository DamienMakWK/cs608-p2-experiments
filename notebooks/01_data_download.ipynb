{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1. Download kaggle datasets\n",
    "2. Scrape following from MAL websites:\n",
    "    - Synopsis\n",
    "    - Background\n",
    "    - Voice actors (TBC)\n",
    "    - Image links\n",
    "\n",
    "Data preprocessing intended for next ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download kaggle datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "ds_path = \"../data/01_raw\"\n",
    "\n",
    "kaggle.api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/CooperUnion/anime-recommendations-database\n",
      "Dataset URL: https://www.kaggle.com/datasets/crazygump/myanimelist-scrappind-a-decade-of-anime\n"
     ]
    }
   ],
   "source": [
    "ds1_kagglename = \"CooperUnion/anime-recommendations-database\"\n",
    "ds2_kagglename = \"crazygump/myanimelist-scrappind-a-decade-of-anime\"\n",
    "\n",
    "kaggle.api.dataset_download_files(ds1_kagglename, path=ds_path, unzip=True)\n",
    "kaggle.api.dataset_download_files(ds2_kagglename, path=ds_path, unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scrape data from MAL Websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraper development (To be run outside this notebook)\n",
    "\n",
    "# import scrapy \n",
    "# import csv\n",
    "# import os\n",
    "# import logging\n",
    "\n",
    "# class MALSpider(scrapy.Spider):\n",
    "#     def __init__(self):\n",
    "#         anime_df = pd.read_csv(\"../data/rating.csv\")\n",
    "#         self.MAL_id_list = anime_df[\"anime_id\"]\n",
    "#         self.target_cols_list = [\"anime_id\", \"synopsis\", \"background\", \"image_url\"]\n",
    "\n",
    "#     name = \"MAL_spider\"\n",
    "#     base_url = \"https://myanimelist.net/anime/\"\n",
    "#     output_directory = \"../data/01_raw/\"\n",
    "#     os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "#     def start_requests(self):\n",
    "\n",
    "#         print(f\"Starting scraping operation\\n\")\n",
    "\n",
    "#         for MAL_id in self.MAL_id_list:\n",
    "\n",
    "#             url = self.base_url + f\"/{MAL_id}\"\n",
    "#             output_file = self.output_directory + \"/anime_scrapy.csv\"\n",
    "\n",
    "#             self.log(f\"URL: {url}\", level=logging.INFO)\n",
    "            \n",
    "#             with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "#                 writer = csv.writer(file, delimiter=\",\")\n",
    "#                 writer.writerow(self.target_cols_list)\n",
    "\n",
    "#             yield scrapy.Request(url=url, callback=self.parse, meta={\"output_file\":output_file})\n",
    "\n",
    "#     def parse(self, response):\n",
    "#         pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs608_p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
